{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyN9cQHhue0QZ2agF+u2o9hv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OneFineStarstuff/Cosmic-Brilliance/blob/main/train_physics_informed_pinn_py.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pqV3mVF0ZApQ"
      },
      "outputs": [],
      "source": [
        "pip install torch numpy matplotlib scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "train_physics_informed_pinn.py\n",
        "\n",
        "End-to-end training and OOD analysis for a physics-informed MLP with MC-Dropout.\n",
        "\n",
        "Features:\n",
        "- DataLoader tuned (num_workers, pin_memory)\n",
        "- CuDNN autotuner enabled\n",
        "- Batch-wise physics-consistent augmentation\n",
        "- PINN-style MLP with Dropout\n",
        "- Physics residual via ∂y_pred/∂x gradient norm\n",
        "- Cosine LR schedule, weight decay\n",
        "- MC-Dropout inference for uncertainty & OOD AUROC\n",
        "- Reliability diagram & uncertainty histograms\n",
        "- Loss curves saved to disk\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
        "from torch.backends import cudnn\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# fix: calibration_curve lives in sklearn.calibration\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.calibration import calibration_curve\n",
        "\n",
        "# Enable CuDNN autotuner for fixed-size inputs\n",
        "cudnn.benchmark = True\n",
        "\n",
        "# Device\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# ----------------------------\n",
        "# 1) Physics-consistent augmentation\n",
        "# ----------------------------\n",
        "def physics_augment(x, sigma=1e-2):\n",
        "    return x + sigma * torch.randn_like(x)\n",
        "\n",
        "# ----------------------------\n",
        "# 2) Model definition\n",
        "# ----------------------------\n",
        "class PINN(nn.Module):\n",
        "    def __init__(self, in_dim, hidden_dims=[128, 128], dropout_p=0.1):\n",
        "        super().__init__()\n",
        "        layers = []\n",
        "        last = in_dim\n",
        "        for h in hidden_dims:\n",
        "            layers += [nn.Linear(last, h), nn.ReLU(), nn.Dropout(dropout_p)]\n",
        "            last = h\n",
        "        layers.append(nn.Linear(last, in_dim))\n",
        "        self.net = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "# ----------------------------\n",
        "# 3) Physics residual function\n",
        "# ----------------------------\n",
        "def physics_residual(x, y_pred):\n",
        "    \"\"\"\n",
        "    Dummy placeholder: norm of gradient ∂y_pred/∂x.\n",
        "    Replace with your actual ODE/PDE residual.\n",
        "    \"\"\"\n",
        "    grads = torch.autograd.grad(y_pred.sum(), x, create_graph=True)[0]\n",
        "    return grads.norm(dim=-1)\n",
        "\n",
        "# ----------------------------\n",
        "# 4) MC-Dropout inference\n",
        "# ----------------------------\n",
        "def mc_dropout_predict(model, x, n_samples=50):\n",
        "    model.train()  # keep dropout active\n",
        "    preds = []\n",
        "    with torch.no_grad():\n",
        "        for _ in range(n_samples):\n",
        "            preds.append(model(x).cpu().numpy())\n",
        "    arr = np.stack(preds, axis=0)        # [S, N, D]\n",
        "    mean = arr.mean(axis=0)              # [N, D]\n",
        "    var  = arr.var(axis=0).mean(axis=1)  # [N]\n",
        "    return mean, var\n",
        "\n",
        "# ----------------------------\n",
        "# 5) Training & validation loops\n",
        "# ----------------------------\n",
        "mse_loss = nn.MSELoss()\n",
        "\n",
        "def train_epoch(model, loader, optimizer, phys_coeff):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    for xb, yb in loader:\n",
        "        xb = xb.to(DEVICE)\n",
        "        yb = yb.to(DEVICE)\n",
        "\n",
        "        # augment + require grad on inputs\n",
        "        xb_aug = physics_augment(xb)\n",
        "        xb_aug.requires_grad_(True)\n",
        "\n",
        "        # forward\n",
        "        y_pred = model(xb_aug)\n",
        "        mse   = mse_loss(y_pred, yb)\n",
        "        phys  = physics_residual(xb_aug, y_pred).mean()\n",
        "        loss  = mse + phys_coeff * phys\n",
        "\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item() * xb.size(0)\n",
        "\n",
        "    return total_loss / len(loader.dataset)\n",
        "\n",
        "\n",
        "def validate_epoch(model, loader, phys_coeff):\n",
        "    model.eval()\n",
        "    total_loss = 0.0\n",
        "    for xb, yb in loader:\n",
        "        xb = xb.to(DEVICE)\n",
        "        yb = yb.to(DEVICE)\n",
        "\n",
        "        xb = xb.clone().detach().requires_grad_(True)\n",
        "        y_pred = model(xb)\n",
        "        mse   = mse_loss(y_pred, yb)\n",
        "        phys  = physics_residual(xb, y_pred).mean()\n",
        "        total_loss += (mse + phys_coeff * phys).item() * xb.size(0)\n",
        "\n",
        "    return total_loss / len(loader.dataset)\n",
        "\n",
        "# ----------------------------\n",
        "# 6) Plotting utilities\n",
        "# ----------------------------\n",
        "def plot_losses(hist, save_dir=\"plots\"):\n",
        "    epochs = np.arange(1, len(hist['train']) + 1)\n",
        "    plt.figure()\n",
        "    plt.plot(epochs, hist['train'], label=\"Train Loss\")\n",
        "    plt.plot(epochs, hist['val'],   label=\"Val   Loss\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.legend()\n",
        "    plt.title(\"Loss Curve\")\n",
        "    plt.savefig(f\"{save_dir}/loss_curve.png\")\n",
        "    plt.close()\n",
        "\n",
        "def plot_reliability(y_true, conf, save_dir=\"plots\", n_bins=10):\n",
        "    frac_true, frac_pred = calibration_curve(y_true, conf, n_bins=n_bins)\n",
        "    plt.figure()\n",
        "    plt.plot(frac_pred, frac_true, 'o-')\n",
        "    plt.plot([0,1],[0,1],'--', color='gray')\n",
        "    plt.xlabel(\"Predicted Confidence\")\n",
        "    plt.ylabel(\"True Frequency\")\n",
        "    plt.title(\"Reliability Diagram\")\n",
        "    plt.savefig(f\"{save_dir}/reliability.png\")\n",
        "    plt.close()\n",
        "\n",
        "def plot_uncertainty_hist(unc_dict, save_dir=\"plots\"):\n",
        "    plt.figure()\n",
        "    for mode, uc in unc_dict.items():\n",
        "        plt.hist(uc, bins=30, alpha=0.6, label=mode)\n",
        "    plt.xlabel(\"Predictive Variance\")\n",
        "    plt.legend()\n",
        "    plt.title(\"OOD Uncertainty Histogram\")\n",
        "    plt.savefig(f\"{save_dir}/ood_uncertainty_hist.png\")\n",
        "    plt.close()\n",
        "\n",
        "# ----------------------------\n",
        "# 7) Main workflow\n",
        "# ----------------------------\n",
        "def main():\n",
        "    os.makedirs(\"plots\", exist_ok=True)\n",
        "\n",
        "    # Dummy data (replace with your physics dataset)\n",
        "    N_train, N_val = 5000, 1000\n",
        "    D = 4\n",
        "    X_train = torch.randn(N_train, D)\n",
        "    Y_train = torch.randn(N_train, D)\n",
        "    X_val   = torch.randn(N_val,   D)\n",
        "    Y_val   = torch.randn(N_val,   D)\n",
        "\n",
        "    # OOD sets\n",
        "    X_ood = {\n",
        "        'presence':      torch.randn(500, D) + 2.0,\n",
        "        'dissolution':   torch.randn(500, D) - 2.0,\n",
        "        'transcendence': torch.randn(500, D) * 3.0,\n",
        "    }\n",
        "\n",
        "    # DataLoaders\n",
        "    train_loader = DataLoader(\n",
        "        TensorDataset(X_train, Y_train),\n",
        "        batch_size=128, shuffle=True,\n",
        "        num_workers=4, pin_memory=True\n",
        "    )\n",
        "    val_loader = DataLoader(\n",
        "        TensorDataset(X_val, Y_val),\n",
        "        batch_size=128,\n",
        "        num_workers=2, pin_memory=True\n",
        "    )\n",
        "\n",
        "    # Model, optimizer, scheduler\n",
        "    model = PINN(in_dim=D, hidden_dims=[128,128], dropout_p=0.1).to(DEVICE)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
        "    scheduler = CosineAnnealingLR(optimizer, T_max=100)\n",
        "\n",
        "    # Training loop\n",
        "    history = {'train': [], 'val': []}\n",
        "    phys_coeff = 1e-3\n",
        "    for epoch in range(1, 101):\n",
        "        tr_loss = train_epoch(model, train_loader, optimizer, phys_coeff)\n",
        "        vl_loss = validate_epoch(model, val_loader, phys_coeff)\n",
        "        scheduler.step()\n",
        "        history['train'].append(tr_loss)\n",
        "        history['val'].append(vl_loss)\n",
        "        print(f\"Epoch {epoch:3d} | Train {tr_loss:.4f} | Val {vl_loss:.4f}\")\n",
        "\n",
        "    # Save curves and history\n",
        "    plot_losses(history, \"plots\")\n",
        "    np.savez(\"history.npz\", train=history['train'], val=history['val'])\n",
        "\n",
        "    # MC-Dropout ID & OOD evaluation\n",
        "    X_val_dev = X_val.to(DEVICE)\n",
        "    _, unc_id = mc_dropout_predict(model, X_val_dev, n_samples=50)\n",
        "    y_id = np.zeros_like(unc_id)\n",
        "\n",
        "    aurocs, unc_dict = {}, {}\n",
        "    for mode, Xo in X_ood.items():\n",
        "        Xo_dev = Xo.to(DEVICE)\n",
        "        _, unc_ood = mc_dropout_predict(model, Xo_dev, n_samples=50)\n",
        "        y_ood = np.ones_like(unc_ood)\n",
        "        aurocs[mode] = roc_auc_score(\n",
        "            np.concatenate([y_id, y_ood]),\n",
        "            np.concatenate([unc_id, unc_ood])\n",
        "        )\n",
        "        unc_dict[mode] = unc_ood\n",
        "        print(f\"{mode:13s} AUROC: {aurocs[mode]:.4f}\")\n",
        "\n",
        "    plot_uncertainty_hist(unc_dict, \"plots\")\n",
        "\n",
        "    # Reliability\n",
        "    var_all  = np.concatenate([unc_id] + list(unc_dict.values()))\n",
        "    conf_all = 1.0 - (var_all - var_all.min()) / (var_all.max() - var_all.min())\n",
        "    y_all    = np.concatenate([y_id] + [np.ones_like(v) for v in unc_dict.values()])\n",
        "    plot_reliability(y_all, conf_all, \"plots\")\n",
        "\n",
        "    print(\"Done. Check the 'plots/' folder for figures.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "I9vPMefEdXny"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}