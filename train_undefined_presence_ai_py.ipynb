{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyNdTSsNA2LMvdz5lr4jwLaT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OneFineStarstuff/Cosmic-Brilliance/blob/main/train_undefined_presence_ai_py.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SsYSQ1MQlCoF"
      },
      "outputs": [],
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "train_undefined_presence_ai.py\n",
        "\n",
        "1. Synthetic dataset (6â†’3)\n",
        "2. Float32 normalization\n",
        "3. MC-Dropout model\n",
        "4. Physics-informed residual loss\n",
        "5. AdamW training with scheduler, clipping, early stop\n",
        "6. MC-Dropout inference mean/std\n",
        "7. UMAP latent visualization\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import umap\n",
        "import random\n",
        "\n",
        "# 1. Reproducibility\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# 2. Synthetic Dataset\n",
        "class UndefinedPresenceDataset(Dataset):\n",
        "    def __init__(self, n=5000):\n",
        "        np.random.seed(SEED)\n",
        "        u = np.random.uniform(-1,1,(n,1))\n",
        "        v = np.random.uniform(0,2,(n,1))\n",
        "        w = np.random.uniform(-2,2,(n,1))\n",
        "        x = np.random.uniform(0,5,(n,1))\n",
        "        y = np.random.uniform(-1,1,(n,1))\n",
        "        z = np.random.uniform(0,1,(n,1))\n",
        "        X = np.hstack([u,v,w,x,y,z]).astype(np.float32)\n",
        "\n",
        "        # toy physics:\n",
        "        # presence = sin(u)*v + cos(w)\n",
        "        # dissolution = exp(-x*y)\n",
        "        # transcendence = z * (presence + dissolution)\n",
        "        presence = np.sin(u)*v + np.cos(w)\n",
        "        dissolution = np.exp(-x*y)\n",
        "        transcendence = z*(presence+dissolution)\n",
        "        Y = np.hstack([presence, dissolution, transcendence]).astype(np.float32)\n",
        "        Y += 0.01 * Y.std(axis=0) * np.random.randn(*Y.shape).astype(np.float32)\n",
        "\n",
        "        self.X_mean, self.X_std = X.mean(0), X.std(0)+1e-8\n",
        "        self.Y_mean, self.Y_std = Y.mean(0), Y.std(0)+1e-8\n",
        "        self.X = ((X-self.X_mean)/self.X_std).astype(np.float32)\n",
        "        self.Y = ((Y-self.Y_mean)/self.Y_std).astype(np.float32)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "    def __getitem__(self,i):\n",
        "        return torch.from_numpy(self.X[i]), torch.from_numpy(self.Y[i])\n",
        "\n",
        "# 3. Model with MC-Dropout\n",
        "class UndefinedPresenceAI(nn.Module):\n",
        "    def __init__(self, inp=6, hid=32, out=3, p=0.1):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(inp,hid),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(p),\n",
        "            nn.Linear(hid,out)\n",
        "        )\n",
        "    def forward(self,x):\n",
        "        return self.net(x)\n",
        "\n",
        "# 4. Physics-Informed Loss\n",
        "def physics_residual(pred, X, stats):\n",
        "    X_den = X*stats['X_std']+stats['X_mean']\n",
        "    u,v,w,x,y,z = X_den.T\n",
        "    presence = torch.sin(u)*v + torch.cos(w)\n",
        "    dissolution = torch.exp(-x*y)\n",
        "    transcend = z*(presence+dissolution)\n",
        "    Yt = torch.stack([presence,dissolution,transcend],1)\n",
        "    Yn = (Yt-stats['Y_mean'])/stats['Y_std']\n",
        "    return nn.MSELoss()(pred,Yn)\n",
        "\n",
        "def total_loss(pred, true, X, stats, lam=1.0):\n",
        "    l = nn.MSELoss()(pred,true)\n",
        "    p = physics_residual(pred,X,stats)\n",
        "    return l+lam*p, l, p\n",
        "\n",
        "# 5. MC-Dropout Prediction\n",
        "def mc_predict(model, X, T=50):\n",
        "    model.train()\n",
        "    ps=[]\n",
        "    with torch.no_grad():\n",
        "        for _ in range(T):\n",
        "            ps.append(model(X))\n",
        "    arr=torch.stack(ps)\n",
        "    return arr.mean(0), arr.std(0)\n",
        "\n",
        "# 6. Training Loop\n",
        "def train(model, dl_tr, dl_va, stats, epochs=100, lr=1e-3, wd=1e-5, lam=1.0, patience=10):\n",
        "    model.to(DEVICE)\n",
        "    opt=optim.AdamW(model.parameters(),lr=lr,weight_decay=wd)\n",
        "    sch=optim.lr_scheduler.ReduceLROnPlateau(opt,'min',factor=0.5,patience=5)\n",
        "    best=1e9; wait=0; hist={'tr':[],'va':[]}\n",
        "    for e in range(1,epochs+1):\n",
        "        model.train(); tr=0\n",
        "        for Xb,Yb in dl_tr:\n",
        "            Xb,Yb=Xb.to(DEVICE),Yb.to(DEVICE)\n",
        "            pred=model(Xb)\n",
        "            loss,_,_=total_loss(pred,Yb,Xb,stats,lam)\n",
        "            opt.zero_grad();loss.backward();opt.step()\n",
        "            tr+=loss.item()*len(Xb)\n",
        "        tr/=len(dl_tr.dataset)\n",
        "        model.eval(); va=0\n",
        "        with torch.no_grad():\n",
        "            for Xv,Yv in dl_va:\n",
        "                Xv,Yv=Xv.to(DEVICE),Yv.to(DEVICE)\n",
        "                pred=model(Xv)\n",
        "                loss,_,_=total_loss(pred,Yv,Xv,stats,lam)\n",
        "                va+=loss.item()*len(Xv)\n",
        "        va/=len(dl_va.dataset)\n",
        "        sch.step(va); hist['tr'].append(tr); hist['va'].append(va)\n",
        "        print(f\"Epoch {e} | Train {tr:.4f} | Val {va:.4f}\")\n",
        "        if va<best-1e-6: best=va; wait=0;torch.save(model.state_dict(),\"best.pth\")\n",
        "        else: wait+=1\n",
        "        if wait>=patience: break\n",
        "    model.load_state_dict(torch.load(\"best.pth\")); return hist,model\n",
        "\n",
        "# 7. Main Execution\n",
        "if __name__==\"__main__\":\n",
        "    ds=UndefinedPresenceDataset()\n",
        "    nva=int(0.2*len(ds))\n",
        "    tr,va=random_split(ds,[len(ds)-nva,nva])\n",
        "    dlt=DataLoader(tr,batch_size=128,shuffle=True)\n",
        "    dlv=DataLoader(va,batch_size=256)\n",
        "    stats={k:torch.tensor(v,device=DEVICE) for k,v in ds.__dict__.items() if k.endswith(('mean','std'))}\n",
        "    m=UndefinedPresenceAI().to(DEVICE)\n",
        "    history,model=train(m,dlt,dlv,stats)\n",
        "    # inference\n",
        "    Xall=torch.from_numpy(ds.X).to(DEVICE)\n",
        "    mean,std=mc_predict(model,Xall)\n",
        "    print(\"Mean:\",mean[0].cpu().numpy(), \"Std:\",std[0].cpu().numpy())\n",
        "    # UMAP\n",
        "    feats=model.net[0](Xall).detach().cpu().numpy()\n",
        "    emb=umap.UMAP(n_components=2,random_state=SEED).fit_transform(feats)\n",
        "    plt.scatter(emb[:,0],emb[:,1],s=4,alpha=0.5);plt.show()"
      ]
    }
  ]
}