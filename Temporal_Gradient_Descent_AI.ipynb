{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyMSkC8UD6r4RewlZNfeVxUH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OneFineStarstuff/Cosmic-Brilliance/blob/main/Temporal_Gradient_Descent_AI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0OIPPLZH3b-S"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "# Reproducibility\n",
        "SEED = 42\n",
        "np.random.seed(SEED)\n",
        "tf.random.set_seed(SEED)\n",
        "\n",
        "def make_temporal_data(n_samples=12000, length=40, n_classes=10,\n",
        "                       noise_std=0.15, burst_prob=0.08, seed=SEED):\n",
        "    \"\"\"\n",
        "    Build labelable 1D time series:\n",
        "    - Class c has frequency fc and phase phic; optional trend and bursts.\n",
        "    - Add AR(1) noise for temporal correlation.\n",
        "    Output shape: (n_samples, length, 1), labels: (n_samples,)\n",
        "    \"\"\"\n",
        "    rng = np.random.default_rng(seed)\n",
        "    per_class = n_samples // n_classes\n",
        "    X, y = [], []\n",
        "\n",
        "    freqs = np.linspace(0.6, 2.0, n_classes)           # distinct frequencies\n",
        "    phases = np.linspace(0.0, np.pi, n_classes)        # distinct phases\n",
        "    t = np.linspace(0, 1, length)\n",
        "\n",
        "    for c in range(n_classes):\n",
        "        f, ph = freqs[c], phases[c]\n",
        "        for _ in range(per_class):\n",
        "            base = np.sin(2*np.pi*f*t + ph)\n",
        "            trend = 0.4 * (t - 0.5) * ((c % 2)*2 - 1)   # alternating up/down trend\n",
        "            signal = 0.8*base + 0.2*trend\n",
        "\n",
        "            # AR(1) noise\n",
        "            eps = rng.normal(0, noise_std, size=length)\n",
        "            ar = np.zeros_like(eps)\n",
        "            phi = 0.6\n",
        "            for i in range(1, length):\n",
        "                ar[i] = phi * ar[i-1] + eps[i]\n",
        "\n",
        "            x = signal + ar\n",
        "\n",
        "            # Occasional burst\n",
        "            if rng.random() < burst_prob:\n",
        "                k = rng.integers(5, length-5)\n",
        "                x[k:k+3] += rng.normal(1.5, 0.3)\n",
        "\n",
        "            X.append(x[:, None].astype(\"float32\"))\n",
        "            y.append(c)\n",
        "\n",
        "    X = np.stack(X, axis=0)\n",
        "    y = np.array(y, dtype=np.int32)\n",
        "\n",
        "    # Shuffle\n",
        "    idx = rng.permutation(len(X))\n",
        "    return X[idx], y[idx]\n",
        "\n",
        "# Data\n",
        "X, y = make_temporal_data(n_samples=12000, length=40, n_classes=10, noise_std=0.12, burst_prob=0.06)\n",
        "\n",
        "# Split 70/15/15\n",
        "n = len(X)\n",
        "n_train = int(0.70 * n); n_val = int(0.15 * n)\n",
        "X_train, y_train = X[:n_train], y[:n_train]\n",
        "X_val,   y_val   = X[n_train:n_train+n_val], y[n_train:n_train+n_val]\n",
        "X_test,  y_test  = X[n_train+n_val:],       y[n_train+n_val:]\n",
        "\n",
        "# Standardize by train stats (per feature/channel)\n",
        "mean = X_train.mean(axis=(0,1), keepdims=True)\n",
        "std  = X_train.std(axis=(0,1), keepdims=True) + 1e-7\n",
        "X_train = (X_train - mean) / std\n",
        "X_val   = (X_val   - mean) / std\n",
        "X_test  = (X_test  - mean) / std\n",
        "\n",
        "# tf.data pipelines\n",
        "BATCH = 128\n",
        "train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train)).shuffle(8192, seed=SEED).batch(BATCH).prefetch(tf.data.AUTOTUNE)\n",
        "val_ds   = tf.data.Dataset.from_tensor_slices((X_val, y_val)).batch(BATCH).prefetch(tf.data.AUTOTUNE)\n",
        "test_ds  = tf.data.Dataset.from_tensor_slices((X_test, y_test)).batch(BATCH).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "# Temporal Intelligence Model with learned retrocausality\n",
        "class TemporalAI(tf.keras.Model):\n",
        "    def __init__(self, n_classes=10, width=64, l2=1e-5, drop=0.2):\n",
        "        super().__init__()\n",
        "        self.past_layer = tf.keras.layers.Conv1D(width, kernel_size=5, padding='same', activation='relu',\n",
        "                                                 kernel_regularizer=tf.keras.regularizers.l2(l2))\n",
        "        self.present_layer = tf.keras.layers.GRU(2*width, return_sequences=False,\n",
        "                                                 kernel_regularizer=tf.keras.regularizers.l2(l2))\n",
        "        self.future_layer = tf.keras.layers.Dense(width, activation='relu',\n",
        "                                                  kernel_regularizer=tf.keras.regularizers.l2(l2))\n",
        "        self.proj_past = tf.keras.layers.Dense(width, activation=None)\n",
        "        self.gate = tf.keras.layers.Dense(width, activation='sigmoid')\n",
        "        self.dropout = tf.keras.layers.Dropout(drop)\n",
        "        self.global_pool = tf.keras.layers.GlobalAveragePooling1D()\n",
        "        self.output_layer = tf.keras.layers.Dense(n_classes, activation='softmax')\n",
        "\n",
        "    def call(self, inputs, training=False):\n",
        "        # Past: local temporal context via Conv1D + pooling\n",
        "        past_feat = self.past_layer(inputs)\n",
        "        past_ctx = self.global_pool(past_feat)          # (batch, width)\n",
        "        past_ctx = self.proj_past(past_ctx)             # align dims\n",
        "\n",
        "        # Present: sequence state via GRU\n",
        "        present_state = self.present_layer(past_feat)   # (batch, 2*width)\n",
        "\n",
        "        # Future: projected hypothesis of what's ahead\n",
        "        future_feat = self.future_layer(present_state)  # (batch, width)\n",
        "\n",
        "        # Learned retrocausal gate\n",
        "        gate_in = tf.concat([past_ctx, future_feat], axis=-1)\n",
        "        g = self.gate(gate_in)                          # (batch, width) in [0,1]\n",
        "        time_optimized = (1.0 - g) * past_ctx + g * future_feat\n",
        "        time_optimized = self.dropout(time_optimized, training=training)\n",
        "\n",
        "        return self.output_layer(time_optimized)\n",
        "\n",
        "model = TemporalAI(n_classes=10, width=64, l2=1e-5, drop=0.2)\n",
        "\n",
        "# Optimizer and compile\n",
        "try:\n",
        "    optimizer = tf.keras.optimizers.AdamW(learning_rate=1e-3, weight_decay=1e-4)\n",
        "except AttributeError:\n",
        "    # Fallback if AdamW is unavailable\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
        "\n",
        "model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Callbacks & logging\n",
        "os.makedirs(\"results\", exist_ok=True)\n",
        "callbacks = [\n",
        "    tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=8, restore_best_weights=True),\n",
        "    tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=3, min_lr=1e-5, verbose=1),\n",
        "    tf.keras.callbacks.CSVLogger(\"results/training_history.csv\"),\n",
        "    tf.keras.callbacks.ModelCheckpoint(\"results/temporal_best.keras\", monitor=\"val_accuracy\",\n",
        "                                       save_best_only=True)\n",
        "]\n",
        "\n",
        "history = model.fit(train_ds, validation_data=val_ds, epochs=60, callbacks=callbacks, verbose=1)\n",
        "\n",
        "# Evaluate\n",
        "test_loss, test_acc = model.evaluate(test_ds, verbose=0)\n",
        "print(f\"✅ Test accuracy: {test_acc:.4f} | Test loss: {test_loss:.4f}\")\n",
        "print(\"✅ Training history saved to results/training_history.csv\")\n",
        "print(\"✅ Best model saved to results/temporal_best.keras\")\n",
        "\n",
        "# Confusion matrix\n",
        "y_pred = model.predict(test_ds, verbose=0).argmax(axis=1)\n",
        "cm = tf.math.confusion_matrix(y_test, y_pred, num_classes=10).numpy()\n",
        "print(\"Confusion matrix:\\n\", cm)\n",
        "\n",
        "# If you prefer to keep your original Dense-only architecture:\n",
        "# X_flat = X.reshape(len(X), -1); use Dense layers on X_flat."
      ]
    }
  ]
}