{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyODcXwRfPtn75T0i12+AfkX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OneFineStarstuff/Cosmic-Brilliance/blob/main/train_wormhole_network_ai_py.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kw5GRn34tP2A"
      },
      "outputs": [],
      "source": [
        "pip install torch numpy matplotlib"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "train_wormhole_network_ai.py\n",
        "\n",
        "Physics‐informed WormholeNetworkAI pipeline:\n",
        "\n",
        " 1. Synthetic dataset of 6 wormhole network parameters → 3 stability factors\n",
        " 2. PINN loss: supervised MSE + toy “energy–topology” residual\n",
        " 3. MLP with LayerNorm & Dropout for uncertainty\n",
        " 4. MC‐Dropout inference to quantify predictive variance\n",
        " 5. Training loop with AdamW, ReduceLROnPlateau, early stopping\n",
        " 6. Visualizations: loss curves, true vs. predicted scatter, uncertainty heatmap\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 1. Synthetic Dataset\n",
        "# ------------------------------------------------------------------------------\n",
        "class WormholeNetworkDataset(Dataset):\n",
        "    def __init__(self, n_samples=5000, seed=0):\n",
        "        np.random.seed(seed)\n",
        "        # Inputs: exotic_energy, topo0, topo1, topo2, var0, var1\n",
        "        X_raw = np.random.uniform(0.1, 10.0, size=(n_samples, 6)).astype(np.float32)\n",
        "        # True stability factors (toy analytic):\n",
        "        #   s0 = energy * topo0\n",
        "        #   s1 = topo1 * topo2\n",
        "        #   s2 = var0 + var1\n",
        "        s0 = X_raw[:,0] * X_raw[:,1]\n",
        "        s1 = X_raw[:,3] * X_raw[:,4]\n",
        "        s2 = X_raw[:,2] + X_raw[:,5]\n",
        "        Y_raw = np.stack([s0, s1, s2], axis=1).astype(np.float32)\n",
        "        Y_raw += 0.05 * np.random.randn(*Y_raw.shape).astype(np.float32)\n",
        "\n",
        "        # Convert to torch\n",
        "        X_t = torch.from_numpy(X_raw)\n",
        "        Y_t = torch.from_numpy(Y_raw)\n",
        "\n",
        "        # Compute normalization stats\n",
        "        self.stats = {\n",
        "            'X_mean': X_t.mean(0), 'X_std': X_t.std(0),\n",
        "            'Y_mean': Y_t.mean(0), 'Y_std': Y_t.std(0),\n",
        "        }\n",
        "\n",
        "        # Normalize\n",
        "        self.X = (X_t - self.stats['X_mean']) / self.stats['X_std']\n",
        "        self.Y = (Y_t - self.stats['Y_mean']) / self.stats['Y_std']\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx], self.Y[idx]\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 2. Model Definition\n",
        "# ------------------------------------------------------------------------------\n",
        "class WormholeNetworkAI(nn.Module):\n",
        "    def __init__(self, input_dim=6, hidden_dims=(64,64), output_dim=3, p_drop=0.1):\n",
        "        super().__init__()\n",
        "        layers, dim = [], input_dim\n",
        "        for h in hidden_dims:\n",
        "            layers += [\n",
        "                nn.Linear(dim, h),\n",
        "                nn.LayerNorm(h),\n",
        "                nn.ReLU(),\n",
        "                nn.Dropout(p_drop)\n",
        "            ]\n",
        "            dim = h\n",
        "        layers.append(nn.Linear(dim, output_dim))\n",
        "        self.net = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 3. Physics‐Informed Loss\n",
        "# ------------------------------------------------------------------------------\n",
        "def physics_residual(pred, inp, stats_torch):\n",
        "    \"\"\"\n",
        "    Enforce toy residual: sum(pred) ≈\n",
        "      energy*topo0 + topo1*topo2 + var0+var1\n",
        "    \"\"\"\n",
        "    # Denormalize\n",
        "    X_den = inp * stats_torch['X_std'] + stats_torch['X_mean']\n",
        "    Y_den = pred * stats_torch['Y_std'] + stats_torch['Y_mean']\n",
        "    analytic = (\n",
        "        X_den[:,0]*X_den[:,1] +\n",
        "        X_den[:,3]*X_den[:,4] +\n",
        "        (X_den[:,2] + X_den[:,5])\n",
        "    )\n",
        "    pred_sum = Y_den.sum(dim=1)\n",
        "    return nn.MSELoss()(pred_sum, analytic)\n",
        "\n",
        "def total_loss(pred, true, inp, stats_torch, lambda_phys=1.0):\n",
        "    mse  = nn.MSELoss()(pred, true)\n",
        "    phys = physics_residual(pred, inp, stats_torch)\n",
        "    return mse + lambda_phys * phys, mse, phys\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 4. MC‐Dropout Inference\n",
        "# ------------------------------------------------------------------------------\n",
        "def mc_dropout_predict(model, x, n_samples=50):\n",
        "    model.train()\n",
        "    preds = []\n",
        "    with torch.no_grad():\n",
        "        for _ in range(n_samples):\n",
        "            preds.append(model(x).cpu().numpy())\n",
        "    arr = np.stack(preds, axis=0)\n",
        "    return arr.mean(axis=0), arr.std(axis=0)\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 5. Training Loop\n",
        "# ------------------------------------------------------------------------------\n",
        "def train_model(model, train_loader, val_loader, stats_torch, device,\n",
        "                lr=1e-3, wd=1e-5, lambda_phys=1.0,\n",
        "                max_epochs=200, patience=20):\n",
        "    model.to(device)\n",
        "    opt = optim.AdamW(model.parameters(), lr=lr, weight_decay=wd)\n",
        "    sched = optim.lr_scheduler.ReduceLROnPlateau(opt, mode='min',\n",
        "                                                factor=0.5, patience=5)\n",
        "\n",
        "    best_val, wait = float('inf'), 0\n",
        "    history = {'train_loss':[], 'val_loss':[]}\n",
        "\n",
        "    for epoch in range(1, max_epochs+1):\n",
        "        # Training\n",
        "        model.train()\n",
        "        tr_loss = 0.0\n",
        "        for xb, yb in train_loader:\n",
        "            xb, yb = xb.to(device), yb.to(device)\n",
        "            pred = model(xb)\n",
        "            loss, _, _ = total_loss(pred, yb, xb, stats_torch, lambda_phys)\n",
        "            opt.zero_grad(); loss.backward(); opt.step()\n",
        "            tr_loss += loss.item() * xb.size(0)\n",
        "        tr_loss /= len(train_loader.dataset)\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        with torch.no_grad():\n",
        "            for xb, yb in val_loader:\n",
        "                xb, yb = xb.to(device), yb.to(device)\n",
        "                pred = model(xb)\n",
        "                loss, _, _ = total_loss(pred, yb, xb, stats_torch, lambda_phys)\n",
        "                val_loss += loss.item() * xb.size(0)\n",
        "        val_loss /= len(val_loader.dataset)\n",
        "\n",
        "        sched.step(val_loss)\n",
        "        history['train_loss'].append(tr_loss)\n",
        "        history['val_loss'].append(val_loss)\n",
        "        print(f\"Epoch {epoch:03d} | Train {tr_loss:.4e} | Val {val_loss:.4e}\")\n",
        "\n",
        "        if val_loss < best_val - 1e-6:\n",
        "            best_val, wait = val_loss, 0\n",
        "            torch.save(model.state_dict(), \"best_wormhole_net_ai.pth\")\n",
        "        else:\n",
        "            wait += 1\n",
        "            if wait >= patience:\n",
        "                print(f\"Early stopping at epoch {epoch}\")\n",
        "                break\n",
        "\n",
        "    model.load_state_dict(torch.load(\"best_wormhole_net_ai.pth\"))\n",
        "    return history\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 6. Visualization Helpers\n",
        "# ------------------------------------------------------------------------------\n",
        "def plot_history(h):\n",
        "    plt.figure()\n",
        "    plt.plot(h['train_loss'], label='Train')\n",
        "    plt.plot(h['val_loss'],   label='Val')\n",
        "    plt.xlabel(\"Epoch\"); plt.ylabel(\"Loss\")\n",
        "    plt.legend(); plt.title(\"Training Curve\")\n",
        "    plt.tight_layout(); plt.show()\n",
        "\n",
        "def plot_scatter(true_vals, pred_vals, label):\n",
        "    plt.figure()\n",
        "    plt.scatter(true_vals, pred_vals, s=5, alpha=0.5)\n",
        "    m, M = true_vals.min(), true_vals.max()\n",
        "    plt.plot([m,M],[m,M],'r--')\n",
        "    plt.xlabel(f\"True {label}\"); plt.ylabel(f\"Pred {label}\")\n",
        "    plt.title(f\"{label}: True vs Pred\")\n",
        "    plt.tight_layout(); plt.show()\n",
        "\n",
        "def plot_uncertainty_heatmap(model, stats_np, device):\n",
        "    # heatmap over exotic_energy vs topology0\n",
        "    E_vals = np.linspace(0.1,10,100)\n",
        "    T0_vals = np.linspace(0.1,10,100)\n",
        "    EE, T0 = np.meshgrid(E_vals, T0_vals)\n",
        "\n",
        "    grid = np.zeros((EE.size, 6), dtype=np.float32)\n",
        "    grid[:,0] = EE.ravel()\n",
        "    grid[:,1] = T0.ravel()\n",
        "    # fix other inputs at their mean\n",
        "    for i in (2,3,4,5):\n",
        "        grid[:,i] = stats_np['X_mean'][i]\n",
        "\n",
        "    Xn = (grid - stats_np['X_mean']) / stats_np['X_std']\n",
        "    Xt = torch.from_numpy(Xn).to(device).float()\n",
        "\n",
        "    _, std = mc_dropout_predict(model, Xt, n_samples=100)\n",
        "    std0 = std[:,0].reshape(EE.shape)\n",
        "\n",
        "    plt.figure(figsize=(6,5))\n",
        "    plt.pcolormesh(EE, T0, std0, shading='auto', cmap='viridis')\n",
        "    plt.colorbar(label=\"Std of stability₀\")\n",
        "    plt.xlabel(\"Exotic Energy\")\n",
        "    plt.ylabel(\"Topology₀\")\n",
        "    plt.title(\"Uncertainty Heatmap (stability₀)\")\n",
        "    plt.tight_layout(); plt.show()\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 7. Main Execution\n",
        "# ------------------------------------------------------------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    ds = WormholeNetworkDataset(n_samples=8000)\n",
        "    # Torch stats for loss\n",
        "    stats_torch = {k:v.to(device) for k,v in ds.stats.items()}\n",
        "    # NumPy stats for plotting\n",
        "    stats_np    = {k:v.cpu().numpy() for k,v in ds.stats.items()}\n",
        "\n",
        "    n_val = int(0.2 * len(ds))\n",
        "    n_trn = len(ds) - n_val\n",
        "    trn, val = random_split(ds, [n_trn, n_val])\n",
        "    trn_ld = DataLoader(trn, batch_size=128, shuffle=True)\n",
        "    val_ld = DataLoader(val, batch_size=256)\n",
        "\n",
        "    model = WormholeNetworkAI().to(device)\n",
        "    history = train_model(\n",
        "        model, trn_ld, val_ld,\n",
        "        stats_torch, device,\n",
        "        lr=1e-3, wd=1e-5, lambda_phys=1.0,\n",
        "        max_epochs=200, patience=20\n",
        "    )\n",
        "\n",
        "    plot_history(history)\n",
        "\n",
        "    # scatter true vs predicted for each stability factor\n",
        "    X_all = ds.X.to(device)\n",
        "    with torch.no_grad():\n",
        "        Y_pred_norm = model(X_all).cpu().numpy()\n",
        "    Y_true_norm = ds.Y.numpy()\n",
        "\n",
        "    # denormalize\n",
        "    Y_pred = Y_pred_norm * stats_np['Y_std'] + stats_np['Y_mean']\n",
        "    Y_true = Y_true_norm * stats_np['Y_std'] + stats_np['Y_mean']\n",
        "\n",
        "    for i, name in enumerate([\"stability₀\",\"stability₁\",\"stability₂\"]):\n",
        "        plot_scatter(Y_true[:,i], Y_pred[:,i], name)\n",
        "\n",
        "    plot_uncertainty_heatmap(model, stats_np, device)"
      ],
      "metadata": {
        "id": "X62PgaLsuNZA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}