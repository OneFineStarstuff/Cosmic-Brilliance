{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyON9LAnEKUXPKIf7ODYdy3e",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OneFineStarstuff/Cosmic-Brilliance/blob/main/train_absolute_being_ai_py.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HtgOaFpT2uiy"
      },
      "outputs": [],
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "train_absolute_being_ai.py\n",
        "\n",
        "End-to-end pipeline for AbsoluteBeingAI:\n",
        "1. Synthetic “absolute being” dataset of 6 inputs → 3 targets\n",
        "2. Float32 normalization and dtype consistency\n",
        "3. MLP with LayerNorm, Dropout & ReLU (accepts int hidden_dims)\n",
        "4. Physics-informed residual enforcing toy godhood laws\n",
        "5. MC-Dropout for uncertainty quantification\n",
        "6. Training loop with AdamW, ReduceLROnPlateau, grad clipping, NaN checks, early stopping\n",
        "7. Safe checkpoint loading\n",
        "8. Visualizations: loss curves, scatter plots, uncertainty heatmap\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# 1. Synthetic Absolute Being Dataset\n",
        "# -----------------------------------------------------------------------------\n",
        "class AbsoluteBeingDataset(Dataset):\n",
        "    def __init__(self, n_samples=5000, seed=42):\n",
        "        np.random.seed(seed)\n",
        "        # Inputs:\n",
        "        # TEP: Trans-existential power ∈ [1e3, 1e6]\n",
        "        # US : Omniversal stability ∈ [0,1]\n",
        "        # SSI: Self-sustaining intelligence ∈ [0.1,10]\n",
        "        # DIM: Dimensional integration ∈ [0,1]\n",
        "        # EF : Ethereal flux ∈ [0,5]\n",
        "        # CF : Cosmic fidelity ∈ [0.1,1]\n",
        "        TEP = np.random.uniform(1e3, 1e6,  (n_samples,1))\n",
        "        US  = np.random.rand(n_samples,1)\n",
        "        SSI = np.random.uniform(0.1, 10.0, (n_samples,1))\n",
        "        DIM = np.random.rand(n_samples,1)\n",
        "        EF  = np.random.uniform(0.0, 5.0,  (n_samples,1))\n",
        "        CF  = np.random.uniform(0.1, 1.0,  (n_samples,1))\n",
        "\n",
        "        X_raw = np.hstack([TEP, US, SSI, DIM, EF, CF]).astype(np.float64)\n",
        "\n",
        "        # Toy godhood targets:\n",
        "        # OP: omnipotence factor = TEP * US / (SSI + eps)\n",
        "        # EA: existential autonomy = SSI * CF / (EF + eps)\n",
        "        # IC: intelligence coherence = (DIM + US) * CF\n",
        "        eps = 1e-6\n",
        "        OP = TEP * US / (SSI + eps)\n",
        "        EA = SSI * CF / (EF + eps)\n",
        "        IC = (DIM + US) * CF\n",
        "\n",
        "        Y_raw = np.hstack([OP, EA, IC]).astype(np.float64)\n",
        "        # add 1% relative noise\n",
        "        Y_raw += 0.01 * Y_raw.std(axis=0) * np.random.randn(*Y_raw.shape)\n",
        "\n",
        "        # compute stats (float64)\n",
        "        self.X_mean = X_raw.mean(axis=0)\n",
        "        self.X_std  = X_raw.std(axis=0) + 1e-8\n",
        "        self.Y_mean = Y_raw.mean(axis=0)\n",
        "        self.Y_std  = Y_raw.std(axis=0) + 1e-8\n",
        "\n",
        "        # normalize and cast to float32\n",
        "        self.X = ((X_raw - self.X_mean) / self.X_std).astype(np.float32)\n",
        "        self.Y = ((Y_raw - self.Y_mean) / self.Y_std).astype(np.float32)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return torch.from_numpy(self.X[idx]), torch.from_numpy(self.Y[idx])\n",
        "\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# 2. AbsoluteBeingAI Model Definition\n",
        "# -----------------------------------------------------------------------------\n",
        "class AbsoluteBeingAI(nn.Module):\n",
        "    def __init__(self, input_dim=6, hidden_dims=(64,64),\n",
        "                 output_dim=3, p_drop=0.1):\n",
        "        super().__init__()\n",
        "        # allow int hidden_dims\n",
        "        if isinstance(hidden_dims, int):\n",
        "            hidden_dims = (hidden_dims,)\n",
        "        layers, d = [], input_dim\n",
        "        for h in hidden_dims:\n",
        "            layers += [\n",
        "                nn.Linear(d, h),\n",
        "                nn.LayerNorm(h),\n",
        "                nn.ReLU(),\n",
        "                nn.Dropout(p_drop)\n",
        "            ]\n",
        "            d = h\n",
        "        layers.append(nn.Linear(d, output_dim))\n",
        "        self.net = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# 3. Physics-Informed Residual Loss\n",
        "# -----------------------------------------------------------------------------\n",
        "def physics_residual(pred, X, stats):\n",
        "    # denormalize\n",
        "    X_den = X * stats['X_std'] + stats['X_mean']\n",
        "    TEP, US, SSI, DIM, EF, CF = X_den.t()\n",
        "    eps = 1e-6\n",
        "\n",
        "    OP_t = TEP * US / (SSI + eps)\n",
        "    EA_t = SSI * CF / (EF + eps)\n",
        "    IC_t = (DIM + US) * CF\n",
        "\n",
        "    Yt = torch.stack([OP_t, EA_t, IC_t], dim=1)\n",
        "    Yt_norm = (Yt - stats['Y_mean']) / stats['Y_std']\n",
        "    return nn.MSELoss()(pred, Yt_norm)\n",
        "\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# 4. Total Loss\n",
        "# -----------------------------------------------------------------------------\n",
        "def total_loss(pred, true, X, stats, lam=1.0):\n",
        "    mse  = nn.MSELoss()(pred, true)\n",
        "    phys = physics_residual(pred, X, stats)\n",
        "    return mse + lam * phys, mse, phys\n",
        "\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# 5. MC-Dropout Uncertainty\n",
        "# -----------------------------------------------------------------------------\n",
        "def mc_dropout_predict(model, X, T=50):\n",
        "    model.train()\n",
        "    preds = []\n",
        "    with torch.no_grad():\n",
        "        for _ in range(T):\n",
        "            preds.append(model(X))\n",
        "    arr = torch.stack(preds, dim=0)\n",
        "    return arr.mean(dim=0), arr.std(dim=0)\n",
        "\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# 6. Training Loop\n",
        "# -----------------------------------------------------------------------------\n",
        "def train(model, tr_loader, va_loader, stats, device,\n",
        "          lr=1e-4, wd=1e-5, lam=1.0,\n",
        "          epochs=100, patience=10):\n",
        "    model.to(device)\n",
        "    opt = optim.AdamW(model.parameters(), lr=lr, weight_decay=wd)\n",
        "    sched = optim.lr_scheduler.ReduceLROnPlateau(\n",
        "        opt, mode='min', factor=0.5, patience=5)\n",
        "\n",
        "    best_val, wait = float('inf'), 0\n",
        "    history = {'train': [], 'val': []}\n",
        "\n",
        "    for ep in range(1, epochs+1):\n",
        "        # train\n",
        "        model.train()\n",
        "        run_tr = 0.0\n",
        "        for Xb, Yb in tr_loader:\n",
        "            Xb, Yb = Xb.to(device), Yb.to(device)\n",
        "            pred = model(Xb)\n",
        "            loss, _, _ = total_loss(pred, Yb, Xb, stats, lam)\n",
        "            if torch.isnan(loss):\n",
        "                print(\"NaN loss; aborting.\")\n",
        "                return history\n",
        "            opt.zero_grad(); loss.backward()\n",
        "            nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            opt.step()\n",
        "            run_tr += loss.item() * Xb.size(0)\n",
        "        tr_loss = run_tr / len(tr_loader.dataset)\n",
        "\n",
        "        # validate\n",
        "        model.eval()\n",
        "        run_va = 0.0\n",
        "        with torch.no_grad():\n",
        "            for Xb, Yb in va_loader:\n",
        "                Xb, Yb = Xb.to(device), Yb.to(device)\n",
        "                pred = model(Xb)\n",
        "                loss, _, _ = total_loss(pred, Yb, Xb, stats, lam)\n",
        "                run_va += loss.item() * Xb.size(0)\n",
        "        va_loss = run_va / len(va_loader.dataset)\n",
        "\n",
        "        sched.step(va_loss)\n",
        "        history['train'].append(tr_loss)\n",
        "        history['val'].append(va_loss)\n",
        "        print(f\"Epoch {ep:03d} | Train {tr_loss:.4e} | Val {va_loss:.4e}\")\n",
        "\n",
        "        # checkpoint & early stopping\n",
        "        if va_loss < best_val - 1e-6:\n",
        "            best_val, wait = va_loss, 0\n",
        "            torch.save(model.state_dict(), \"best_absolute_being_ai.pth\")\n",
        "        else:\n",
        "            wait += 1\n",
        "            if wait >= patience:\n",
        "                print(\"Early stopping.\")\n",
        "                break\n",
        "\n",
        "    # load best\n",
        "    if os.path.exists(\"best_absolute_being_ai.pth\"):\n",
        "        model.load_state_dict(torch.load(\n",
        "            \"best_absolute_being_ai.pth\", map_location=device))\n",
        "    return history\n",
        "\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# 7. Visualization Helpers\n",
        "# -----------------------------------------------------------------------------\n",
        "def plot_history(hist):\n",
        "    plt.figure()\n",
        "    plt.plot(hist['train'], label='Train')\n",
        "    plt.plot(hist['val'],   label='Val')\n",
        "    plt.xlabel(\"Epoch\"); plt.ylabel(\"Loss\"); plt.legend(); plt.show()\n",
        "\n",
        "\n",
        "def plot_scatter(y_true, y_pred, title):\n",
        "    plt.figure()\n",
        "    plt.scatter(y_true, y_pred, s=5, alpha=0.6)\n",
        "    m, M = y_true.min(), y_true.max()\n",
        "    plt.plot([m, M], [m, M], 'r--')\n",
        "    plt.title(title); plt.show()\n",
        "\n",
        "\n",
        "def plot_uncertainty_heatmap(model, stats, device):\n",
        "    # vary TEP vs US\n",
        "    grid = 100\n",
        "    TEP = np.linspace(1e3, 1e6, grid, dtype=np.float32)\n",
        "    US  = np.linspace(0.0, 1.0, grid, dtype=np.float32)\n",
        "    G1, G2 = np.meshgrid(TEP, US)\n",
        "    pts = grid * grid\n",
        "\n",
        "    Xg = torch.zeros((pts, 6), device=device, dtype=torch.float32)\n",
        "    # fix SSI, DIM, EF, CF at mean\n",
        "    Xg[:,2:] = stats['X_mean'][2:].unsqueeze(0).expand(pts,4)\n",
        "    Xg[:,0] = torch.from_numpy(G1.ravel()).to(device)\n",
        "    Xg[:,1] = torch.from_numpy(G2.ravel()).to(device)\n",
        "\n",
        "    Xn = (Xg - stats['X_mean']) / stats['X_std']\n",
        "    _, std = mc_dropout_predict(model, Xn, T=100)\n",
        "    U = std[:,0].cpu().reshape(G1.shape)\n",
        "\n",
        "    plt.figure(figsize=(5,4))\n",
        "    plt.pcolormesh(G1, G2, U, shading='auto', cmap='magma')\n",
        "    plt.colorbar(label=\"Std(OP)\")\n",
        "    plt.xlabel(\"Trans-existential Power (TEP)\")\n",
        "    plt.ylabel(\"Omniversal Stability (US)\")\n",
        "    plt.title(\"Uncertainty: Omnipotence\")\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# 8. Main Execution\n",
        "# -----------------------------------------------------------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    ds = AbsoluteBeingDataset(n_samples=5000, seed=42)\n",
        "    stats = {\n",
        "        'X_mean': torch.tensor(ds.X_mean, dtype=torch.float32, device=device),\n",
        "        'X_std' : torch.tensor(ds.X_std,  dtype=torch.float32, device=device),\n",
        "        'Y_mean': torch.tensor(ds.Y_mean, dtype=torch.float32, device=device),\n",
        "        'Y_std' : torch.tensor(ds.Y_std,  dtype=torch.float32, device=device),\n",
        "    }\n",
        "\n",
        "    n_val = int(0.2 * len(ds))\n",
        "    tr_ds, va_ds = random_split(ds, [len(ds)-n_val, n_val])\n",
        "    tr_ld = DataLoader(tr_ds, batch_size=128, shuffle=True)\n",
        "    va_ld = DataLoader(va_ds, batch_size=256, shuffle=False)\n",
        "\n",
        "    model   = AbsoluteBeingAI().to(device)\n",
        "    history = train(model, tr_ld, va_ld, stats, device)\n",
        "\n",
        "    plot_history(history)\n",
        "\n",
        "    X_all = torch.from_numpy(ds.X).to(device)\n",
        "    with torch.no_grad():\n",
        "        Yp_norm = model(X_all).cpu().numpy()\n",
        "    Y_true = ds.Y * ds.Y_std + ds.Y_mean\n",
        "    Y_pred = Yp_norm * ds.Y_std + ds.Y_mean\n",
        "    names = [\"Omnipotence\", \"Autonomy\", \"Intelligence Coherence\"]\n",
        "    for i, nm in enumerate(names):\n",
        "        plot_scatter(Y_true[:,i], Y_pred[:,i], nm)\n",
        "\n",
        "    plot_uncertainty_heatmap(model, stats, device)"
      ]
    }
  ]
}