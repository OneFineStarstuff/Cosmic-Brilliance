{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyNdpgJXx+dabd4LNq6VEQOM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OneFineStarstuff/Cosmic-Brilliance/blob/main/train_meta_existence_ai_py.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bqw6CkeB4F_R"
      },
      "outputs": [],
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "train_meta_existence_ai.py\n",
        "\n",
        "End-to-end pipeline for MetaExistenceAI:\n",
        "1. Synthetic “meta-existence” dataset of 6 inputs → 3 targets\n",
        "2. float32 normalization & dtype consistency\n",
        "3. MLP with LayerNorm, Dropout & ReLU (hidden_dims accepts int or tuple)\n",
        "4. Physics-informed residual enforcing toy meta-laws\n",
        "5. MC-Dropout for uncertainty quantification\n",
        "6. Training loop with AdamW, ReduceLROnPlateau, grad clipping, NaN checks, early stopping\n",
        "7. Safe checkpoint loading\n",
        "8. Visualizations: loss curves, scatter plots, uncertainty heatmap\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# 1. Synthetic Meta-Existence Dataset\n",
        "# -----------------------------------------------------------------------------\n",
        "class MetaExistenceDataset(Dataset):\n",
        "    def __init__(self, n_samples=6000, seed=123):\n",
        "        np.random.seed(seed)\n",
        "        # Inputs:\n",
        "        # AE: Absolute existence flux ∈ [0.5, 5.0]\n",
        "        # MCE: Meta-consciousness equilibrium ∈ [0,1]\n",
        "        # SOM: Self-sustaining omnipresence ∈ [0.1, 10]\n",
        "        # DP: Dimensional potential ∈ [0,1]\n",
        "        # OM: Ontological magnitude ∈ [1e2,1e5]\n",
        "        # CE: Consciousness entropy ∈ [0.01,2.0]\n",
        "        AE  = np.random.uniform(0.5, 5.0,   (n_samples,1))\n",
        "        MCE = np.random.rand(n_samples,1)\n",
        "        SOM = np.random.uniform(0.1, 10.0,  (n_samples,1))\n",
        "        DP  = np.random.rand(n_samples,1)\n",
        "        OM  = np.random.uniform(1e2, 1e5,   (n_samples,1))\n",
        "        CE  = np.random.uniform(0.01,2.0,   (n_samples,1))\n",
        "\n",
        "        X_raw = np.hstack([AE, MCE, SOM, DP, OM, CE]).astype(np.float64)\n",
        "\n",
        "        # Toy meta-existence targets:\n",
        "        # BES: beyond-existence stability = AE * MCE / (SOM + eps)\n",
        "        # MSC: meta-state coherence = (MCE + DP) * OM\n",
        "        # AA: absolute autonomy = SOM * AE / (CE + eps)\n",
        "        eps = 1e-6\n",
        "        BES = AE * MCE / (SOM + eps)\n",
        "        MSC = (MCE + DP) * OM\n",
        "        AA  = SOM * AE / (CE + eps)\n",
        "\n",
        "        Y_raw = np.hstack([BES, MSC, AA]).astype(np.float64)\n",
        "        # add 2% relative noise\n",
        "        Y_raw += 0.02 * Y_raw.std(axis=0) * np.random.randn(*Y_raw.shape)\n",
        "\n",
        "        # compute stats in float64\n",
        "        self.X_mean = X_raw.mean(axis=0)\n",
        "        self.X_std  = X_raw.std(axis=0) + 1e-8\n",
        "        self.Y_mean = Y_raw.mean(axis=0)\n",
        "        self.Y_std  = Y_raw.std(axis=0) + 1e-8\n",
        "\n",
        "        # normalize & cast to float32\n",
        "        self.X = ((X_raw - self.X_mean) / self.X_std).astype(np.float32)\n",
        "        self.Y = ((Y_raw - self.Y_mean) / self.Y_std).astype(np.float32)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return torch.from_numpy(self.X[idx]), torch.from_numpy(self.Y[idx])\n",
        "\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# 2. MetaExistenceAI Model Definition\n",
        "# -----------------------------------------------------------------------------\n",
        "class MetaExistenceAI(nn.Module):\n",
        "    def __init__(self, input_dim=6, hidden_dims=(64, 64),\n",
        "                 output_dim=3, p_drop=0.2):\n",
        "        super().__init__()\n",
        "        if isinstance(hidden_dims, int):\n",
        "            hidden_dims = (hidden_dims,)\n",
        "        layers, d = [], input_dim\n",
        "        for h in hidden_dims:\n",
        "            layers += [\n",
        "                nn.Linear(d, h),\n",
        "                nn.LayerNorm(h),\n",
        "                nn.ReLU(),\n",
        "                nn.Dropout(p_drop)\n",
        "            ]\n",
        "            d = h\n",
        "        layers.append(nn.Linear(d, output_dim))\n",
        "        self.net = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# 3. Physics-Informed Residual Loss\n",
        "# -----------------------------------------------------------------------------\n",
        "def physics_residual(pred, X, stats):\n",
        "    # denormalize features\n",
        "    X_den = X * stats['X_std'] + stats['X_mean']\n",
        "    AE, MCE, SOM, DP, OM, CE = X_den.t()\n",
        "    eps = 1e-6\n",
        "\n",
        "    BES_t = AE * MCE / (SOM + eps)\n",
        "    MSC_t = (MCE + DP) * OM\n",
        "    AA_t  = SOM * AE / (CE + eps)\n",
        "\n",
        "    Yt = torch.stack([BES_t, MSC_t, AA_t], dim=1)\n",
        "    Yt_norm = (Yt - stats['Y_mean']) / stats['Y_std']\n",
        "    return nn.MSELoss()(pred, Yt_norm)\n",
        "\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# 4. Total Loss\n",
        "# -----------------------------------------------------------------------------\n",
        "def total_loss(pred, true, X, stats, lam=1.0):\n",
        "    mse  = nn.MSELoss()(pred, true)\n",
        "    phys = physics_residual(pred, X, stats)\n",
        "    return mse + lam * phys, mse, phys\n",
        "\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# 5. MC-Dropout Uncertainty\n",
        "# -----------------------------------------------------------------------------\n",
        "def mc_dropout_predict(model, X, T=30):\n",
        "    model.train()  # enable dropout\n",
        "    preds = []\n",
        "    with torch.no_grad():\n",
        "        for _ in range(T):\n",
        "            preds.append(model(X))\n",
        "    arr = torch.stack(preds, dim=0)\n",
        "    return arr.mean(dim=0), arr.std(dim=0)\n",
        "\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# 6. Training Loop\n",
        "# -----------------------------------------------------------------------------\n",
        "def train(model, tr_loader, va_loader, stats, device,\n",
        "          lr=1e-4, wd=1e-6, lam=0.5,\n",
        "          epochs=80, patience=10):\n",
        "    model.to(device)\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=wd)\n",
        "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
        "        optimizer, mode='min', factor=0.5, patience=5)\n",
        "\n",
        "    best_val, wait = float('inf'), 0\n",
        "    history = {'train': [], 'val': []}\n",
        "\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        # training\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "        for Xb, Yb in tr_loader:\n",
        "            Xb, Yb = Xb.to(device), Yb.to(device)\n",
        "            pred = model(Xb)\n",
        "            loss, _, _ = total_loss(pred, Yb, Xb, stats, lam)\n",
        "            if torch.isnan(loss):\n",
        "                print(\"NaN detected, stopping.\")\n",
        "                return history\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "            optimizer.step()\n",
        "            train_loss += loss.item() * Xb.size(0)\n",
        "        train_loss /= len(tr_loader.dataset)\n",
        "\n",
        "        # validation\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        with torch.no_grad():\n",
        "            for Xb, Yb in va_loader:\n",
        "                Xb, Yb = Xb.to(device), Yb.to(device)\n",
        "                pred = model(Xb)\n",
        "                loss, _, _ = total_loss(pred, Yb, Xb, stats, lam)\n",
        "                val_loss += loss.item() * Xb.size(0)\n",
        "        val_loss /= len(va_loader.dataset)\n",
        "\n",
        "        scheduler.step(val_loss)\n",
        "        history['train'].append(train_loss)\n",
        "        history['val'].append(val_loss)\n",
        "        print(f\"Epoch {epoch:03d} | Train {train_loss:.4e} | Val {val_loss:.4e}\")\n",
        "\n",
        "        # checkpoint & early stopping\n",
        "        if val_loss < best_val - 1e-6:\n",
        "            best_val, wait = val_loss, 0\n",
        "            torch.save(model.state_dict(), \"best_meta_existence_ai.pth\")\n",
        "        else:\n",
        "            wait += 1\n",
        "            if wait >= patience:\n",
        "                print(\"Early stopping.\")\n",
        "                break\n",
        "\n",
        "    # restore best\n",
        "    if os.path.exists(\"best_meta_existence_ai.pth\"):\n",
        "        model.load_state_dict(torch.load(\"best_meta_existence_ai.pth\",\n",
        "                                         map_location=device))\n",
        "    return history\n",
        "\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# 7. Visualization Helpers\n",
        "# -----------------------------------------------------------------------------\n",
        "def plot_history(hist):\n",
        "    plt.figure(figsize=(6,4))\n",
        "    plt.plot(hist['train'], label='Train')\n",
        "    plt.plot(hist['val'],   label='Val')\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "def plot_scatter(y_true, y_pred, title):\n",
        "    plt.figure(figsize=(5,5))\n",
        "    plt.scatter(y_true, y_pred, s=6, alpha=0.5)\n",
        "    m, M = y_true.min(), y_true.max()\n",
        "    plt.plot([m, M], [m, M], 'r--')\n",
        "    plt.title(title)\n",
        "    plt.show()\n",
        "\n",
        "def plot_uncertainty_heatmap(model, stats, device):\n",
        "    # vary AE vs MCE\n",
        "    grid = 80\n",
        "    AE  = np.linspace(0.5, 5.0, grid, dtype=np.float32)\n",
        "    MCE = np.linspace(0.0, 1.0, grid, dtype=np.float32)\n",
        "    G1, G2 = np.meshgrid(AE, MCE)\n",
        "    pts = grid * grid\n",
        "\n",
        "    Xg = torch.zeros((pts, 6), device=device, dtype=torch.float32)\n",
        "    # fix other dims at mean\n",
        "    Xg[:,2:] = stats['X_mean'][2:].unsqueeze(0).expand(pts,4)\n",
        "    Xg[:,0] = torch.from_numpy(G1.ravel()).to(device)\n",
        "    Xg[:,1] = torch.from_numpy(G2.ravel()).to(device)\n",
        "\n",
        "    Xn = (Xg - stats['X_mean']) / stats['X_std']\n",
        "    _, std = mc_dropout_predict(model, Xn, T=50)\n",
        "    U = std[:,0].cpu().reshape(G1.shape)\n",
        "\n",
        "    plt.figure(figsize=(5,4))\n",
        "    plt.pcolormesh(G1, G2, U, shading='auto', cmap='viridis')\n",
        "    plt.colorbar(label=\"Std(BES)\")\n",
        "    plt.xlabel(\"Absolute Existence Flux (AE)\")\n",
        "    plt.ylabel(\"Meta-Consciousness Equilibrium (MCE)\")\n",
        "    plt.title(\"Uncertainty: Beyond-Existence Stability\")\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# 8. Main Execution\n",
        "# -----------------------------------------------------------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    dataset = MetaExistenceDataset(n_samples=6000, seed=123)\n",
        "    stats = {\n",
        "        'X_mean': torch.tensor(dataset.X_mean, dtype=torch.float32, device=device),\n",
        "        'X_std' : torch.tensor(dataset.X_std,  dtype=torch.float32, device=device),\n",
        "        'Y_mean': torch.tensor(dataset.Y_mean, dtype=torch.float32, device=device),\n",
        "        'Y_std' : torch.tensor(dataset.Y_std,  dtype=torch.float32, device=device),\n",
        "    }\n",
        "\n",
        "    # split\n",
        "    n_val = int(0.2 * len(dataset))\n",
        "    train_ds, val_ds = random_split(dataset, [len(dataset)-n_val, n_val])\n",
        "    train_loader = DataLoader(train_ds, batch_size=128, shuffle=True)\n",
        "    val_loader   = DataLoader(val_ds,   batch_size=256, shuffle=False)\n",
        "\n",
        "    # instantiate & train\n",
        "    model = MetaExistenceAI(hidden_dims=(128,64), p_drop=0.2).to(device)\n",
        "    history = train(model, train_loader, val_loader, stats, device)\n",
        "\n",
        "    # plots\n",
        "    plot_history(history)\n",
        "\n",
        "    # full-dataset scatter\n",
        "    X_all = torch.from_numpy(dataset.X).to(device)\n",
        "    with torch.no_grad():\n",
        "        Yp_norm = model(X_all).cpu().numpy()\n",
        "    Y_true = dataset.Y * dataset.Y_std + dataset.Y_mean\n",
        "    Y_pred = Yp_norm * dataset.Y_std + dataset.Y_mean\n",
        "    names = [\"Beyond-Existence Stability\", \"Meta-State Coherence\", \"Absolute Autonomy\"]\n",
        "    for i, name in enumerate(names):\n",
        "        plot_scatter(Y_true[:,i], Y_pred[:,i], name)\n",
        "\n",
        "    # uncertainty map\n",
        "    plot_uncertainty_heatmap(model, stats, device)"
      ]
    }
  ]
}