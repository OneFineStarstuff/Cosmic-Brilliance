{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyNboRkHZiTIUgWAZHAqGskv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OneFineStarstuff/Cosmic-Brilliance/blob/main/train_multiverse_ai_py.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PVS1S-_Blx4F"
      },
      "outputs": [],
      "source": [
        "pip install torch numpy matplotlib"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "train_multiverse_ai.py\n",
        "\n",
        "Physics‐informed MultiverseAI pipeline with separate Torch/NumPy stats:\n",
        "\n",
        " 1. Synthetic dataset for (energy, prob, variance, coupling) →\n",
        "    (transition_prob, stability_metric)\n",
        " 2. PINN loss combining data MSE and residuals enforcing analytic toy‐physics\n",
        " 3. MLP with LayerNorm & Dropout for uncertainty quantification\n",
        " 4. MC‐Dropout inference at test time\n",
        " 5. Training loop with AdamW, ReduceLROnPlateau, early stopping\n",
        " 6. Plots: training curves, scatter true vs predicted, and uncertainty heatmap\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 1. Synthetic Dataset\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def analytic_transition_probability(E, v, var):\n",
        "    return torch.sigmoid(- (E * var))\n",
        "\n",
        "def analytic_stability(p, var, cpl):\n",
        "    return p * torch.exp(- var * cpl)\n",
        "\n",
        "class MultiverseDataset(Dataset):\n",
        "    def __init__(self, n_samples=8000, seed=0):\n",
        "        torch.manual_seed(seed)\n",
        "        # Sample ranges\n",
        "        E   = torch.rand(n_samples, 1) * 10.0\n",
        "        p   = torch.rand(n_samples, 1)\n",
        "        var = torch.rand(n_samples, 1)\n",
        "        cpl = torch.rand(n_samples, 1) * 5.0\n",
        "\n",
        "        X_raw = torch.cat([E, p, var, cpl], dim=1)\n",
        "        y1 = analytic_transition_probability(E, var, var)\n",
        "        y2 = analytic_stability(p, var, cpl)\n",
        "        Y_raw = torch.cat([y1, y2], dim=1) + 0.02 * torch.randn(n_samples, 2)\n",
        "\n",
        "        # keep raw PyTorch stats here\n",
        "        self.stats = {\n",
        "            'X_mean': X_raw.mean(0),\n",
        "            'X_std':  X_raw.std(0),\n",
        "            'Y_mean': Y_raw.mean(0),\n",
        "            'Y_std':  Y_raw.std(0),\n",
        "        }\n",
        "\n",
        "        # normalize\n",
        "        self.X = (X_raw - self.stats['X_mean']) / self.stats['X_std']\n",
        "        self.Y = (Y_raw - self.stats['Y_mean']) / self.stats['Y_std']\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx], self.Y[idx]\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 2. Model Definition\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "class MultiverseAI(nn.Module):\n",
        "    def __init__(self, input_dim=4, hidden_dims=(64,64), output_dim=2, p_drop=0.1):\n",
        "        super().__init__()\n",
        "        layers, dim = [], input_dim\n",
        "        for h in hidden_dims:\n",
        "            layers += [\n",
        "                nn.Linear(dim, h),\n",
        "                nn.LayerNorm(h),\n",
        "                nn.ReLU(),\n",
        "                nn.Dropout(p_drop)\n",
        "            ]\n",
        "            dim = h\n",
        "        layers.append(nn.Linear(dim, output_dim))\n",
        "        self.net = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 3. Physics‐Informed Loss\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def physics_residual(pred, inp, stats_torch):\n",
        "    # Denormalize predictions and inputs (all Torch tensors on same device)\n",
        "    Y_den = pred * stats_torch['Y_std'] + stats_torch['Y_mean']\n",
        "    X_den = inp  * stats_torch['X_std'] + stats_torch['X_mean']\n",
        "    E, p, var, cpl = X_den[:,0], X_den[:,1], X_den[:,2], X_den[:,3]\n",
        "\n",
        "    y1_pred = Y_den[:,0]\n",
        "    y2_pred = Y_den[:,1]\n",
        "    y1_true = analytic_transition_probability(E, var, var).detach()\n",
        "    y2_true = analytic_stability(p, var, cpl).detach()\n",
        "\n",
        "    return nn.MSELoss()(y1_pred, y1_true) + nn.MSELoss()(y2_pred, y2_true)\n",
        "\n",
        "def total_loss(pred, true, inp, stats_torch, lambda_phys=1.0):\n",
        "    mse_loss  = nn.MSELoss()(pred, true)\n",
        "    phys_loss = physics_residual(pred, inp, stats_torch)\n",
        "    return mse_loss + lambda_phys * phys_loss, mse_loss, phys_loss\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 4. MC‐Dropout Inference\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def mc_dropout_predict(model, x, n_samples=50):\n",
        "    model.train()  # activate dropout\n",
        "    preds = []\n",
        "    with torch.no_grad():\n",
        "        for _ in range(n_samples):\n",
        "            preds.append(model(x).cpu().numpy())\n",
        "    arr = np.stack(preds, axis=0)\n",
        "    return arr.mean(axis=0), arr.std(axis=0)\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 5. Training Loop\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def train_model(model, train_loader, val_loader, stats_torch, device,\n",
        "                lr=1e-3, weight_decay=1e-5, lambda_phys=1.0,\n",
        "                max_epochs=200, patience=20):\n",
        "    model.to(device)\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
        "        optimizer, mode='min', factor=0.5, patience=5\n",
        "    )\n",
        "\n",
        "    best_val, wait = float('inf'), 0\n",
        "    history = {'train_loss': [], 'val_loss': []}\n",
        "\n",
        "    for epoch in range(1, max_epochs+1):\n",
        "        # --- Training ---\n",
        "        model.train()\n",
        "        running_train = 0.0\n",
        "        for xb, yb in train_loader:\n",
        "            xb, yb = xb.to(device), yb.to(device)\n",
        "            pred = model(xb)\n",
        "            loss, _, _ = total_loss(pred, yb, xb, stats_torch, lambda_phys)\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_train += loss.item() * xb.size(0)\n",
        "        train_loss = running_train / len(train_loader.dataset)\n",
        "\n",
        "        # --- Validation ---\n",
        "        model.eval()\n",
        "        running_val = 0.0\n",
        "        with torch.no_grad():\n",
        "            for xb, yb in val_loader:\n",
        "                xb, yb = xb.to(device), yb.to(device)\n",
        "                pred = model(xb)\n",
        "                loss, _, _ = total_loss(pred, yb, xb, stats_torch, lambda_phys)\n",
        "                running_val += loss.item() * xb.size(0)\n",
        "        val_loss = running_val / len(val_loader.dataset)\n",
        "\n",
        "        scheduler.step(val_loss)\n",
        "        history['train_loss'].append(train_loss)\n",
        "        history['val_loss'].append(val_loss)\n",
        "        print(f\"Epoch {epoch:03d} | Train {train_loss:.4e} | Val {val_loss:.4e}\")\n",
        "\n",
        "        # Early stopping\n",
        "        if val_loss < best_val - 1e-6:\n",
        "            best_val, wait = val_loss, 0\n",
        "            torch.save(model.state_dict(), \"best_multiverse_ai.pth\")\n",
        "        else:\n",
        "            wait += 1\n",
        "            if wait >= patience:\n",
        "                print(f\"Early stopping at epoch {epoch}\")\n",
        "                break\n",
        "\n",
        "    # Load best model\n",
        "    model.load_state_dict(torch.load(\"best_multiverse_ai.pth\"))\n",
        "    return history\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 6. Visualization Helpers\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def plot_history(hist):\n",
        "    plt.plot(hist['train_loss'], label='Train Loss')\n",
        "    plt.plot(hist['val_loss'],   label='Val Loss')\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.legend()\n",
        "    plt.title(\"Training Curve\")\n",
        "    plt.show()\n",
        "\n",
        "def plot_scatter(true_vals, pred_vals, name):\n",
        "    plt.scatter(true_vals, pred_vals, s=5, alpha=0.5)\n",
        "    m, M = true_vals.min(), true_vals.max()\n",
        "    plt.plot([m, M], [m, M], 'r--')\n",
        "    plt.xlabel(f\"True {name}\")\n",
        "    plt.ylabel(f\"Pred {name}\")\n",
        "    plt.title(f\"{name}: True vs Pred\")\n",
        "    plt.show()\n",
        "\n",
        "def plot_uncertainty_heatmap(model, stats_np, device):\n",
        "    E_vals   = np.linspace(0,10,100)\n",
        "    var_vals = np.linspace(0,1,100)\n",
        "    P, C     = 0.5, 2.5\n",
        "\n",
        "    EE, VV = np.meshgrid(E_vals, var_vals)\n",
        "    # Normalize with NumPy stats\n",
        "    Xgrid = np.stack([\n",
        "        (EE - stats_np['X_mean'][0]) / stats_np['X_std'][0],\n",
        "        (np.full_like(EE, P) - stats_np['X_mean'][1]) / stats_np['X_std'][1],\n",
        "        (VV - stats_np['X_mean'][2]) / stats_np['X_std'][2],\n",
        "        (np.full_like(EE, C) - stats_np['X_mean'][3]) / stats_np['X_std'][3],\n",
        "    ], axis=-1).reshape(-1,4)\n",
        "\n",
        "    X_tensor = torch.from_numpy(Xgrid).float().to(device)\n",
        "    _, std = mc_dropout_predict(model, X_tensor, n_samples=50)\n",
        "    std1 = std[:,0].reshape(EE.shape)\n",
        "\n",
        "    plt.pcolormesh(E_vals, var_vals, std1, shading='auto', cmap='viridis')\n",
        "    plt.colorbar(label=\"Std of y1_pred\")\n",
        "    plt.xlabel(\"Energy\")\n",
        "    plt.ylabel(\"Variance\")\n",
        "    plt.title(\"Prediction Uncertainty (y1)\")\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 7. Main Execution\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # Prepare data\n",
        "    dataset = MultiverseDataset(n_samples=8000)\n",
        "\n",
        "    # Extract Torch stats for training (on correct device)\n",
        "    stats_torch = {\n",
        "        k: v.to(device)\n",
        "        for k, v in dataset.stats.items()\n",
        "    }\n",
        "\n",
        "    # Extract NumPy stats for plotting\n",
        "    stats_np = {\n",
        "        k: v.cpu().numpy()\n",
        "        for k, v in dataset.stats.items()\n",
        "    }\n",
        "\n",
        "    # Split and loaders\n",
        "    n_val       = int(0.2 * len(dataset))\n",
        "    n_train     = len(dataset) - n_val\n",
        "    train_ds, val_ds = random_split(dataset, [n_train, n_val])\n",
        "    train_loader     = DataLoader(train_ds, batch_size=128, shuffle=True)\n",
        "    val_loader       = DataLoader(val_ds,   batch_size=256)\n",
        "\n",
        "    # Build & train\n",
        "    model   = MultiverseAI().to(device)\n",
        "    history = train_model(\n",
        "        model, train_loader, val_loader,\n",
        "        stats_torch, device,\n",
        "        lr=1e-3, weight_decay=1e-5, lambda_phys=1.0,\n",
        "        max_epochs=200, patience=20\n",
        "    )\n",
        "\n",
        "    # Visualize\n",
        "    plot_history(history)\n",
        "\n",
        "    # Scatter true vs. pred\n",
        "    X_all = dataset.X.to(device)\n",
        "    with torch.no_grad():\n",
        "        Y_pred_norm = model(X_all).cpu().numpy()\n",
        "    Y_true_norm = dataset.Y.numpy()\n",
        "\n",
        "    Y_pred = Y_pred_norm * stats_np['Y_std'] + stats_np['Y_mean']\n",
        "    Y_true = Y_true_norm * stats_np['Y_std'] + stats_np['Y_mean']\n",
        "\n",
        "    plot_scatter(Y_true[:,0], Y_pred[:,0], \"Transition Probability\")\n",
        "    plot_scatter(Y_true[:,1], Y_pred[:,1], \"Stability Metric\")\n",
        "\n",
        "    plot_uncertainty_heatmap(model, stats_np, device)"
      ],
      "metadata": {
        "id": "uhvdDbRhpRE5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}