{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyMMHrIQcLk2KcPcuXeQtEQq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OneFineStarstuff/Cosmic-Brilliance/blob/main/self_referential_training_that_runs_on_TF_2_x_(Python_3_9%E2%80%933_11).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zGL-YXXAzLyc"
      },
      "outputs": [],
      "source": [
        "pip install --upgrade pip setuptools wheel"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "# -*- coding: utf-8 -*-\n",
        "\n",
        "import os\n",
        "import math\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "class MetaIntelligence(tf.keras.Model):\n",
        "    \"\"\"Simple 4->16->4 network that supports self-modifying updates.\"\"\"\n",
        "\n",
        "    def __init__(self, hidden=16):\n",
        "        super().__init__()\n",
        "        self.d1 = tf.keras.layers.Dense(hidden, activation=\"relu\",\n",
        "                                        kernel_initializer=\"he_normal\")\n",
        "        # Return logits; we’ll apply softmax in the loss for control.\n",
        "        self.d2 = tf.keras.layers.Dense(4, activation=None,\n",
        "                                        kernel_initializer=\"glorot_uniform\")\n",
        "\n",
        "    def call(self, inputs, training=False):\n",
        "        x = self.d1(inputs)\n",
        "        logits = self.d2(x)\n",
        "        return logits\n",
        "\n",
        "\n",
        "def l2_weight_decay(model, weight=1e-4):\n",
        "    reg = tf.add_n([tf.nn.l2_loss(v) for v in model.trainable_variables])\n",
        "    return weight * reg\n",
        "\n",
        "\n",
        "@tf.function\n",
        "def self_modify_step(model,\n",
        "                     optimizer,\n",
        "                     batch_size=128,\n",
        "                     wd=1e-4,\n",
        "                     entropy_bonus=0.0,\n",
        "                     label_sharpen=0.0):\n",
        "    \"\"\"\n",
        "    One self-referential update using pseudo-labeling:\n",
        "      1) Draw random inputs.\n",
        "      2) Predict logits -> probs.\n",
        "      3) Create pseudo-labels via argmax (optionally sharpen).\n",
        "      4) Minimize CE(pseudo, probs) + weight_decay - entropy_bonus * H(probs).\n",
        "    \"\"\"\n",
        "    # Random “experience” — replace with real data if available.\n",
        "    x = tf.random.normal([batch_size, 4], dtype=tf.float32)\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "        logits = model(x, training=True)\n",
        "        probs = tf.nn.softmax(logits, axis=-1)\n",
        "\n",
        "        # Pseudo-labels: stop gradient so the target is fixed in this step.\n",
        "        pseudo_idx = tf.argmax(probs, axis=-1)\n",
        "        pseudo = tf.one_hot(pseudo_idx, depth=4)\n",
        "        pseudo = tf.stop_gradient(pseudo)\n",
        "\n",
        "        # Optional temperature sharpening: move pseudo toward sharper targets.\n",
        "        if label_sharpen > 0.0:\n",
        "            sharpened = tf.pow(probs + 1e-8, 1.0 - label_sharpen)\n",
        "            sharpened = sharpened / tf.reduce_sum(sharpened, axis=-1, keepdims=True)\n",
        "            pseudo = tf.stop_gradient(sharpened)\n",
        "\n",
        "        ce = tf.keras.losses.categorical_crossentropy(pseudo, probs, from_logits=False)\n",
        "        ce = tf.reduce_mean(ce)\n",
        "\n",
        "        # Encourage confident predictions (via CE); optionally add entropy bonus to counter-collapse.\n",
        "        entropy = -tf.reduce_mean(tf.reduce_sum(probs * tf.math.log(probs + 1e-8), axis=-1))\n",
        "        reg = l2_weight_decay(model, weight=wd)\n",
        "\n",
        "        loss = ce + reg - entropy_bonus * entropy\n",
        "\n",
        "    grads = tape.gradient(loss, model.trainable_variables)\n",
        "\n",
        "    # Safety: replace None grads with zeros; clip to avoid explosions.\n",
        "    safe_grads = []\n",
        "    for g, v in zip(grads, model.trainable_variables):\n",
        "        if g is None:\n",
        "            safe_grads.append(tf.zeros_like(v))\n",
        "        else:\n",
        "            safe_grads.append(g)\n",
        "    safe_grads, _ = tf.clip_by_global_norm(safe_grads, 1.0)\n",
        "\n",
        "    optimizer.apply_gradients(zip(safe_grads, model.trainable_variables))\n",
        "\n",
        "    # Diagnostics\n",
        "    max_conf = tf.reduce_mean(tf.reduce_max(probs, axis=-1))\n",
        "    return {\n",
        "        \"loss\": loss,\n",
        "        \"ce\": ce,\n",
        "        \"entropy\": entropy,\n",
        "        \"max_conf\": max_conf\n",
        "    }\n",
        "\n",
        "\n",
        "@tf.function\n",
        "def supervised_step(model, optimizer, x, y, wd=1e-4):\n",
        "    \"\"\"Standard supervised update for comparison.\"\"\"\n",
        "    with tf.GradientTape() as tape:\n",
        "        logits = model(x, training=True)\n",
        "        loss = tf.reduce_mean(\n",
        "            tf.keras.losses.sparse_categorical_crossentropy(y, logits, from_logits=True)\n",
        "        )\n",
        "        loss += l2_weight_decay(model, weight=wd)\n",
        "    grads = tape.gradient(loss, model.trainable_variables)\n",
        "    grads, _ = tf.clip_by_global_norm(grads, 1.0)\n",
        "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "    return loss\n",
        "\n",
        "\n",
        "def run_self_modification(steps=200, batch_size=256, lr=1e-3, wd=1e-4,\n",
        "                          entropy_bonus=0.0, label_sharpen=0.0, seed=42):\n",
        "    tf.random.set_seed(seed)\n",
        "    np.random.seed(seed)\n",
        "\n",
        "    model = MetaIntelligence(hidden=16)\n",
        "    optimizer = tf.keras.optimizers.Adam(lr)\n",
        "\n",
        "    # Build model by running a dummy forward pass\n",
        "    _ = model(tf.zeros([1, 4], dtype=tf.float32))\n",
        "\n",
        "    print(\"Starting self-modification...\")\n",
        "    for t in range(1, steps + 1):\n",
        "        metrics = self_modify_step(\n",
        "            model, optimizer,\n",
        "            batch_size=batch_size,\n",
        "            wd=wd,\n",
        "            entropy_bonus=entropy_bonus,\n",
        "            label_sharpen=label_sharpen\n",
        "        )\n",
        "        if t % max(1, steps // 10) == 0 or t == 1:\n",
        "            print(f\"[{t:04d}] loss={metrics['loss']:.4f} \"\n",
        "                  f\"ce={metrics['ce']:.4f} \"\n",
        "                  f\"H={metrics['entropy']:.4f} \"\n",
        "                  f\"max_conf={metrics['max_conf']:.3f}\")\n",
        "\n",
        "    # Quick probe on random inputs\n",
        "    x_probe = tf.random.normal([8, 4])\n",
        "    logits = model(x_probe, training=False)\n",
        "    probs = tf.nn.softmax(logits, axis=-1)\n",
        "    preds = tf.argmax(probs, axis=-1)\n",
        "    print(\"Sample probs:\\n\", np.round(probs.numpy(), 3))\n",
        "    print(\"Preds:\", preds.numpy().tolist())\n",
        "    return model\n",
        "\n",
        "\n",
        "def run_supervised_demo(steps=200, batch_size=256, lr=1e-3, wd=1e-4, seed=7):\n",
        "    \"\"\"Optional: shows standard supervised updates on a synthetic rule.\"\"\"\n",
        "    tf.random.set_seed(seed)\n",
        "    np.random.seed(seed)\n",
        "\n",
        "    model = MetaIntelligence(hidden=16)\n",
        "    optimizer = tf.keras.optimizers.Adam(lr)\n",
        "    _ = model(tf.zeros([1, 4], dtype=tf.float32))\n",
        "\n",
        "    def synth_data(n):\n",
        "        x = tf.random.normal([n, 4])\n",
        "        # Label is index of max absolute feature (simple, learnable rule).\n",
        "        y = tf.argmax(tf.abs(x), axis=-1, output_type=tf.int32)\n",
        "        return x, y\n",
        "\n",
        "    print(\"Starting supervised demo...\")\n",
        "    for t in range(1, steps + 1):\n",
        "        x, y = synth_data(batch_size)\n",
        "        loss = supervised_step(model, optimizer, x, y, wd=wd)\n",
        "        if t % max(1, steps // 10) == 0 or t == 1:\n",
        "            print(f\"[{t:04d}] sup_loss={loss:.4f}\")\n",
        "\n",
        "    # Evaluate\n",
        "    x_eval, y_eval = synth_data(512)\n",
        "    logits = model(x_eval, training=False)\n",
        "    preds = tf.argmax(logits, axis=-1, output_type=tf.int32)\n",
        "    acc = tf.reduce_mean(tf.cast(tf.equal(preds, y_eval), tf.float32))\n",
        "    print(f\"Supervised accuracy: {acc.numpy():.3f}\")\n",
        "    return model\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Choose one:\n",
        "    # 1) Pure self-referential updating (pseudo-label bootstrapping).\n",
        "    model = run_self_modification(\n",
        "        steps=300,\n",
        "        batch_size=256,\n",
        "        lr=1e-3,\n",
        "        wd=1e-4,\n",
        "        entropy_bonus=0.0,     # Try 0.05 to encourage exploration\n",
        "        label_sharpen=0.0,     # Try 0.2 to softly sharpen pseudo-labels\n",
        "        seed=42\n",
        "    )\n",
        "\n",
        "    # 2) Or run a supervised sanity check:\n",
        "    # model = run_supervised_demo(steps=300, batch_size=256, lr=1e-3, wd=1e-4, seed=7)"
      ],
      "metadata": {
        "id": "M6TSjYn7zMoR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}