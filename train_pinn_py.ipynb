{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyNDiR+AFrg39meVvR2QJydA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OneFineStarstuff/Cosmic-Brilliance/blob/main/train_pinn_py.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_NyGqWJ1lUq9"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.optim import Adam\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "\n",
        "# For reproducibility\n",
        "torch.manual_seed(0)\n",
        "np.random.seed(0)\n",
        "\n",
        "\n",
        "class PhysicsDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Wraps input/output arrays into a torch Dataset with normalization\n",
        "    and sanity checks against NaNs/Infs.\n",
        "    \"\"\"\n",
        "    def __init__(self, X, Y):\n",
        "        # Convert to float32\n",
        "        self.X = X.astype(np.float32)\n",
        "        self.Y = Y.astype(np.float32)\n",
        "\n",
        "        # Compute normalization stats\n",
        "        self.X_mean, self.X_std = self.X.mean(0), self.X.std(0) + 1e-8\n",
        "        self.Y_mean, self.Y_std = self.Y.mean(0), self.Y.std(0) + 1e-8\n",
        "\n",
        "        # Apply normalization\n",
        "        self.X = (self.X - self.X_mean) / self.X_std\n",
        "        self.Y = (self.Y - self.Y_mean) / self.Y_std\n",
        "\n",
        "        # Sanity checks\n",
        "        assert not np.isnan(self.X).any() and not np.isinf(self.X).any(), \\\n",
        "            \"X contains NaN or Inf\"\n",
        "        assert not np.isnan(self.Y).any() and not np.isinf(self.Y).any(), \\\n",
        "            \"Y contains NaN or Inf\"\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx], self.Y[idx]\n",
        "\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    \"\"\"\n",
        "    Simple fully connected network with Tanh activations.\n",
        "    \"\"\"\n",
        "    def __init__(self, in_dim, out_dim, hidden=(64, 64)):\n",
        "        super().__init__()\n",
        "        layers = []\n",
        "        dims = [in_dim] + list(hidden) + [out_dim]\n",
        "        for i in range(len(dims) - 1):\n",
        "            layers.append(nn.Linear(dims[i], dims[i+1]))\n",
        "            if i < len(dims) - 1:\n",
        "                layers.append(nn.Tanh())\n",
        "        self.net = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "\n",
        "def physics_residual(u_pred, x, stats):\n",
        "    \"\"\"\n",
        "    Example PDE residual: u_xx - f(x) = 0\n",
        "    Adjust this to your specific operator.\n",
        "    \"\"\"\n",
        "    # Un-normalize u\n",
        "    u = u_pred * stats['Y_std'] + stats['Y_mean']\n",
        "\n",
        "    # First derivative\n",
        "    grads = torch.autograd.grad(\n",
        "        outputs=u.sum(),\n",
        "        inputs=x,\n",
        "        create_graph=True\n",
        "    )[0]\n",
        "\n",
        "    # Second derivative\n",
        "    u_xx = torch.autograd.grad(\n",
        "        outputs=grads.sum(),\n",
        "        inputs=x,\n",
        "        create_graph=True\n",
        "    )[0]\n",
        "\n",
        "    # Example source term\n",
        "    f = torch.sin(x)\n",
        "\n",
        "    # Residual and renormalize\n",
        "    res = (u_xx - f) / stats['Y_std']\n",
        "    return res\n",
        "\n",
        "\n",
        "def train(model, train_loader, val_loader, stats,\n",
        "          lr=1e-3, lam=1.0, max_epochs=100, patience=10):\n",
        "    \"\"\"\n",
        "    Training loop with:\n",
        "    - Adam optimizer\n",
        "    - ReduceLROnPlateau scheduler\n",
        "    - Gradient clipping\n",
        "    - NaN loss check\n",
        "    - Early stopping based on validation loss\n",
        "    \"\"\"\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model.to(device)\n",
        "    optimizer = Adam(model.parameters(), lr=lr)\n",
        "    scheduler = ReduceLROnPlateau(optimizer, factor=0.5, patience=5, min_lr=1e-6)\n",
        "\n",
        "    best_val, epochs_no_improve = float('inf'), 0\n",
        "    history = {'train_loss': [], 'val_loss': []}\n",
        "\n",
        "    for epoch in range(1, max_epochs + 1):\n",
        "        # ---- Training ----\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "\n",
        "        for xb, yb in train_loader:\n",
        "            xb = xb.to(device).requires_grad_(True)\n",
        "            yb = yb.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            yp = model(xb)\n",
        "\n",
        "            mse_loss = nn.functional.mse_loss(yp, yb)\n",
        "            pres = physics_residual(yp, xb, stats)\n",
        "            pres_loss = pres.clamp(-1.0, 1.0).pow(2).mean()\n",
        "\n",
        "            loss = mse_loss + lam * pres_loss\n",
        "            loss.backward()\n",
        "\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "            optimizer.step()\n",
        "\n",
        "            if torch.isnan(loss):\n",
        "                raise RuntimeError(f\"NaN encountered in loss at epoch {epoch}\")\n",
        "\n",
        "            train_loss += loss.item() * xb.size(0)\n",
        "\n",
        "        train_loss /= len(train_loader.dataset)\n",
        "        history['train_loss'].append(train_loss)\n",
        "\n",
        "        # ---- Validation ----\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        with torch.no_grad():\n",
        "            for xb, yb in val_loader:\n",
        "                xb, yb = xb.to(device), yb.to(device)\n",
        "                yp = model(xb)\n",
        "                val_loss += nn.functional.mse_loss(yp, yb, reduction='sum').item()\n",
        "\n",
        "        val_loss /= len(val_loader.dataset)\n",
        "        history['val_loss'].append(val_loss)\n",
        "\n",
        "        # Scheduler step\n",
        "        scheduler.step(val_loss)\n",
        "\n",
        "        print(f\"Epoch {epoch:03d} | Train {train_loss:.6f} | Val {val_loss:.6f}\")\n",
        "\n",
        "        # Early stopping\n",
        "        if val_loss + 1e-6 < best_val:\n",
        "            best_val, epochs_no_improve = val_loss, 0\n",
        "        else:\n",
        "            epochs_no_improve += 1\n",
        "            if epochs_no_improve >= patience:\n",
        "                print(f\"Early stopping at epoch {epoch}\")\n",
        "                break\n",
        "\n",
        "    return history\n",
        "\n",
        "\n",
        "def main():\n",
        "    # -- Generate or load data --\n",
        "    N = 2000\n",
        "    X = np.random.uniform(-np.pi, np.pi, size=(N, 1))\n",
        "    Y = np.sin(X)  # target function\n",
        "\n",
        "    # Split into train/validation\n",
        "    perm = np.random.permutation(N)\n",
        "    train_idx, val_idx = perm[:1600], perm[1600:]\n",
        "    ds_train = PhysicsDataset(X[train_idx], Y[train_idx])\n",
        "    ds_val = PhysicsDataset(X[val_idx], Y[val_idx])\n",
        "\n",
        "    # DataLoaders\n",
        "    train_loader = DataLoader(ds_train, batch_size=64, shuffle=True)\n",
        "    val_loader = DataLoader(ds_val, batch_size=64, shuffle=False)\n",
        "\n",
        "    # Stats for residual normalization\n",
        "    stats = {\n",
        "        'X_mean': torch.tensor(ds_train.X_mean, dtype=torch.float32),\n",
        "        'X_std':  torch.tensor(ds_train.X_std,  dtype=torch.float32),\n",
        "        'Y_mean': torch.tensor(ds_train.Y_mean, dtype=torch.float32),\n",
        "        'Y_std':  torch.tensor(ds_train.Y_std,  dtype=torch.float32),\n",
        "    }\n",
        "\n",
        "    # Model instantiation\n",
        "    model = MLP(in_dim=1, out_dim=1, hidden=(64, 64))\n",
        "\n",
        "    # Train\n",
        "    history = train(\n",
        "        model,\n",
        "        train_loader,\n",
        "        val_loader,\n",
        "        stats,\n",
        "        lr=1e-4,      # Lower LR for stability\n",
        "        lam=0.1,      # Physics loss weight\n",
        "        max_epochs=200,\n",
        "        patience=20\n",
        "    )\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ]
    }
  ]
}