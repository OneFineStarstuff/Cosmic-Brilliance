{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyMP6qgVcomnUMmNJrYLr7i9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OneFineStarstuff/Cosmic-Brilliance/blob/main/open_ended_question_generator_secure_py.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H4Pyrkbzjwkq"
      },
      "outputs": [],
      "source": [
        "#!/usr/bin/env python3\n",
        "# -*- coding: utf-8 -*-\n",
        "\n",
        "\"\"\"\n",
        "open_ended_question_generator_secure.py\n",
        "\n",
        "End-to-end script to generate open-ended questions from context(s) with:\n",
        "- Robust list-formatted parsing\n",
        "- CLI with single or batch inputs (TXT/CSV)\n",
        "- Reproducibility (seed)\n",
        "- Device auto-select (CUDA / MPS / CPU)\n",
        "- Export to JSON / CSV / TXT\n",
        "- Optional AES-256-like authenticated encryption via Fernet (with PBKDF2 key derivation)\n",
        "- Optional decryption utility\n",
        "\n",
        "Dependencies:\n",
        "  pip install torch transformers cryptography\n",
        "\n",
        "Example:\n",
        "  python open_ended_question_generator_secure.py \\\n",
        "    --context \"AGI for cosmology\" --n 5 --model gpt2-large \\\n",
        "    --out questions.json --format json --encrypt --password \"your-secret\"\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import re\n",
        "import csv\n",
        "import json\n",
        "import argparse\n",
        "import getpass\n",
        "import base64\n",
        "import sys\n",
        "from typing import List, Dict, Tuple, Optional\n",
        "\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "# --- Optional encryption deps ---\n",
        "try:\n",
        "    from cryptography.fernet import Fernet\n",
        "    from cryptography.hazmat.primitives.kdf.pbkdf2 import PBKDF2HMAC\n",
        "    from cryptography.hazmat.primitives import hashes\n",
        "    from cryptography.hazmat.backends import default_backend\n",
        "except Exception:\n",
        "    Fernet = None  # Will validate at runtime if encryption/decryption is used.\n",
        "\n",
        "\n",
        "# ----------------------------\n",
        "# Device selection\n",
        "# ----------------------------\n",
        "def select_device() -> torch.device:\n",
        "    if hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
        "        return torch.device(\"mps\")\n",
        "    if torch.cuda.is_available():\n",
        "        return torch.device(\"cuda\")\n",
        "    return torch.device(\"cpu\")\n",
        "\n",
        "\n",
        "# ----------------------------\n",
        "# Prompt and parsing\n",
        "# ----------------------------\n",
        "PROMPT_TEMPLATE = \"\"\"You are a master at generating deep, open-ended, and thought-provoking questions.\n",
        "Each question must be:\n",
        "- Self-contained and understandable without extra context.\n",
        "- Exploratory (not answerable with yes/no).\n",
        "- Written in clear, engaging language.\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Output exactly {n} questions as a numbered list, one per line, formatted like:\n",
        "1. ...\n",
        "2. ...\n",
        "3. ...\n",
        "No extra commentary, no headings, no explanations â€” just the list.\n",
        "\"\"\"\n",
        "\n",
        "def build_prompt(context: str, n: int) -> str:\n",
        "    return PROMPT_TEMPLATE.format(context=context.strip(), n=n)\n",
        "\n",
        "_Q_LINE_RE = re.compile(r\"^\\s*(\\d+)\\.\\s+(.*\\S)\\s*$\")\n",
        "\n",
        "def normalize_q(q: str) -> str:\n",
        "    q = q.strip()\n",
        "    # Ensure it ends with a question mark for consistency\n",
        "    if not q.endswith(\"?\"):\n",
        "        q += \"?\"\n",
        "    return q\n",
        "\n",
        "def parse_questions_from_text(text: str, n: int) -> List[str]:\n",
        "    lines = text.splitlines()\n",
        "    candidates = []\n",
        "    for line in lines:\n",
        "        m = _Q_LINE_RE.match(line)\n",
        "        if m:\n",
        "            q_text = normalize_q(m.group(2))\n",
        "            candidates.append(q_text)\n",
        "    # Deduplicate while preserving order\n",
        "    seen = set()\n",
        "    unique = []\n",
        "    for q in candidates:\n",
        "        key = q.lower().strip()\n",
        "        if key not in seen:\n",
        "            seen.add(key)\n",
        "            unique.append(q)\n",
        "    return unique[:n]\n",
        "\n",
        "\n",
        "# ----------------------------\n",
        "# Model loading and generation\n",
        "# ----------------------------\n",
        "def load_model_and_tokenizer(model_name: str, device: torch.device):\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "    model.to(device)\n",
        "    # For models like GPT-2 without a pad token\n",
        "    if tokenizer.pad_token_id is None and tokenizer.eos_token_id is not None:\n",
        "        tokenizer.pad_token_id = tokenizer.eos_token_id\n",
        "    return model, tokenizer\n",
        "\n",
        "def generate_questions_once(\n",
        "    model,\n",
        "    tokenizer,\n",
        "    device: torch.device,\n",
        "    context: str,\n",
        "    n: int,\n",
        "    max_new_tokens: int,\n",
        "    temperature: float,\n",
        "    top_p: float,\n",
        "    top_k: int,\n",
        ") -> List[str]:\n",
        "    prompt = build_prompt(context, n)\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "    output = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=max_new_tokens,\n",
        "        temperature=temperature,\n",
        "        top_p=top_p,\n",
        "        top_k=top_k,\n",
        "        do_sample=True,\n",
        "        pad_token_id=tokenizer.pad_token_id,\n",
        "        eos_token_id=tokenizer.eos_token_id,\n",
        "    )\n",
        "    decoded = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "    # Extract only the continuation after the prompt\n",
        "    # In many causal LMs, decoded contains prompt + completion; we slice from len(input_ids)\n",
        "    # Simpler approach: parse all lines and trust the numbered format.\n",
        "    questions = parse_questions_from_text(decoded, n)\n",
        "    return questions\n",
        "\n",
        "def generate_questions(\n",
        "    model,\n",
        "    tokenizer,\n",
        "    device: torch.device,\n",
        "    context: str,\n",
        "    n: int = 3,\n",
        "    max_new_tokens: int = 200,\n",
        "    temperature: float = 0.95,\n",
        "    top_p: float = 0.95,\n",
        "    top_k: int = 50,\n",
        "    seed: Optional[int] = None,\n",
        "    attempts: int = 3,\n",
        ") -> List[str]:\n",
        "    if seed is not None:\n",
        "        torch.manual_seed(seed)\n",
        "        if device.type == \"cuda\":\n",
        "            torch.cuda.manual_seed_all(seed)\n",
        "    collected: List[str] = []\n",
        "    tried = 0\n",
        "    while len(collected) < n and tried < attempts:\n",
        "        tried += 1\n",
        "        # Slightly adjust temperature on retries to improve variety\n",
        "        temp = min(1.2, max(0.7, temperature + 0.1 * (tried - 1)))\n",
        "        qs = generate_questions_once(\n",
        "            model, tokenizer, device, context, n, max_new_tokens, temp, top_p, top_k\n",
        "        )\n",
        "        # Merge unique\n",
        "        existing = set([q.lower().strip() for q in collected])\n",
        "        for q in qs:\n",
        "            key = q.lower().strip()\n",
        "            if key not in existing and len(collected) < n:\n",
        "                collected.append(q)\n",
        "                existing.add(key)\n",
        "    # If still short, pad with simple variants (rare)\n",
        "    while len(collected) < n:\n",
        "        collected.append(collected[-1] + \" (expand)\") if collected else collected.append(\"What deeper questions arise from this context?\")\n",
        "    return collected[:n]\n",
        "\n",
        "\n",
        "# ----------------------------\n",
        "# Batch input handling\n",
        "# ----------------------------\n",
        "def load_contexts(source_text: Optional[str], source_file: Optional[str]) -> List[Tuple[str, str]]:\n",
        "    \"\"\"\n",
        "    Returns list of (context_id, context_text).\n",
        "    - If source_text is provided, returns single-item list.\n",
        "    - If CSV file: expects a 'context' column.\n",
        "    - If TXT/MD: splits on lines containing only '---' or returns whole file as one context.\n",
        "    \"\"\"\n",
        "    out: List[Tuple[str, str]] = []\n",
        "    if source_text:\n",
        "        out.append((\"context_1\", source_text.strip()))\n",
        "        return out\n",
        "    if not source_file:\n",
        "        raise ValueError(\"Either --context or --context-file is required.\")\n",
        "    if not os.path.exists(source_file):\n",
        "        raise FileNotFoundError(f\"Context file not found: {source_file}\")\n",
        "\n",
        "    ext = os.path.splitext(source_file)[1].lower()\n",
        "    if ext == \".csv\":\n",
        "        with open(source_file, \"r\", encoding=\"utf-8\", newline=\"\") as f:\n",
        "            reader = csv.DictReader(f)\n",
        "            if \"context\" not in reader.fieldnames:\n",
        "                raise ValueError(\"CSV must have a 'context' column.\")\n",
        "            for i, row in enumerate(reader, start=1):\n",
        "                ctx = (row.get(\"context\") or \"\").strip()\n",
        "                if ctx:\n",
        "                    out.append((f\"context_{i}\", ctx))\n",
        "    else:\n",
        "        # Plain text / markdown: split on '---' delimiter lines if present\n",
        "        with open(source_file, \"r\", encoding=\"utf-8\") as f:\n",
        "            content = f.read()\n",
        "        parts = re.split(r\"^\\s*---\\s*$\", content, flags=re.MULTILINE)\n",
        "        parts = [p.strip() for p in parts if p.strip()]\n",
        "        if not parts:\n",
        "            raise ValueError(\"No context found in file.\")\n",
        "        for i, ctx in enumerate(parts, start=1):\n",
        "            out.append((f\"context_{i}\", ctx))\n",
        "    return out\n",
        "\n",
        "\n",
        "# ----------------------------\n",
        "# Output writers\n",
        "# ----------------------------\n",
        "def write_json(out_path: str, rows: List[Dict]):\n",
        "    with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(rows, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "def write_csv(out_path: str, rows: List[Dict], n: int):\n",
        "    fieldnames = [\"context_id\", \"context\"] + [f\"q{i}\" for i in range(1, n + 1)]\n",
        "    with open(out_path, \"w\", encoding=\"utf-8\", newline=\"\") as f:\n",
        "        writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
        "        writer.writeheader()\n",
        "        for r in rows:\n",
        "            writer.writerow(r)\n",
        "\n",
        "def write_txt(out_path: str, rows: List[Dict], n: int):\n",
        "    with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        for r in rows:\n",
        "            f.write(f\"[{r['context_id']}]\\n\")\n",
        "            f.write(r[\"context\"].strip() + \"\\n\")\n",
        "            for i in range(1, n + 1):\n",
        "                f.write(f\"{i}. {r[f'q{i}']}\\n\")\n",
        "            f.write(\"\\n\")\n",
        "\n",
        "\n",
        "# ----------------------------\n",
        "# Encryption / Decryption\n",
        "# ----------------------------\n",
        "MAGIC = b\"QSEC1\"\n",
        "\n",
        "def require_crypto():\n",
        "    if Fernet is None:\n",
        "        raise RuntimeError(\"Encryption requested but 'cryptography' is not installed. Run: pip install cryptography\")\n",
        "\n",
        "def derive_key_from_password(password: str, salt: bytes) -> bytes:\n",
        "    kdf = PBKDF2HMAC(\n",
        "        algorithm=hashes.SHA256(),\n",
        "        length=32,\n",
        "        salt=salt,\n",
        "        iterations=200_000,\n",
        "        backend=default_backend(),\n",
        "    )\n",
        "    key = kdf.derive(password.encode(\"utf-8\"))\n",
        "    return base64.urlsafe_b64encode(key)\n",
        "\n",
        "def encrypt_file(in_path: str, out_path: str, password: str):\n",
        "    require_crypto()\n",
        "    with open(in_path, \"rb\") as f:\n",
        "        plaintext = f.read()\n",
        "    salt = os.urandom(16)\n",
        "    key = derive_key_from_password(password, salt)\n",
        "    fernet = Fernet(key)\n",
        "    ciphertext = fernet.encrypt(plaintext)\n",
        "    with open(out_path, \"wb\") as f:\n",
        "        f.write(MAGIC + salt + ciphertext)\n",
        "\n",
        "def decrypt_file(in_path: str, out_path: str, password: str):\n",
        "    require_crypto()\n",
        "    with open(in_path, \"rb\") as f:\n",
        "        blob = f.read()\n",
        "    if not blob.startswith(MAGIC) or len(blob) < len(MAGIC) + 16 + 1:\n",
        "        raise ValueError(\"Invalid or unsupported encrypted file.\")\n",
        "    salt = blob[len(MAGIC):len(MAGIC)+16]\n",
        "    ciphertext = blob[len(MAGIC)+16:]\n",
        "    key = derive_key_from_password(password, salt)\n",
        "    fernet = Fernet(key)\n",
        "    plaintext = fernet.decrypt(ciphertext)\n",
        "    with open(out_path, \"wb\") as f:\n",
        "        f.write(plaintext)\n",
        "\n",
        "\n",
        "# ----------------------------\n",
        "# Main CLI\n",
        "# ----------------------------\n",
        "def main():\n",
        "    parser = argparse.ArgumentParser(description=\"Generate deep open-ended questions with optional encryption/decryption.\")\n",
        "    mode = parser.add_mutuallyExclusiveGroup(required=True)\n",
        "    mode.add_argument(\"--generate\", action=\"store_true\", help=\"Generate questions from context(s).\")\n",
        "    mode.add_argument(\"--decrypt\", action=\"store_true\", help=\"Decrypt an encrypted file (no generation).\")\n",
        "\n",
        "    # Generation inputs\n",
        "    parser.add_argument(\"--context\", type=str, help=\"Inline context text.\")\n",
        "    parser.add_argument(\"--context-file\", type=str, help=\"Path to TXT/MD (split by ---) or CSV with 'context' column.\")\n",
        "    parser.add_argument(\"--n\", type=int, default=3, help=\"Number of questions to generate per context.\")\n",
        "    parser.add_argument(\"--model\", type=str, default=\"gpt2-large\", help=\"HuggingFace model name.\")\n",
        "    parser.add_argument(\"--max-new-tokens\", type=int, default=220, help=\"Max new tokens for generation.\")\n",
        "    parser.add_argument(\"--temperature\", type=float, default=0.95, help=\"Sampling temperature.\")\n",
        "    parser.add_argument(\"--top-p\", type=float, default=0.95, help=\"Top-p nucleus sampling.\")\n",
        "    parser.add_argument(\"--top-k\", type=int, default=50, help=\"Top-k sampling.\")\n",
        "    parser.add_argument(\"--seed\", type=int, default=None, help=\"Random seed for reproducibility.\")\n",
        "    parser.add_argument(\"--attempts\", type=int, default=3, help=\"Max attempts to reach exactly n questions.\")\n",
        "\n",
        "    # Output\n",
        "    parser.add_argument(\"--out\", type=str, default=None, help=\"Output file path. If omitted, prints to stdout.\")\n",
        "    parser.add_argument(\"--format\", type=str, choices=[\"json\", \"csv\", \"txt\"], default=\"json\", help=\"Output format when generating.\")\n",
        "    parser.add_argument(\"--encrypt\", action=\"store_true\", help=\"Encrypt the output file after generation.\")\n",
        "    parser.add_argument(\"--password\", type=str, default=None, help=\"Password for encryption/decryption. If omitted, prompts securely.\")\n",
        "\n",
        "    # Decryption I/O\n",
        "    parser.add_argument(\"--in\", dest=\"in_path\", type=str, help=\"Input file for decryption (encrypted).\")\n",
        "    parser.add_argument(\"--out-decrypted\", type=str, help=\"Output file for decrypted plaintext.\")\n",
        "\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    device = select_device()\n",
        "\n",
        "    if args.decrypt:\n",
        "        # Decrypt mode\n",
        "        if not args.in_path or not args.out_decrypted:\n",
        "            parser.error(\"--decrypt requires --in and --out-decrypted.\")\n",
        "        password = args.password or getpass.getpass(\"Enter password: \")\n",
        "        decrypt_file(args.in_path, args.out_decrypted, password)\n",
        "        print(f\"Decrypted to: {args.out_decrypted}\")\n",
        "        return\n",
        "\n",
        "    # Generate mode\n",
        "    contexts = load_contexts(args.context, args.context_file)\n",
        "    model, tokenizer = load_model_and_tokenizer(args.model, device)\n",
        "\n",
        "    rows: List[Dict] = []\n",
        "    for ctx_id, ctx in contexts:\n",
        "        qs = generate_questions(\n",
        "            model=model,\n",
        "            tokenizer=tokenizer,\n",
        "            device=device,\n",
        "            context=ctx,\n",
        "            n=args.n,\n",
        "            max_new_tokens=args.max_new_tokens,\n",
        "            temperature=args.temperature,\n",
        "            top_p=args.top_p,\n",
        "            top_k=args.top_k,\n",
        "            seed=args.seed,\n",
        "            attempts=args.attempts,\n",
        "        )\n",
        "        row = {\"context_id\": ctx_id, \"context\": ctx}\n",
        "        for i, q in enumerate(qs, start=1):\n",
        "            row[f\"q{i}\"] = q\n",
        "        rows.append(row)\n",
        "\n",
        "    # Output\n",
        "    if args.out:\n",
        "        out_path = args.out\n",
        "        os.makedirs(os.path.dirname(out_path) or \".\", exist_ok=True)\n",
        "        if args.format == \"json\":\n",
        "            write_json(out_path, rows)\n",
        "        elif args.format == \"csv\":\n",
        "            write_csv(out_path, rows, args.n)\n",
        "        else:\n",
        "            write_txt(out_path, rows, args.n)\n",
        "\n",
        "        if args.encrypt:\n",
        "            password = args.password or getpass.getpass(\"Enter password: \")\n",
        "            enc_path = out_path + \".enc\"\n",
        "            encrypt_file(out_path, enc_path, password)\n",
        "            print(f\"Saved: {out_path}\")\n",
        "            print(f\"Encrypted copy: {enc_path}\")\n",
        "        else:\n",
        "            print(f\"Saved: {out_path}\")\n",
        "    else:\n",
        "        # Print to stdout in selected format\n",
        "        if args.format == \"json\":\n",
        "            print(json.dumps(rows, ensure_ascii=False, indent=2))\n",
        "        elif args.format == \"csv\":\n",
        "            # Minimal CSV to stdout\n",
        "            fieldnames = [\"context_id\", \"context\"] + [f\"q{i}\" for i in range(1, args.n + 1)]\n",
        "            writer = csv.DictWriter(sys.stdout, fieldnames=fieldnames)\n",
        "            writer.writeheader()\n",
        "            for r in rows:\n",
        "                writer.writerow(r)\n",
        "        else:\n",
        "            for r in rows:\n",
        "                print(f\"[{r['context_id']}]\")\n",
        "                print(r[\"context\"].strip())\n",
        "                for i in range(1, args.n + 1):\n",
        "                    print(f\"{i}. {r[f'q{i}']}\")\n",
        "                print()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ]
}