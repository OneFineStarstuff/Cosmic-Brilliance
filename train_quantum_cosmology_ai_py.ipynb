{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyPWgDuhxvp3Lz/ktzmJbYIX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OneFineStarstuff/Cosmic-Brilliance/blob/main/train_quantum_cosmology_ai_py.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l5VE5-OJ3qIv"
      },
      "outputs": [],
      "source": [
        "pip install torch numpy matplotlib"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "train_quantum_cosmology_ai.py\n",
        "\n",
        "Physics-informed AI pipeline for early-universe quantum state evolution:\n",
        "\n",
        "1. Synthetic dataset of 6 initial cosmological variables → 3 evolution metrics\n",
        "2. PINN loss: supervised MSE + Friedmann‐equation residual\n",
        "3. MLP with LayerNorm & Dropout for predictive uncertainty\n",
        "4. MC-Dropout inference to quantify uncertainty\n",
        "5. Training loop with AdamW, ReduceLROnPlateau, early stopping\n",
        "6. Visualizations: loss curves, true vs. predicted scatter, uncertainty heatmap\n",
        "\"\"\"\n",
        "\n",
        "import math\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 1. Synthetic Dataset\n",
        "# ------------------------------------------------------------------------------\n",
        "class QuantumCosmologyDataset(Dataset):\n",
        "    def __init__(self, n_samples=5000, seed=0):\n",
        "        np.random.seed(seed)\n",
        "        # Sample inputs:\n",
        "        # rho_rad, rho_mat, rho_de in [0.1, 1.0]\n",
        "        # a0 in [0.5, 1.5], k in [-0.1,0.1], t in [0.1,5.0]\n",
        "        rho_rad = np.random.uniform(0.1,1.0, size=(n_samples,1)).astype(np.float32)\n",
        "        rho_mat = np.random.uniform(0.1,1.0, size=(n_samples,1)).astype(np.float32)\n",
        "        rho_de  = np.random.uniform(0.1,1.0, size=(n_samples,1)).astype(np.float32)\n",
        "        a0      = np.random.uniform(0.5,1.5, size=(n_samples,1)).astype(np.float32)\n",
        "        k       = np.random.uniform(-0.1,0.1, size=(n_samples,1)).astype(np.float32)\n",
        "        t       = np.random.uniform(0.1,5.0, size=(n_samples,1)).astype(np.float32)\n",
        "\n",
        "        X_raw = np.hstack([rho_rad, rho_mat, rho_de, a0, k, t])\n",
        "\n",
        "        # Compute analytic targets\n",
        "        sum_rho = rho_rad + rho_mat + rho_de  # total density\n",
        "        H0_sq = np.maximum(sum_rho - k/(a0**2), 1e-6)\n",
        "        H0 = np.sqrt(H0_sq)\n",
        "\n",
        "        a_t = a0 * np.exp(H0 * t)                        # scale factor\n",
        "        H_t = H0                                         # Hubble parameter\n",
        "        Omega_k = - k / (a_t**2 * H0_sq)                 # curvature parameter\n",
        "\n",
        "        Y_raw = np.hstack([a_t, H_t, Omega_k]).astype(np.float32)\n",
        "        # add small noise\n",
        "        Y_raw += 0.01 * np.random.randn(*Y_raw.shape).astype(np.float32)\n",
        "\n",
        "        # stats for normalization\n",
        "        self.X_mean, self.X_std = X_raw.mean(0), X_raw.std(0)\n",
        "        self.Y_mean, self.Y_std = Y_raw.mean(0), Y_raw.std(0)\n",
        "\n",
        "        # normalize\n",
        "        self.X = (X_raw - self.X_mean) / self.X_std\n",
        "        self.Y = (Y_raw - self.Y_mean) / self.Y_std\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x = torch.from_numpy(self.X[idx])\n",
        "        y = torch.from_numpy(self.Y[idx])\n",
        "        return x, y\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 2. Model Architecture with Dropout\n",
        "# ------------------------------------------------------------------------------\n",
        "class QuantumCosmologyAI(nn.Module):\n",
        "    def __init__(self, input_dim=6, hidden_dims=(64,64), output_dim=3, p_drop=0.1):\n",
        "        super().__init__()\n",
        "        layers, dim = [], input_dim\n",
        "        for h in hidden_dims:\n",
        "            layers += [\n",
        "                nn.Linear(dim, h),\n",
        "                nn.LayerNorm(h),\n",
        "                nn.ReLU(),\n",
        "                nn.Dropout(p_drop)\n",
        "            ]\n",
        "            dim = h\n",
        "        layers.append(nn.Linear(dim, output_dim))\n",
        "        self.net = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 3. Physics-Informed Loss\n",
        "# ------------------------------------------------------------------------------\n",
        "def physics_residual(pred, inp, stats):\n",
        "    # Denormalize inputs and predictions\n",
        "    rho_rad = inp[:,0] * stats['X_std'][0] + stats['X_mean'][0]\n",
        "    rho_mat = inp[:,1] * stats['X_std'][1] + stats['X_mean'][1]\n",
        "    rho_de  = inp[:,2] * stats['X_std'][2] + stats['X_mean'][2]\n",
        "    a0      = inp[:,3] * stats['X_std'][3] + stats['X_mean'][3]\n",
        "    k       = inp[:,4] * stats['X_std'][4] + stats['X_mean'][4]\n",
        "    t       = inp[:,5] * stats['X_std'][5] + stats['X_mean'][5]\n",
        "\n",
        "    # Predictions\n",
        "    a_pred = pred[:,0] * stats['Y_std'][0] + stats['Y_mean'][0]\n",
        "    H_pred = pred[:,1] * stats['Y_std'][1] + stats['Y_mean'][1]\n",
        "    # Omega_k_pred not used in residual\n",
        "\n",
        "    # Friedmann equation residual: H^2 ≈ rho_total - k/a^2\n",
        "    rho_tot = rho_rad + rho_mat + rho_de\n",
        "    resid = H_pred**2 - (rho_tot - k / (a_pred**2))\n",
        "    return torch.mean(resid**2)\n",
        "\n",
        "def total_loss(pred, true, inp, stats, lambda_phys=1.0):\n",
        "    mse = nn.MSELoss()(pred, true)\n",
        "    phys = physics_residual(pred, inp, stats)\n",
        "    return mse + lambda_phys * phys, mse, phys\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 4. MC-Dropout Inference\n",
        "# ------------------------------------------------------------------------------\n",
        "def mc_dropout_predict(model, x, n_samples=100):\n",
        "    model.train()\n",
        "    preds = []\n",
        "    with torch.no_grad():\n",
        "        for _ in range(n_samples):\n",
        "            preds.append(model(x).cpu().numpy())\n",
        "    arr = np.stack(preds, axis=0)\n",
        "    return arr.mean(0), arr.std(0)\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 5. Training Loop\n",
        "# ------------------------------------------------------------------------------\n",
        "def train(model, train_loader, val_loader, stats, device,\n",
        "          lr=1e-3, wd=1e-5, lambda_phys=1.0,\n",
        "          max_epochs=100, patience=10):\n",
        "    model.to(device)\n",
        "    opt = optim.AdamW(model.parameters(), lr=lr, weight_decay=wd)\n",
        "    sched = optim.lr_scheduler.ReduceLROnPlateau(opt, mode='min',\n",
        "                                                factor=0.5, patience=5)\n",
        "\n",
        "    best_val, wait = float('inf'), 0\n",
        "    history = {'train': [], 'val': []}\n",
        "\n",
        "    for epoch in range(1, max_epochs+1):\n",
        "        # Training\n",
        "        model.train()\n",
        "        tr_loss = 0.0\n",
        "        for xb, yb in train_loader:\n",
        "            xb, yb = xb.to(device), yb.to(device)\n",
        "            pred = model(xb)\n",
        "            loss, _, _ = total_loss(pred, yb, xb, stats, lambda_phys)\n",
        "            opt.zero_grad()\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "            tr_loss += loss.item() * xb.size(0)\n",
        "        tr_loss /= len(train_loader.dataset)\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        vl_loss = 0.0\n",
        "        with torch.no_grad():\n",
        "            for xb, yb in val_loader:\n",
        "                xb, yb = xb.to(device), yb.to(device)\n",
        "                pred = model(xb)\n",
        "                loss, _, _ = total_loss(pred, yb, xb, stats, lambda_phys)\n",
        "                vl_loss += loss.item() * xb.size(0)\n",
        "        vl_loss /= len(val_loader.dataset)\n",
        "\n",
        "        sched.step(vl_loss)\n",
        "        history['train'].append(tr_loss)\n",
        "        history['val'].append(vl_loss)\n",
        "        print(f\"Epoch {epoch:03d} | Train {tr_loss:.4e} | Val {vl_loss:.4e}\")\n",
        "\n",
        "        # Early stopping\n",
        "        if vl_loss < best_val - 1e-6:\n",
        "            best_val, wait = vl_loss, 0\n",
        "            torch.save(model.state_dict(), \"best_quantum_cosmology_ai.pth\")\n",
        "        else:\n",
        "            wait += 1\n",
        "            if wait >= patience:\n",
        "                print(f\"Early stopping at epoch {epoch}\")\n",
        "                break\n",
        "\n",
        "    model.load_state_dict(torch.load(\"best_quantum_cosmology_ai.pth\"))\n",
        "    return history\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 6. Visualization Helpers\n",
        "# ------------------------------------------------------------------------------\n",
        "def plot_losses(history):\n",
        "    plt.figure()\n",
        "    plt.plot(history['train'], label='Train')\n",
        "    plt.plot(history['val'],   label='Val')\n",
        "    plt.xlabel(\"Epoch\"); plt.ylabel(\"Loss\")\n",
        "    plt.legend(); plt.title(\"Training Curve\")\n",
        "    plt.tight_layout(); plt.show()\n",
        "\n",
        "def plot_scatter(true_vals, pred_vals, name):\n",
        "    plt.figure()\n",
        "    plt.scatter(true_vals, pred_vals, s=5, alpha=0.5)\n",
        "    m, M = true_vals.min(), true_vals.max()\n",
        "    plt.plot([m, M], [m, M], 'r--')\n",
        "    plt.xlabel(f\"True {name}\"); plt.ylabel(f\"Pred {name}\")\n",
        "    plt.title(f\"{name}: True vs Pred\")\n",
        "    plt.tight_layout(); plt.show()\n",
        "\n",
        "def plot_uncertainty_heatmap(model, stats_np, device):\n",
        "    # vary rho_mat vs rho_de, fix others\n",
        "    vals = np.linspace(0.1, 1.0, 100)\n",
        "    GM, GD = np.meshgrid(vals, vals)\n",
        "    grid = np.zeros((GM.size, 6), dtype=np.float32)\n",
        "    # set rho_rad, rho_mat, rho_de\n",
        "    grid[:,0] = 0.5\n",
        "    grid[:,1] = GM.ravel()\n",
        "    grid[:,2] = GD.ravel()\n",
        "    # fix a0,k,t at mean\n",
        "    grid[:,3] = stats_np['X_mean'][3]\n",
        "    grid[:,4] = stats_np['X_mean'][4]\n",
        "    grid[:,5] = stats_np['X_mean'][5]\n",
        "\n",
        "    Xn = (grid - stats_np['X_mean']) / stats_np['X_std']\n",
        "    Xt = torch.from_numpy(Xn).float().to(device)\n",
        "    _, std = mc_dropout_predict(model, Xt, n_samples=100)\n",
        "    std_map = std[:,0].reshape(GM.shape)  # uncertainty in a(t)\n",
        "\n",
        "    plt.figure(figsize=(6,5))\n",
        "    plt.pcolormesh(vals, vals, std_map, cmap='viridis', shading='auto')\n",
        "    plt.colorbar(label=\"Std(a(t))\")\n",
        "    plt.xlabel(\"rho_mat\"); plt.ylabel(\"rho_de\")\n",
        "    plt.title(\"Uncertainty Heatmap for Scale Factor\")\n",
        "    plt.tight_layout(); plt.show()\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 7. Main Execution\n",
        "# ------------------------------------------------------------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    ds = QuantumCosmologyDataset(n_samples=8000)\n",
        "    # stats for torch\n",
        "    stats = {\n",
        "        'X_mean': torch.tensor(ds.X_mean, device=device),\n",
        "        'X_std':  torch.tensor(ds.X_std,  device=device),\n",
        "        'Y_mean': torch.tensor(ds.Y_mean, device=device),\n",
        "        'Y_std':  torch.tensor(ds.Y_std,  device=device),\n",
        "    }\n",
        "    # stats for numpy\n",
        "    stats_np = {\n",
        "        'X_mean': ds.X_mean,\n",
        "        'X_std':  ds.X_std,\n",
        "        'Y_mean': ds.Y_mean,\n",
        "        'Y_std':  ds.Y_std,\n",
        "    }\n",
        "\n",
        "    # train/val split\n",
        "    n_val = int(0.2 * len(ds))\n",
        "    tr_ds, va_ds = random_split(ds, [len(ds)-n_val, n_val])\n",
        "    tr_ld = DataLoader(tr_ds, batch_size=64, shuffle=True)\n",
        "    va_ld = DataLoader(va_ds, batch_size=128)\n",
        "\n",
        "    # build & train\n",
        "    model   = QuantumCosmologyAI().to(device)\n",
        "    history = train(model, tr_ld, va_ld, stats, device)\n",
        "\n",
        "    # plots\n",
        "    plot_losses(history)\n",
        "\n",
        "    # scatter for outputs\n",
        "    X_all = ds.X\n",
        "    with torch.no_grad():\n",
        "        Y_pred_norm = model(torch.from_numpy(X_all).float().to(device)).cpu().numpy()\n",
        "    Y_true_norm = ds.Y\n",
        "    Y_pred = Y_pred_norm * ds.Y_std + ds.Y_mean\n",
        "    Y_true = Y_true_norm * ds.Y_std + ds.Y_mean\n",
        "\n",
        "    for i, name in enumerate([\"Scale Factor a(t)\", \"Hubble H(t)\", \"Omega_k(t)\"]):\n",
        "        plot_scatter(Y_true[:,i], Y_pred[:,i], name)\n",
        "\n",
        "    # uncertainty heatmap\n",
        "    plot_uncertainty_heatmap(model, stats_np, device)"
      ],
      "metadata": {
        "id": "2j5DVfZM46gE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}