{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyOITC/xLVHvuQztzAuVN32B",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OneFineStarstuff/Cosmic-Brilliance/blob/main/space_time_lattice_pinn_py.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wLj77MuNE-NM"
      },
      "outputs": [],
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "space_time_lattice_pinn.py\n",
        "\n",
        "A fully corrected, self-contained script for physics-informed SpaceTimeLatticeAI:\n",
        "\n",
        " 1. Generates synthetic lattice data\n",
        " 2. Defines PINN with a toy determinant–sum constraint\n",
        " 3. Trains with AdamW, ReduceLROnPlateau, early stopping\n",
        " 4. Quantifies uncertainty via MC-Dropout\n",
        " 5. Plots training curves and true vs predicted scatter plots\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 1. Synthetic Dataset\n",
        "# ------------------------------------------------------------------------------\n",
        "def true_lattice_deformation(x: torch.Tensor) -> torch.Tensor:\n",
        "    \"\"\"Toy physics mapping: output_i = x_i * sum(x[:3]).\"\"\"\n",
        "    A = x[:, :3]\n",
        "    S = x[:, :3].sum(dim=1, keepdim=True)\n",
        "    return A * S\n",
        "\n",
        "class SyntheticLatticeDataset(Dataset):\n",
        "    def __init__(self, n_samples=5000, seed=42):\n",
        "        super().__init__()\n",
        "        torch.manual_seed(seed)\n",
        "        X_raw = torch.rand(n_samples, 5) * 2 - 1  # uniform in [-1,1]\n",
        "        Y_raw = true_lattice_deformation(X_raw) + 0.05 * torch.randn(n_samples, 3)\n",
        "\n",
        "        # compute means and stds\n",
        "        self.stats = {\n",
        "            'X_mean': X_raw.mean(0), 'X_std': X_raw.std(0),\n",
        "            'Y_mean': Y_raw.mean(0), 'Y_std': Y_raw.std(0)\n",
        "        }\n",
        "\n",
        "        # normalize\n",
        "        self.X = (X_raw - self.stats['X_mean']) / self.stats['X_std']\n",
        "        self.Y = (Y_raw - self.stats['Y_mean']) / self.stats['Y_std']\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx], self.Y[idx]\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 2. Model Definition\n",
        "# ------------------------------------------------------------------------------\n",
        "class SpaceTimeLatticeAI(nn.Module):\n",
        "    def __init__(self, input_dim=5, hidden_dims=(64, 64), output_dim=3, p_drop=0.1):\n",
        "        super().__init__()\n",
        "        layers = []\n",
        "        dim = input_dim\n",
        "        for h in hidden_dims:\n",
        "            layers += [\n",
        "                nn.Linear(dim, h),\n",
        "                nn.LayerNorm(h),\n",
        "                nn.ReLU(),\n",
        "                nn.Dropout(p_drop)\n",
        "            ]\n",
        "            dim = h\n",
        "        layers.append(nn.Linear(dim, output_dim))\n",
        "        self.net = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 3. Physics-Informed Loss\n",
        "# ------------------------------------------------------------------------------\n",
        "def physics_residual(pred: torch.Tensor,\n",
        "                     inp: torch.Tensor,\n",
        "                     stats: dict) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Enforce det(diag(pred_denorm)) ≈ (sum(inp_denorm[:3]))^2.\n",
        "    \"\"\"\n",
        "    # denormalize\n",
        "    Y_den = pred * stats['Y_std'] + stats['Y_mean']       # [batch,3]\n",
        "    X_den = inp  * stats['X_std'] + stats['X_mean']       # [batch,5]\n",
        "\n",
        "    # determinant of diagonal matrix = product of diagonal entries\n",
        "    det_pred = Y_den[:,0] * Y_den[:,1] * Y_den[:,2]\n",
        "    target   = (X_den[:,0] + X_den[:,1] + X_den[:,2]).pow(2)\n",
        "\n",
        "    return nn.MSELoss()(det_pred, target)\n",
        "\n",
        "def total_loss(pred: torch.Tensor,\n",
        "               true: torch.Tensor,\n",
        "               inp: torch.Tensor,\n",
        "               stats: dict,\n",
        "               lambda_phys: float = 0.5):\n",
        "    mse   = nn.MSELoss()(pred, true)\n",
        "    phys  = physics_residual(pred, inp, stats)\n",
        "    return mse + lambda_phys * phys, mse, phys\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 4. MC-Dropout Prediction\n",
        "# ------------------------------------------------------------------------------\n",
        "def mc_dropout_predict(model: nn.Module,\n",
        "                       x: torch.Tensor,\n",
        "                       n_samples: int = 30) -> (np.ndarray, np.ndarray):\n",
        "    model.train()  # enable dropout\n",
        "    preds = []\n",
        "    with torch.no_grad():\n",
        "        for _ in range(n_samples):\n",
        "            preds.append(model(x).cpu().numpy())\n",
        "    arr = np.stack(preds, axis=0)      # [samples, batch, output_dim]\n",
        "    return arr.mean(axis=0), arr.std(axis=0)\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 5. Training Routine\n",
        "# ------------------------------------------------------------------------------\n",
        "def train(model: nn.Module,\n",
        "          train_loader: DataLoader,\n",
        "          val_loader: DataLoader,\n",
        "          stats: dict,\n",
        "          device: torch.device,\n",
        "          lr: float = 1e-3,\n",
        "          weight_decay: float = 1e-5,\n",
        "          lambda_phys: float = 0.5,\n",
        "          max_epochs: int = 100,\n",
        "          patience: int = 10):\n",
        "    model.to(device)\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
        "        optimizer, mode='min', factor=0.5, patience=3\n",
        "    )\n",
        "\n",
        "    best_val = float('inf')\n",
        "    wait = 0\n",
        "    history = {'train': [], 'val': []}\n",
        "\n",
        "    for epoch in range(1, max_epochs+1):\n",
        "        # Training\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "        for xb, yb in train_loader:\n",
        "            xb, yb = xb.to(device), yb.to(device)\n",
        "            pred   = model(xb)\n",
        "            loss, _, _ = total_loss(pred, yb, xb, stats, lambda_phys)\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            train_loss += loss.item() * xb.size(0)\n",
        "        train_loss /= len(train_loader.dataset)\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        with torch.no_grad():\n",
        "            for xb, yb in val_loader:\n",
        "                xb, yb = xb.to(device), yb.to(device)\n",
        "                pred   = model(xb)\n",
        "                loss, _, _ = total_loss(pred, yb, xb, stats, lambda_phys)\n",
        "                val_loss += loss.item() * xb.size(0)\n",
        "        val_loss /= len(val_loader.dataset)\n",
        "\n",
        "        scheduler.step(val_loss)\n",
        "        history['train'].append(train_loss)\n",
        "        history['val'].append(val_loss)\n",
        "        print(f\"Epoch {epoch:03d} | Train: {train_loss:.4e} | Val: {val_loss:.4e}\")\n",
        "\n",
        "        # Early stopping\n",
        "        if val_loss < best_val - 1e-6:\n",
        "            best_val = val_loss\n",
        "            wait = 0\n",
        "            torch.save(model.state_dict(), \"best_lattice_ai.pth\")\n",
        "        else:\n",
        "            wait += 1\n",
        "            if wait >= patience:\n",
        "                print(f\"Early stopping at epoch {epoch}\")\n",
        "                break\n",
        "\n",
        "    # Load the best model\n",
        "    model.load_state_dict(torch.load(\"best_lattice_ai.pth\"))\n",
        "    return history\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 6. Visualization Helpers\n",
        "# ------------------------------------------------------------------------------\n",
        "def plot_history(history: dict):\n",
        "    plt.figure()\n",
        "    plt.plot(history['train'], label='Train Loss')\n",
        "    plt.plot(history['val'],   label='Val Loss')\n",
        "    plt.xlabel('Epoch'); plt.ylabel('Loss')\n",
        "    plt.legend(); plt.title('Training Curve')\n",
        "    plt.tight_layout(); plt.show()\n",
        "\n",
        "def plot_scatter(true_vals: np.ndarray, pred_vals: np.ndarray, name: str):\n",
        "    plt.figure()\n",
        "    plt.scatter(true_vals, pred_vals, s=5, alpha=0.5)\n",
        "    m, M = true_vals.min(), true_vals.max()\n",
        "    plt.plot([m, M], [m, M], 'r--')\n",
        "    plt.xlabel(f\"True {name}\"); plt.ylabel(f\"Pred {name}\")\n",
        "    plt.title(f\"{name}: True vs. Pred\")\n",
        "    plt.tight_layout(); plt.show()\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 7. Main Execution\n",
        "# ------------------------------------------------------------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # Prepare data\n",
        "    dataset = SyntheticLatticeDataset(n_samples=8000)\n",
        "    stats = dataset.stats\n",
        "\n",
        "    n_val = int(0.2 * len(dataset))\n",
        "    n_trn = len(dataset) - n_val\n",
        "    trn_ds, val_ds = random_split(dataset, [n_trn, n_val])\n",
        "    trn_loader = DataLoader(trn_ds, batch_size=128, shuffle=True)\n",
        "    val_loader = DataLoader(val_ds, batch_size=256)\n",
        "\n",
        "    # Build and train model\n",
        "    model = SpaceTimeLatticeAI().to(device)\n",
        "    history = train(\n",
        "        model, trn_loader, val_loader, stats, device,\n",
        "        lr=1e-3, weight_decay=1e-5, lambda_phys=0.5,\n",
        "        max_epochs=100, patience=10\n",
        "    )\n",
        "\n",
        "    # Plot training history\n",
        "    plot_history(history)\n",
        "\n",
        "    # Full dataset scatter plots\n",
        "    X_all = dataset.X.to(device)\n",
        "    with torch.no_grad():\n",
        "        Y_pred_norm = model(X_all).cpu()\n",
        "    Y_true_norm = dataset.Y\n",
        "\n",
        "    # Denormalize\n",
        "    Y_pred = Y_pred_norm * stats['Y_std'] + stats['Y_mean']\n",
        "    Y_true = Y_true_norm * stats['Y_std'] + stats['Y_mean']\n",
        "\n",
        "    # Scatter for each output dim\n",
        "    Y_pred_np = Y_pred.numpy()\n",
        "    Y_true_np = Y_true.numpy()\n",
        "    for i, name in enumerate([\"Deformation₁\", \"Deformation₂\", \"Deformation₃\"]):\n",
        "        plot_scatter(Y_true_np[:, i], Y_pred_np[:, i], name)\n",
        "\n",
        "    # MC-Dropout uncertainty example\n",
        "    sample = torch.randn(100, 5).to(device)\n",
        "    mean, std = mc_dropout_predict(model, sample, n_samples=50)\n",
        "    print(\"MC-Dropout sample mean (first 3):\", mean[:3])\n",
        "    print(\"MC-Dropout sample std  (first 3):\", std[:3])"
      ]
    }
  ]
}