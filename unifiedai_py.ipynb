{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyOO4zCOFe+HxOiCNCBkgAdo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OneFineStarstuff/Cosmic-Brilliance/blob/main/unifiedai_py.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch --quiet"
      ],
      "metadata": {
        "id": "7oCdQhP4gSG-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e-X7dyOEY-zq"
      },
      "outputs": [],
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "unifiedai.py\n",
        "============\n",
        "End-to-end unified AI scaffold:\n",
        "Perception → Fusion → Episodic Memory → RSSM → Planner\n",
        "Includes synthetic training loop + planner demo.\n",
        "\"\"\"\n",
        "\n",
        "from __future__ import annotations\n",
        "import argparse, math, random\n",
        "from typing import List, Optional, Tuple, Callable, Union, Dict\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from tqdm import trange\n",
        "\n",
        "# --- Utils ---\n",
        "\n",
        "def default_device() -> torch.device:\n",
        "    return torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "def reparam_gauss(stats: torch.Tensor, rng: Optional[torch.Generator] = None) -> torch.Tensor:\n",
        "    mu, logvar = torch.chunk(stats, 2, dim=-1)\n",
        "    std = (0.5 * logvar).exp()\n",
        "    eps = torch.randn_like(std, generator=rng)\n",
        "    return mu + eps * std\n",
        "\n",
        "def kl_gauss(post: torch.Tensor, prior: torch.Tensor, reduce: bool = True) -> torch.Tensor:\n",
        "    mu_p, lv_p = torch.chunk(post, 2, dim=-1)\n",
        "    mu_q, lv_q = torch.chunk(prior, 2, dim=-1)\n",
        "    kl = 0.5 * (lv_q - lv_p + (lv_p.exp() + (mu_p - mu_q).pow(2)) / lv_q.exp() - 1.0).sum(-1)\n",
        "    return kl.mean() if reduce else kl\n",
        "\n",
        "def maybe_norm(x: torch.Tensor, eps: float = 1e-6) -> torch.Tensor:\n",
        "    return x / (x.norm(dim=-1, keepdim=True) + eps)\n",
        "\n",
        "def pair_rotate(x: torch.Tensor, cos: torch.Tensor, sin: torch.Tensor) -> torch.Tensor:\n",
        "    x1, x2 = x[..., ::2], x[..., 1::2]\n",
        "    y1 = x1 * cos - x2 * sin\n",
        "    y2 = x1 * sin + x2 * cos\n",
        "    return torch.stack([y1, y2], dim=-1).flatten(-2)\n",
        "\n",
        "def build_rope_cache(seq_len: int, dim: int, base: float = 10000.0, device=None, dtype=None):\n",
        "    half = dim // 2\n",
        "    idx = torch.arange(half, device=device, dtype=dtype)\n",
        "    freqs = 1.0 / (base ** (idx / half))\n",
        "    t = torch.arange(seq_len, device=device, dtype=dtype).unsqueeze(1)\n",
        "    angles = t * freqs.unsqueeze(0)\n",
        "    cos = torch.cos(angles).repeat_interleave(2, dim=-1)\n",
        "    sin = torch.sin(angles).repeat_interleave(2, dim=-1)\n",
        "    return cos, sin\n",
        "\n",
        "# --- Perception ---\n",
        "\n",
        "class MockVisionEncoder(nn.Module):\n",
        "    def __init__(self, embed_dim: int = 1024, tokens: int = 16):\n",
        "        super().__init__()\n",
        "        self.tokens = tokens\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Conv2d(3, 32, 3, stride=2, padding=1),\n",
        "            nn.GELU(),\n",
        "            nn.Conv2d(32, 64, 3, stride=2, padding=1),\n",
        "            nn.GELU(),\n",
        "            nn.AdaptiveAvgPool2d((tokens, 1)),\n",
        "        )\n",
        "        self.proj = nn.Linear(64, embed_dim)\n",
        "    def forward(self, img: torch.Tensor) -> torch.Tensor:\n",
        "        x = self.mlp(img)\n",
        "        x = x.squeeze(-1).transpose(1, 2)\n",
        "        return self.proj(x)\n",
        "\n",
        "class MockTextEncoder(nn.Module):\n",
        "    def __init__(self, vocab_size: int = 32000, embed_dim: int = 1024, max_len: int = 128):\n",
        "        super().__init__()\n",
        "        self.embed = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.max_len = max_len\n",
        "    def forward(self, texts: Union[str, List[str]]):\n",
        "        if isinstance(texts, str):\n",
        "            texts = [texts]\n",
        "        toks = [[(ord(c) % 255) + 1 for c in t][: self.max_len] for t in texts]\n",
        "        max_t = max(len(x) for x in toks) if toks else 1\n",
        "        pad_id = 0\n",
        "        ids = torch.full((len(toks), max_t), pad_id, dtype=torch.long)\n",
        "        mask = torch.zeros((len(toks), max_t), dtype=torch.bool)\n",
        "        for i, seq in enumerate(toks):\n",
        "            L = len(seq)\n",
        "            ids[i, :L] = torch.tensor(seq, dtype=torch.long)\n",
        "            mask[i, :L] = True\n",
        "        x = self.embed(ids.to(self.embed.weight.device))\n",
        "        pos = torch.arange(max_t, device=x.device).float()\n",
        "        pe = torch.stack([torch.sin(pos / 10000**(2*k/x.size(-1))) if k % 2 == 0 else torch.cos(pos / 10000**(2*(k-1)/x.size(-1))) for k in range(x.size(-1))], dim=-1)\n",
        "        x = x + pe.unsqueeze(0)\n",
        "        return x, mask.to(x.device)\n",
        "\n",
        "# --- Fusion + Heads ---\n",
        "\n",
        "class TokenPool(nn.Module):\n",
        "    def __init__(self, dim: int):\n",
        "        super().__init__()\n",
        "        self.q = nn.Parameter(torch.randn(dim))\n",
        "    def forward(self, x: torch.Tensor, mask: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
        "        attn = (x * self.q).sum(-1)\n",
        "        if mask is not None:\n",
        "            attn = attn.masked_fill(~mask, -1e9)\n",
        "        w = attn.softmax(-1)\n",
        "        return (w.unsqueeze(-1) * x).sum(1)\n",
        "\n",
        "class FusionObsEncoder(nn.Module):\n",
        "    def __init__(self, embed_dim: int = 1024, obs_dim: int = 1024):\n",
        "        super().__init__()\n",
        "        self.v_pool = TokenPool(embed_dim)\n",
        "        self.t_pool = TokenPool(embed_dim)\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.LayerNorm(embed_dim * 2),\n",
        "            nn.Linear(embed_dim * 2, obs_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(obs_dim, obs_dim),\n",
        "        )\n",
        "    def forward(self, v_tokens: torch.Tensor, t_tokens: torch.Tensor, t_mask: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
        "        v = self.v_pool(v_tokens, None)\n",
        "        t = self.t_pool(t_tokens, t_mask)\n",
        "        return self.mlp(torch.cat([v, t], dim=-1))\n",
        "\n",
        "class RewardHead(nn.Module):\n",
        "    def __init__(self, latent: int = 256):\n",
        "        super().__init__()\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.LayerNorm(latent),\n",
        "            nn.Linear(latent, latent // 2),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(latent // 2, 1),\n",
        "        )\n",
        "    def forward(self, h: torch.Tensor) -> torch.Tensor:\n",
        "        return self.mlp(h).squeeze(-1)\n",
        "\n",
        "class ObsDecoder(nn.Module):\n",
        "    def __init__(self, latent: int = 256, obs_dim: int = 1024):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.LayerNorm(latent * 2),\n",
        "            nn.Linear(latent * 2, obs_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(obs_dim, obs_dim),\n",
        "        )\n",
        "    def forward(self, h: torch.Tensor, z: torch.Tensor) -> torch.Tensor:\n",
        "        return self.net(torch.cat([h, z], dim=-1))\n",
        "\n",
        "# --- Episodic Memory ---\n",
        "\n",
        "class EpisodicMemory(nn.Module):\n",
        "    def __init__(self, dim: int = 1024, slots: int = 1024, temperature: float = 0.1, ema: float = 0.1):\n",
        "        super().__init__()\n",
        "        self.dim, self.slots = dim, slots\n",
        "        self.temperature, self.ema = temperature, ema\n",
        "        self.register_buffer(\"keys\", F.normalize(torch.randn(slots, dim), dim=-1))\n",
        "        self.register_buffer(\"values\", torch.zeros(slots, dim))\n",
        "        self.register_buffer(\"age\", torch.zeros(slots, dtype=torch.long))\n",
        "        self.register_buffer(\"ptr\", torch.zeros((), dtype=torch.long))\n",
        "    @torch.no_grad()\n",
        "    def write(self, k: torch.Tensor, v: torch.Tensor, strategy: str = \"nearest\"):\n",
        "        k = k.unsqueeze(0) if k.dim() == 1 else k\n",
        "        v = v.unsqueeze(0) if v.dim() == 1 else v\n",
        "        k = maybe_norm(k)\n",
        "        if strategy == \"ring\":\n",
        "            for i in range(k.size(0)):\n",
        "                idx = int(self.ptr.item() % self.slots)\n",
        "                self.keys[idx] = k[i]\n",
        "                self.values[idx] = v[i]\n",
        "                self.age[idx] = 0\n",
        "                self.ptr += 1\n",
        "        else:\n",
        "            sims = (maybe_norm(self.keys) @ k.T)\n",
        "            idxs = sims.argmax(dim=0)\n",
        "            for i, idx in enumerate(idxs.tolist()):\n",
        "                self.keys[idx] = F.normalize((1 - self.ema) * self.keys[idx] + self.ema * k[i], dim=-1)\n",
        "                self.values[idx] = (1 - self.ema) * self.values[idx] + self.ema * v[i]\n",
        "                self.age[idx] = 0\n",
        "        self.age += 1\n",
        "\n",
        "    def read(self, q: torch.Tensor, topk: int = 0) -> torch.Tensor:\n",
        "        single = q.dim() == 1\n",
        "        q = q.unsqueeze(0) if single else q\n",
        "        logits = (maybe_norm(q) @ maybe_norm(self.keys).T) / self.temperature\n",
        "        if topk and topk < self.slots:\n",
        "            vals, idxs = logits.topk(topk, dim=-1)\n",
        "            w = F.softmax(vals, dim=-1)\n",
        "            out = (w.unsqueeze(-1) * self.values[idxs]).sum(1)\n",
        "        else:\n",
        "            w = F.softmax(logits, dim=-1)\n",
        "            out = w @ self.values\n",
        "        return out.squeeze(0) if single else out\n",
        "\n",
        "# --- RSSM ---\n",
        "\n",
        "class LatentRSSM(nn.Module):\n",
        "    def __init__(self, latent: int = 256, action_dim: int = 32, obs_dim: int = 1024):\n",
        "        super().__init__()\n",
        "        self.latent, self.action_dim, self.obs_dim = latent, action_dim, obs_dim\n",
        "        self.gru = nn.GRUCell(latent + action_dim, latent)\n",
        "        self.prior = nn.Sequential(nn.LayerNorm(latent), nn.Linear(latent, 2 * latent))\n",
        "        self.post = nn.Sequential(nn.LayerNorm(latent + obs_dim), nn.Linear(latent + obs_dim, 2 * latent))\n",
        "    def init_state(self, batch: int, device=None):\n",
        "        device = device or default_device()\n",
        "        return torch.zeros(batch, self.latent, device=device), torch.zeros(batch, self.latent, device=device)\n",
        "    def step(self, h, z, action, obs_embed=None, rng=None):\n",
        "        x = torch.cat([z, action], dim=-1)\n",
        "        h = self.gru(x, h)\n",
        "        prior_stats = self.prior(h)\n",
        "        if obs_embed is not None:\n",
        "            post_stats = self.post(torch.cat([h, obs_embed], dim=-1))\n",
        "            z = reparam_gauss(post_stats, rng=rng)\n",
        "        else:\n",
        "            post_stats = None\n",
        "            z = reparam_gauss(prior_stats, rng=rng)\n",
        "        return h, z, prior_stats, post_stats\n",
        "\n",
        "# --- LLM stub + Policy/Reward ---\n",
        "\n",
        "class DummyLLM:\n",
        "    def __init__(self, action_vocab: List[str]):\n",
        "        self.action_vocab = action_vocab\n",
        "    def propose_actions(self, goal_desc: str, max_steps: int = 5) -> List[str]:\n",
        "        random.seed(hash(goal_desc) % 10000)\n",
        "        return [self.action_vocab[i % len(self.action_vocab)] for i in range(max_steps)]\n",
        "    def revise_actions(self, plan: List[str], max_steps: int = 5) -> List[str]:\n",
        "        return (plan + plan)[1 : 1 + max_steps]\n",
        "\n",
        "class TextualPolicyToAction:\n",
        "    def __init__(self, action_dim: int = 32, actions: Dict[str, int] | None = None):\n",
        "        self.actions = actions or {n: i for i, n in enumerate(\n",
        "            [\"move_left\", \"move_right\", \"move_up\", \"move_down\", \"pick\", \"place\", \"wait\", \"scan\"]\n",
        "        )}\n",
        "        self.n = max(self.actions.values()) + 1\n",
        "        self.emb = nn.Embedding(self.n, action_dim)\n",
        "        with torch.no_grad():\n",
        "            nn.init.normal_(self.emb.weight, std=0.2)\n",
        "    def __call__(self, action_text: str, device: torch.device, action_dim: int) -> torch.Tensor:\n",
        "        idx = torch.tensor(self.actions.get(action_text, 0), device=device, dtype=torch.long)\n",
        "        vec = self.emb(idx)\n",
        "        if vec.size(-1) != action_dim:\n",
        "            W = torch.empty(vec.size(-1), action_dim, device=device)\n",
        "            nn.init.kaiming_uniform_(W, a=math.sqrt(5))\n",
        "            vec = vec @ W\n",
        "        return vec\n",
        "\n",
        "class SimpleReward:\n",
        "    def __init__(self, latent: int = 256):\n",
        "        self.goal = torch.zeros(latent)\n",
        "    def set_goal(self, g: torch.Tensor):\n",
        "        self.goal = g.detach()\n",
        "    def __call__(self, h: torch.Tensor) -> torch.Tensor:\n",
        "        g = self.goal.to(h.device).expand_as(h)\n",
        "        return -((h - g) ** 2).sum(dim=-1)\n",
        "\n",
        "# --- Planner ---\n",
        "\n",
        "class HybridPlanner(nn.Module):\n",
        "    def __init__(self, llm, rssm, action_fn, reward_fn, validate_fn=None, max_depth: int = 5):\n",
        "        super().__init__()\n",
        "        self.llm, self.rssm = llm, rssm\n",
        "        self.action_fn, self.reward_fn = action_fn, reward_fn\n",
        "        self.validate_fn, self.max_depth = validate_fn, max_depth\n",
        "    @torch.no_grad()\n",
        "    def plan(self, goal_desc, init_h, init_z, batch: int = 1):\n",
        "        steps = self.llm.propose_actions(goal_desc, max_steps=self.max_depth)\n",
        "        if self.validate_fn is not None and not self.validate_fn(steps):\n",
        "            steps = self.llm.revise_actions(plan=steps, max_steps=self.max_depth)\n",
        "        device = init_h.device\n",
        "        h, z = init_h, init_z\n",
        "        traj, total_reward = [], torch.zeros((), device=device)\n",
        "        for s in steps[: self.max_depth]:\n",
        "            act = self.action_fn(s, device, self.rssm.action_dim)\n",
        "            act = act.unsqueeze(0) if act.dim() == 1 else act\n",
        "            if act.size(0) != batch:\n",
        "                act = act.expand(batch, -1)\n",
        "            h, z, _, _ = self.rssm.step(h, z, act, obs_embed=None)\n",
        "            r_t = self.reward_fn(h)\n",
        "            total_reward = total_reward + r_t.mean()\n",
        "            traj.append((h.clone(), act.clone(), r_t.clone()))\n",
        "        return traj, total_reward\n",
        "\n",
        "# --- Synthetic env + train/demo ---\n",
        "\n",
        "def synthetic_env_step(state: torch.Tensor, action: torch.Tensor) -> torch.Tensor:\n",
        "    B, S = state.shape\n",
        "    A = action.shape[-1]\n",
        "    W_s = torch.eye(S, device=state.device) * 0.95\n",
        "    W_a = torch.randn(A, S, device=state.device) * 0.05\n",
        "    noise = torch.randn_like(state) * 0.01\n",
        "    return state @ W_s.T + action @ W_a + noise\n",
        "\n",
        "def build_obs_from_state(state: torch.Tensor, obs_dim: int) -> torch.Tensor:\n",
        "    S = state.size(-1)\n",
        "    W = torch.randn(S, obs_dim, device=state.device) / (S ** 0.5)\n",
        "    return state @ W\n",
        "\n",
        "def train_loop(steps: int, batch: int, device: torch.device):\n",
        "    torch.manual_seed(42)\n",
        "    E, latent, action_dim, obs_dim = 1024, 256, 32, 1024\n",
        "    venc, tenc = MockVisionEncoder(E).to(device), MockTextEncoder(embed_dim=E).to(device)\n",
        "    fuse = FusionObsEncoder(E, obs_dim).to(device)\n",
        "    rssm = LatentRSSM(latent, action_dim, obs_dim).to(device)\n",
        "    dec = ObsDecoder(latent, obs_dim).to(device)\n",
        "    mem = EpisodicMemory(obs_dim, slots=512).to(device)\n",
        "\n",
        "    params = list(rssm.parameters()) + list(dec.parameters()) + list(fuse.parameters())\n",
        "    opt = optim.AdamW(params, lr=3e-4, weight_decay=1e-4)\n",
        "    state = torch.zeros(batch, latent, device=device)\n",
        "    free_nats, kl_scale = 1.0, 1.0\n",
        "    pbar = trange(steps, desc=\"train\")\n",
        "    h, z = rssm.init_state(batch, device=device)\n",
        "\n",
        "    for _ in pbar:\n",
        "        imgs = torch.randn(batch, 3, 128, 128, device=device)\n",
        "        vtoks = venc(imgs)\n",
        "        ttoks, tmask = tenc([\"demo text\"] * batch)\n",
        "        obs_embed = fuse(vtoks, ttoks, tmask)\n",
        "        target_obs = build_obs_from_state(state, obs_dim)\n",
        "        action = torch.tanh(torch.randn(batch, action_dim, device=device))\n",
        "        h, z, prior_stats, post_stats = rssm.step(h, z, action, obs_embed=obs_embed)\n",
        "        pred_obs = dec(h, z)\n",
        "        recon = F.mse_loss(pred_obs, target_obs)\n",
        "        kl = kl_gauss(post_stats, prior_stats, reduce=False).mean()\n",
        "        kl_free = torch.clamp(kl - free_nats, min=0.0)\n",
        "        loss = recon + kl_scale * kl_free\n",
        "        opt.zero_grad(); loss.backward(); torch.nn.utils.clip_grad_norm_(params, 1.0); opt.step()\n",
        "        state = synthetic_env_step(state, action)\n",
        "\n",
        "        # Episodic memory: write current fused obs and target obs; read for auxiliary retrieval\n",
        "        mem.write(obs_embed.detach(), target_obs.detach())\n",
        "        retrieved = mem.read(obs_embed, topk=8)\n",
        "        aux = F.mse_loss(retrieved, target_obs.detach())\n",
        "\n",
        "        total = (recon + kl_scale * kl_free + 0.1 * aux).detach().item()\n",
        "        pbar.set_postfix(loss=total, recon=recon.detach().item(), kl=kl.detach().item(), aux=aux.detach().item())\n",
        "\n",
        "        opt.zero_grad()\n",
        "        (recon + kl_scale * kl_free + 0.1 * aux).backward()\n",
        "        torch.nn.utils.clip_grad_norm_(params, 1.0)\n",
        "        opt.step()\n",
        "\n",
        "    return {\n",
        "        \"vision\": venc,\n",
        "        \"text\": tenc,\n",
        "        \"fuse\": fuse,\n",
        "        \"rssm\": rssm,\n",
        "        \"decoder\": dec,\n",
        "        \"memory\": mem,\n",
        "    }\n",
        "\n",
        "# --- Demo planning ---\n",
        "\n",
        "@torch.no_grad()\n",
        "def demo_planning(artifacts: dict, goal_desc: str, batch: int, plan_depth: int, device: torch.device):\n",
        "    rssm: LatentRSSM = artifacts[\"rssm\"]\n",
        "    mem: EpisodicMemory = artifacts[\"memory\"]\n",
        "\n",
        "    # Build LLM, action mapper, reward\n",
        "    default_actions = [\"move_left\", \"move_right\", \"move_up\", \"move_down\", \"pick\", \"place\", \"wait\", \"scan\"]\n",
        "    llm = DummyLLM(action_vocab=default_actions)\n",
        "    action_fn = TextualPolicyToAction(action_dim=rssm.action_dim)\n",
        "    reward_fn = SimpleReward(latent=rssm.latent)\n",
        "    planner = HybridPlanner(llm, rssm, action_fn, reward_fn, validate_fn=None, max_depth=plan_depth).to(device)\n",
        "\n",
        "    # Initialize latent state\n",
        "    h, z = rssm.init_state(batch, device=device)\n",
        "\n",
        "    # Optional: set a simple goal as the running average of memory values (if any)\n",
        "    if mem.values.numel() > 0 and mem.age.max() > 0:\n",
        "        # Use a small top-k read from a random query to seed a goal\n",
        "        probe = torch.randn(batch, mem.values.size(-1), device=device)\n",
        "        goal_vec = mem.read(probe, topk=8).mean(0)\n",
        "        # Project to latent size with a small random map (no grads)\n",
        "        P = torch.randn(mem.values.size(-1), rssm.latent, device=device) / (mem.values.size(-1) ** 0.5)\n",
        "        reward_fn.set_goal(goal_vec @ P)\n",
        "    # Else default goal is zero (already set)\n",
        "\n",
        "    traj, total_reward = planner.plan(goal_desc, h, z, batch=batch)\n",
        "\n",
        "    print(\"Plan summary\")\n",
        "    print(f\"- Steps: {len(traj)}\")\n",
        "    print(f\"- Total reward (mean over batch per step): {total_reward.item():.4f}\")\n",
        "    # Show last hidden norm for a quick sanity metric\n",
        "    last_h, _, _ = traj[-1]\n",
        "    print(f\"- Last hidden L2 norm (avg over batch): {last_h.norm(dim=-1).mean().item():.3f}\")\n",
        "\n",
        "# --- Entry point ---\n",
        "\n",
        "def select_device(name: str) -> torch.device:\n",
        "    if name == \"auto\":\n",
        "        return torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    return torch.device(name)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    import argparse\n",
        "    import random\n",
        "\n",
        "    parser = argparse.ArgumentParser(description=\"UnifiedAI minimal end-to-end demo\")\n",
        "    parser.add_argument(\"--train-steps\", type=int, default=200, help=\"Training iterations\")\n",
        "    parser.add_argument(\"--batch\", type=int, default=16, help=\"Batch size\")\n",
        "    parser.add_argument(\"--plan-depth\", type=int, default=5, help=\"Planner horizon\")\n",
        "    parser.add_argument(\"--device\", type=str, default=\"auto\", help=\"cpu | cuda | auto\")\n",
        "    parser.add_argument(\"--goal\", type=str, default=\"navigate to target and stabilize\", help=\"High-level goal text\")\n",
        "    parser.add_argument(\"--seed\", type=int, default=42, help=\"Random seed\")\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    # Seeding\n",
        "    torch.manual_seed(args.seed)\n",
        "    random.seed(args.seed)\n",
        "\n",
        "    device = select_device(args.device)\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    artifacts = train_loop(steps=args.train_steps, batch=args.batch, device=device)\n",
        "    demo_planning(artifacts, goal_desc=args.goal, batch=args.batch, plan_depth=args.plan_depth, device=device)\n",
        "\n",
        "# --- Checkpointing and reproducibility ---\n",
        "\n",
        "def set_seed(seed: int) -> None:\n",
        "    random.seed(seed)\n",
        "    np_rand = None\n",
        "    try:\n",
        "        import numpy as _np\n",
        "        np_rand = _np\n",
        "    except Exception:\n",
        "        np_rand = None\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "    if np_rand is not None:\n",
        "        np_rand.random.seed(seed)\n",
        "\n",
        "def artifact_state(artifacts: dict) -> dict:\n",
        "    rssm: LatentRSSM = artifacts[\"rssm\"]\n",
        "    mem: EpisodicMemory = artifacts[\"memory\"]\n",
        "    venc = artifacts[\"vision\"]\n",
        "    tenc = artifacts[\"text\"]\n",
        "    fuse: nn.Module = artifacts[\"fuse\"]\n",
        "    dec: nn.Module = artifacts[\"decoder\"]\n",
        "\n",
        "    # Try to infer encoder embed size E, fall back to rssm.obs_dim\n",
        "    E = getattr(tenc, \"embed_dim\", getattr(venc, \"embed_dim\", rssm.obs_dim))\n",
        "    meta = {\n",
        "        \"E\": int(E),\n",
        "        \"latent\": int(rssm.latent),\n",
        "        \"action_dim\": int(rssm.action_dim),\n",
        "        \"obs_dim\": int(rssm.obs_dim),\n",
        "        \"memory\": {\n",
        "            \"slots\": int(mem.slots),\n",
        "            \"temperature\": float(mem.temperature),\n",
        "            \"ema\": float(mem.ema),\n",
        "        },\n",
        "        \"versions\": {\n",
        "            \"torch\": torch.__version__,\n",
        "            \"unifiedai\": \"0.1.0\",\n",
        "        },\n",
        "    }\n",
        "    return {\n",
        "        \"meta\": meta,\n",
        "        \"state\": {\n",
        "            \"vision\": venc.state_dict(),\n",
        "            \"text\": tenc.state_dict(),\n",
        "            \"fuse\": fuse.state_dict(),\n",
        "            \"rssm\": rssm.state_dict(),\n",
        "            \"decoder\": dec.state_dict(),\n",
        "            \"memory\": {\n",
        "                \"keys\": mem.keys,\n",
        "                \"values\": mem.values,\n",
        "                \"age\": mem.age,\n",
        "            },\n",
        "        },\n",
        "    }\n",
        "\n",
        "def save_artifacts(artifacts: dict, path: str) -> None:\n",
        "    pkg = artifact_state(artifacts)\n",
        "    torch.save(pkg, path)\n",
        "\n",
        "def load_artifacts(path: str, device: torch.device = None) -> dict:\n",
        "    device = device or select_device(\"auto\")\n",
        "    pkg = torch.load(path, map_location=device)\n",
        "    meta = pkg[\"meta\"]\n",
        "    E = meta[\"E\"]; latent = meta[\"latent\"]; action_dim = meta[\"action_dim\"]; obs_dim = meta[\"obs_dim\"]\n",
        "    mem_cfg = meta[\"memory\"]\n",
        "\n",
        "    # Rebuild modules\n",
        "    venc = MockVisionEncoder(E).to(device)\n",
        "    tenc = MockTextEncoder(embed_dim=E).to(device)\n",
        "    fuse = FusionObsEncoder(E, obs_dim).to(device)\n",
        "    rssm = LatentRSSM(latent, action_dim, obs_dim).to(device)\n",
        "    dec = ObsDecoder(latent, obs_dim).to(device)\n",
        "    mem = EpisodicMemory(obs_dim, slots=mem_cfg[\"slots\"], temperature=mem_cfg[\"temperature\"], ema=mem_cfg[\"ema\"]).to(device)\n",
        "\n",
        "    # Load weights\n",
        "    venc.load_state_dict(pkg[\"state\"][\"vision\"])\n",
        "    tenc.load_state_dict(pkg[\"state\"][\"text\"])\n",
        "    fuse.load_state_dict(pkg[\"state\"][\"fuse\"])\n",
        "    rssm.load_state_dict(pkg[\"state\"][\"rssm\"])\n",
        "    dec.load_state_dict(pkg[\"state\"][\"decoder\"])\n",
        "\n",
        "    with torch.no_grad():\n",
        "        mem.keys.copy_(pkg[\"state\"][\"memory\"][\"keys\"].to(device))\n",
        "        mem.values.copy_(pkg[\"state\"][\"memory\"][\"values\"].to(device))\n",
        "        mem.age.copy_(pkg[\"state\"][\"memory\"][\"age\"].to(device))\n",
        "\n",
        "    return {\n",
        "        \"vision\": venc,\n",
        "        \"text\": tenc,\n",
        "        \"fuse\": fuse,\n",
        "        \"rssm\": rssm,\n",
        "        \"decoder\": dec,\n",
        "        \"memory\": mem,\n",
        "    }\n",
        "\n",
        "# --- Smoke tests (optional, call manually) ---\n",
        "\n",
        "def run_smoke_tests(device: torch.device = None) -> None:\n",
        "    device = device or select_device(\"auto\")\n",
        "    print(\"[TEST] device:\", device)\n",
        "\n",
        "    # Shapes and forward passes\n",
        "    E, latent, action_dim, obs_dim = 256, 128, 16, 512\n",
        "    venc = MockVisionEncoder(E).to(device)\n",
        "    tenc = MockTextEncoder(embed_dim=E).to(device)\n",
        "    fuse = FusionObsEncoder(E, obs_dim).to(device)\n",
        "    rssm = LatentRSSM(latent, action_dim, obs_dim).to(device)\n",
        "    dec = ObsDecoder(latent, obs_dim).to(device)\n",
        "    mem = EpisodicMemory(obs_dim, slots=64).to(device)\n",
        "\n",
        "    B = 4\n",
        "    imgs = torch.randn(B, 3, 128, 128, device=device)\n",
        "    vtoks = venc(imgs)\n",
        "    texts = [\"hello world\"] * B\n",
        "    ttoks, tmask = tenc(texts)\n",
        "    fused = fuse(vtoks, ttoks, tmask)\n",
        "    assert fused.shape == (B, obs_dim), \"[FAIL] Fusion output shape mismatch\"\n",
        "    print(\"[PASS] fusion\")\n",
        "\n",
        "    h, z = rssm.init_state(B, device=device)\n",
        "    act = torch.randn(B, action_dim, device=device)\n",
        "    h2, z2, prior, post = rssm.step(h, z, act, obs_embed=fused)\n",
        "    pred = dec(h2, z2)\n",
        "    assert pred.shape == (B, obs_dim), \"[FAIL] Decoder output shape mismatch\"\n",
        "    print(\"[PASS] rssm+decoder\")\n",
        "\n",
        "    mem.write(fused, pred.detach())\n",
        "    out = mem.read(fused, topk=8)\n",
        "    assert out.shape == (B, obs_dim), \"[FAIL] Memory read shape mismatch\"\n",
        "    print(\"[PASS] memory\")\n",
        "\n",
        "    # Planner roll\n",
        "    default_actions = [\"move_left\", \"move_right\", \"move_up\", \"move_down\", \"pick\", \"place\", \"wait\", \"scan\"]\n",
        "    llm = DummyLLM(default_actions)\n",
        "    action_fn = TextualPolicyToAction(action_dim=action_dim)\n",
        "    reward_fn = SimpleReward(latent=latent)\n",
        "    planner = HybridPlanner(llm, rssm, action_fn, reward_fn, max_depth=3).to(device)\n",
        "    traj, R = planner.plan(\"test goal\", h2, z2, batch=B)\n",
        "    assert len(traj) == 3 and isinstance(R, torch.Tensor), \"[FAIL] Planner trajectory\"\n",
        "    print(\"[PASS] planner\")\n",
        "    print(\"[TEST] All smoke tests passed.\")\n",
        "\n",
        "# --- Public API ---\n",
        "\n",
        "__version__ = \"0.1.0\"\n",
        "__all__ = [\n",
        "    \"MockVisionEncoder\",\n",
        "    \"MockTextEncoder\",\n",
        "    \"FusionObsEncoder\",\n",
        "    \"ObsDecoder\",\n",
        "    \"EpisodicMemory\",\n",
        "    \"LatentRSSM\",\n",
        "    \"DummyLLM\",\n",
        "    \"TextualPolicyToAction\",\n",
        "    \"SimpleReward\",\n",
        "    \"HybridPlanner\",\n",
        "    \"synthetic_env_step\",\n",
        "    \"build_obs_from_state\",\n",
        "    \"train_loop\",\n",
        "    \"demo_planning\",\n",
        "    \"select_device\",\n",
        "    \"set_seed\",\n",
        "    \"artifact_state\",\n",
        "    \"save_artifacts\",\n",
        "    \"load_artifacts\",\n",
        "    \"run_smoke_tests\",\n",
        "]"
      ]
    }
  ]
}