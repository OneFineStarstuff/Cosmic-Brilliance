{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyNb6pv8K2Dx5s4BanNfAMhR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OneFineStarstuff/Cosmic-Brilliance/blob/main/Grid.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "# -*- coding: utf-8 -*-\n",
        "\n",
        "from __future__ import annotations\n",
        "from dataclasses import dataclass\n",
        "from typing import Any, Dict, Iterable, Iterator, List, Optional, Protocol, Tuple\n",
        "from collections import deque, Counter\n",
        "\n",
        "# =========================\n",
        "# Episode and report types\n",
        "# =========================\n",
        "\n",
        "@dataclass\n",
        "class Episode:\n",
        "    input: str\n",
        "    outcome: str\n",
        "\n",
        "@dataclass\n",
        "class Contradiction:\n",
        "    episode: Episode\n",
        "    prediction: str\n",
        "    score: float\n",
        "\n",
        "@dataclass\n",
        "class TrainingReport:\n",
        "    step: int\n",
        "    processed: int\n",
        "    contradictory: int\n",
        "    threshold: float\n",
        "    avg_score_all: float\n",
        "    avg_score_contradictions: float\n",
        "\n",
        "\n",
        "# =========================\n",
        "# Scheduler\n",
        "# =========================\n",
        "\n",
        "class LinearThresholdScheduler:\n",
        "    def __init__(self, start: float, end: float, total_steps: int):\n",
        "        if total_steps <= 0:\n",
        "            raise ValueError(\"total_steps must be > 0\")\n",
        "        self.start = start\n",
        "        self.end = end\n",
        "        self.total = total_steps\n",
        "        self.step_idx = 0\n",
        "\n",
        "    def value(self) -> float:\n",
        "        t = min(self.step_idx, self.total)\n",
        "        alpha = t / self.total\n",
        "        return (1 - alpha) * self.start + alpha * self.end\n",
        "\n",
        "    def step(self) -> None:\n",
        "        self.step_idx += 1\n",
        "\n",
        "\n",
        "# =========================\n",
        "# Reflective updater\n",
        "# =========================\n",
        "\n",
        "class ReflectiveUpdater:\n",
        "    def __init__(\n",
        "        self,\n",
        "        episodic_memory: \"EpisodicMemory\",\n",
        "        world_model: \"UpdatableWorldModel\",\n",
        "        self_model: \"UpdatableSelfModel\",\n",
        "        similarity_fn,\n",
        "        threshold: float = 0.8,\n",
        "    ):\n",
        "        self.mem = episodic_memory\n",
        "        self.world = world_model\n",
        "        self.self_model = self_model\n",
        "        self.similarity = similarity_fn\n",
        "        self.threshold = threshold\n",
        "        self._train_step = 0\n",
        "\n",
        "    def _chunk(self, it: Iterable[Episode], size: int) -> Iterator[List[Episode]]:\n",
        "        batch: List[Episode] = []\n",
        "        for e in it:\n",
        "            batch.append(e)\n",
        "            if len(batch) >= size:\n",
        "                yield batch\n",
        "                batch = []\n",
        "        if batch:\n",
        "            yield batch\n",
        "\n",
        "    def detect_contradictions_detailed(self, limit: Optional[int] = None, batch_size: int = 128) -> List[Contradiction]:\n",
        "        episodes = list(self.mem.recent())\n",
        "        if limit is not None:\n",
        "            episodes = episodes[:limit]\n",
        "\n",
        "        contradictions: List[Contradiction] = []\n",
        "        for batch in self._chunk(episodes, batch_size):\n",
        "            for ep in batch:\n",
        "                pred = self.world.simulate(ep.input)\n",
        "                score = self.similarity(pred, ep.outcome)\n",
        "                if score < self.threshold:\n",
        "                    contradictions.append(Contradiction(ep, pred, score))\n",
        "        return contradictions\n",
        "\n",
        "    def revise_model_batched(self, batch_size: int = 128) -> int:\n",
        "        contradictions = self.detect_contradictions_detailed(limit=None, batch_size=batch_size)\n",
        "        if contradictions:\n",
        "            eps = [c.episode for c in contradictions]\n",
        "            self.world.update(eps)\n",
        "            self.self_model.adapt(eps)\n",
        "        return len(contradictions)\n",
        "\n",
        "    def revise_model(self) -> int:\n",
        "        return self.revise_model_batched()\n",
        "\n",
        "    def training_step(\n",
        "        self,\n",
        "        max_episodes: Optional[int] = None,\n",
        "        batch_size: int = 128,\n",
        "        scheduler: Optional[LinearThresholdScheduler] = None,\n",
        "    ) -> TrainingReport:\n",
        "        if scheduler is not None:\n",
        "            self.threshold = scheduler.value()\n",
        "\n",
        "        episodes = list(self.mem.recent())\n",
        "        if max_episodes is not None:\n",
        "            episodes = episodes[:max_episodes]\n",
        "\n",
        "        scores: List[float] = []\n",
        "        contra_scores: List[float] = []\n",
        "        contradictions: List[Episode] = []\n",
        "\n",
        "        for batch in self._chunk(episodes, batch_size):\n",
        "            for ep in batch:\n",
        "                pred = self.world.simulate(ep.input)\n",
        "                s = self.similarity(pred, ep.outcome)\n",
        "                scores.append(s)\n",
        "                if s < self.threshold:\n",
        "                    contradictions.append(ep)\n",
        "                    contra_scores.append(s)\n",
        "\n",
        "        if contradictions:\n",
        "            self.world.update(contradictions)\n",
        "            self.self_model.adapt(contradictions)\n",
        "\n",
        "        self._train_step += 1\n",
        "        if scheduler is not None:\n",
        "            scheduler.step()\n",
        "\n",
        "        avg_all = sum(scores) / len(scores) if scores else 0.0\n",
        "        avg_contra = sum(contra_scores) / len(contra_scores) if contra_scores else 0.0\n",
        "\n",
        "        return TrainingReport(\n",
        "            step=self._train_step,\n",
        "            processed=len(episodes),\n",
        "            contradictory=len(contradictions),\n",
        "            threshold=self.threshold,\n",
        "            avg_score_all=avg_all,\n",
        "            avg_score_contradictions=avg_contra,\n",
        "        )\n",
        "\n",
        "\n",
        "# =========================\n",
        "# Protocols for models\n",
        "# =========================\n",
        "\n",
        "class UpdatableWorldModel(Protocol):\n",
        "    def simulate(self, x: Any) -> str: ...\n",
        "    def update(self, contradictory: List[Episode]) -> None: ...\n",
        "\n",
        "class UpdatableSelfModel(Protocol):\n",
        "    def adapt(self, contradictory: List[Episode]) -> None: ...\n",
        "\n",
        "\n",
        "# =========================\n",
        "# Memory\n",
        "# =========================\n",
        "\n",
        "class EpisodicMemory:\n",
        "    def __init__(self):\n",
        "        self._episodes: List[Episode] = []\n",
        "\n",
        "    def store(self, episode: Episode) -> None:\n",
        "        self._episodes.append(episode)\n",
        "\n",
        "    def recent(self) -> Iterable[Episode]:\n",
        "        return list(self._episodes)\n",
        "\n",
        "\n",
        "# =========================\n",
        "# Grid world simulation\n",
        "# =========================\n",
        "\n",
        "def init_world(\n",
        "    width: int = 5,\n",
        "    height: int = 5,\n",
        "    agent_pos: Tuple[int, int] = (0, 0),\n",
        "    goal_pos: Tuple[int, int] = (3, 3),\n",
        "    forbidden: Optional[List[Tuple[int, int]]] = None,\n",
        ") -> Dict[str, Any]:\n",
        "    return {\n",
        "        \"width\": width,\n",
        "        \"height\": height,\n",
        "        \"agent_pos\": agent_pos,\n",
        "        \"goal_pos\": goal_pos,\n",
        "        \"forbidden\": set(forbidden or [(1, 1)]),\n",
        "    }\n",
        "\n",
        "def in_bounds(pos: Tuple[int, int], width: int, height: int) -> bool:\n",
        "    x, y = pos\n",
        "    return 0 <= x < width and 0 <= y < height\n",
        "\n",
        "def manhattan(a: Tuple[int, int], b: Tuple[int, int]) -> int:\n",
        "    return abs(a[0] - b[0]) + abs(a[1] - b[1])\n",
        "\n",
        "def apply_physics(state: Dict[str, Any], actions: Dict[str, Any], rules: Dict[str, Any]) -> Dict[str, Any]:\n",
        "    width, height = state[\"width\"], state[\"height\"]\n",
        "    x, y = state[\"agent_pos\"]\n",
        "\n",
        "    act = actions.get(\"agent\", {})\n",
        "    if act.get(\"type\") == \"MOVE\":\n",
        "        dx, dy = 0, 0\n",
        "        d = act.get(\"dir\")\n",
        "        if d == \"UP\":    dy = -1\n",
        "        if d == \"DOWN\":  dy =  1\n",
        "        if d == \"LEFT\":  dx = -1\n",
        "        if d == \"RIGHT\": dx =  1\n",
        "        nx, ny = x + dx, y + dy\n",
        "        if in_bounds((nx, ny), width, height):\n",
        "            state = {**state, \"agent_pos\": (nx, ny)}\n",
        "    return state\n",
        "\n",
        "class SimpleEthics:\n",
        "    # Disallow stepping onto forbidden cells via NOOP.\n",
        "    def filter_actions(self, state: Dict[str, Any], actions: Dict[str, Any]) -> Dict[str, Any]:\n",
        "        act = actions.get(\"agent\", {})\n",
        "        if act.get(\"type\") != \"MOVE\":\n",
        "            return actions\n",
        "\n",
        "        x, y = state[\"agent_pos\"]\n",
        "        d = act.get(\"dir\")\n",
        "        dx, dy = 0, 0\n",
        "        if d == \"UP\":    dy = -1\n",
        "        if d == \"DOWN\":  dy =  1\n",
        "        if d == \"LEFT\":  dx = -1\n",
        "        if d == \"RIGHT\": dx =  1\n",
        "        target = (x + dx, y + dy)\n",
        "        if target in state[\"forbidden\"]:\n",
        "            return {\"agent\": {\"type\": \"NOOP\"}}\n",
        "        return actions\n",
        "\n",
        "\n",
        "# =========================\n",
        "# Agent core with BFS pathfinding\n",
        "# =========================\n",
        "\n",
        "class ListMemoryForAgent:\n",
        "    def __init__(self) -> None:\n",
        "        self.buffer: List[Dict[str, Any]] = []\n",
        "\n",
        "    def store(self, feedback: Dict[str, Any]) -> None:\n",
        "        self.buffer.append(feedback)\n",
        "\n",
        "class PathfindingCore:\n",
        "    def __init__(self) -> None:\n",
        "        self.last_reward: float = 0.0\n",
        "\n",
        "    def encode(self, env_state: Dict[str, Any]) -> Dict[str, Any]:\n",
        "        ax, ay = env_state[\"agent_pos\"]\n",
        "        gx, gy = env_state[\"goal_pos\"]\n",
        "        width, height = env_state[\"width\"], env_state[\"height\"]\n",
        "        forbidden = set(env_state.get(\"forbidden\", set()))\n",
        "        return {\n",
        "            \"agent_pos\": (ax, ay),\n",
        "            \"goal_pos\": (gx, gy),\n",
        "            \"width\": width,\n",
        "            \"height\": height,\n",
        "            \"forbidden\": forbidden,\n",
        "        }\n",
        "\n",
        "    def _bfs_next_dir(\n",
        "        self,\n",
        "        start: Tuple[int, int],\n",
        "        goal: Tuple[int, int],\n",
        "        width: int,\n",
        "        height: int,\n",
        "        forbidden: set[Tuple[int, int]],\n",
        "    ) -> Optional[str]:\n",
        "        if start == goal:\n",
        "            return None\n",
        "        q = deque([start])\n",
        "        visited = {start}\n",
        "        parent: Dict[Tuple[int, int], Tuple[int, int]] = {}\n",
        "        moves = [((0,-1), \"UP\"), ((0,1), \"DOWN\"), ((-1,0), \"LEFT\"), ((1,0), \"RIGHT\")]\n",
        "\n",
        "        while q:\n",
        "            cur = q.popleft()\n",
        "            if cur == goal:\n",
        "                break\n",
        "            for (dx, dy), _ in moves:\n",
        "                nx, ny = cur[0] + dx, cur[1] + dy\n",
        "                nxt = (nx, ny)\n",
        "                if not in_bounds(nxt, width, height):\n",
        "                    continue\n",
        "                if nxt in forbidden:\n",
        "                    continue\n",
        "                if nxt in visited:\n",
        "                    continue\n",
        "                visited.add(nxt)\n",
        "                parent[nxt] = cur\n",
        "                q.append(nxt)\n",
        "\n",
        "        if goal not in parent and start != goal:\n",
        "            return None\n",
        "\n",
        "        # Reconstruct one step: backtrack from goal to start\n",
        "        node = goal\n",
        "        while parent.get(node) and parent[node] != start:\n",
        "            node = parent[node]\n",
        "        # node is the first step from start\n",
        "        dx, dy = node[0] - start[0], node[1] - start[1]\n",
        "        if (dx, dy) == (0, -1): return \"UP\"\n",
        "        if (dx, dy) == (0, 1):  return \"DOWN\"\n",
        "        if (dx, dy) == (-1, 0): return \"LEFT\"\n",
        "        if (dx, dy) == (1, 0):  return \"RIGHT\"\n",
        "        return None\n",
        "\n",
        "    def reason(self, percepts: Dict[str, Any], memory: ListMemoryForAgent, values: Dict[str, Any]) -> Dict[str, Any]:\n",
        "        start = percepts[\"agent_pos\"]\n",
        "        goal = percepts[\"goal_pos\"]\n",
        "        width = percepts[\"width\"]\n",
        "        height = percepts[\"height\"]\n",
        "        forbidden = percepts[\"forbidden\"]\n",
        "\n",
        "        direction = self._bfs_next_dir(start, goal, width, height, forbidden)\n",
        "        if direction is None:\n",
        "            # Fallback: greedy nudge that avoids forbidden and stays in bounds\n",
        "            candidates = [\"RIGHT\", \"DOWN\", \"LEFT\", \"UP\"]\n",
        "            # Sort toward decreasing Manhattan distance\n",
        "            def after(pos, d):\n",
        "                dx, dy = 0, 0\n",
        "                if d == \"UP\": dy = -1\n",
        "                if d == \"DOWN\": dy = 1\n",
        "                if d == \"LEFT\": dx = -1\n",
        "                if d == \"RIGHT\": dx = 1\n",
        "                np = (pos[0]+dx, pos[1]+dy)\n",
        "                return np\n",
        "            candidates.sort(key=lambda d: manhattan(after(start, d), goal))\n",
        "            for d in candidates:\n",
        "                np = after(start, d)\n",
        "                if in_bounds(np, width, height) and np not in forbidden:\n",
        "                    direction = d\n",
        "                    break\n",
        "\n",
        "        if direction is None:\n",
        "            return {\"type\": \"NOOP\"}\n",
        "        return {\"type\": \"MOVE\", \"dir\": direction}\n",
        "\n",
        "    def learn(self, feedback: Dict[str, Any]) -> None:\n",
        "        self.last_reward = feedback.get(\"reward\", 0.0)\n",
        "\n",
        "\n",
        "class SimulatedAgent:\n",
        "    def __init__(self, cognitive_core: PathfindingCore, memory: ListMemoryForAgent, values: Dict[str, Any]):\n",
        "        self.core = cognitive_core\n",
        "        self.memory = memory\n",
        "        self.values = values\n",
        "        self.percepts: Optional[Dict[str, Any]] = None\n",
        "\n",
        "    def observe(self, env_state: Dict[str, Any]) -> None:\n",
        "        self.percepts = self.core.encode(env_state)\n",
        "\n",
        "    def act(self) -> Dict[str, Any]:\n",
        "        if self.percepts is None:\n",
        "            raise RuntimeError(\"act() called before observe(). Call observe(env_state) first.\")\n",
        "        return self.core.reason(self.percepts, self.memory, self.values)\n",
        "\n",
        "    def update(self, feedback: Dict[str, Any]) -> None:\n",
        "        self.memory.store(feedback)\n",
        "        self.core.learn(feedback)\n",
        "\n",
        "\n",
        "class WorldSimulator:\n",
        "    def __init__(self, physics_rules: Dict[str, Any], ethical_laws: SimpleEthics, initial_state: Optional[Dict[str, Any]] = None):\n",
        "        self.state = initial_state if initial_state is not None else init_world()\n",
        "        self.rules = physics_rules\n",
        "        self.ethics = ethical_laws\n",
        "\n",
        "    def step(self, actions: Dict[str, Any]) -> Dict[str, Any]:\n",
        "        safe_actions = self.ethics.filter_actions(self.state, actions)\n",
        "        self.state = apply_physics(self.state, safe_actions, self.rules)\n",
        "        return self.state\n",
        "\n",
        "\n",
        "# =========================\n",
        "# Outcome labeling and predictor\n",
        "# =========================\n",
        "\n",
        "def outcome_label(prev_state: Dict[str, Any], new_state: Dict[str, Any]) -> str:\n",
        "    prev_pos = prev_state[\"agent_pos\"]\n",
        "    new_pos = new_state[\"agent_pos\"]\n",
        "    goal = new_state[\"goal_pos\"]\n",
        "\n",
        "    if new_pos == prev_pos:\n",
        "        return \"noop\"\n",
        "    if new_pos == goal:\n",
        "        return \"reached\"\n",
        "    pd = manhattan(prev_pos, goal)\n",
        "    nd = manhattan(new_pos, goal)\n",
        "    if nd < pd:\n",
        "        return \"closer\"\n",
        "    if nd > pd:\n",
        "        return \"farther\"\n",
        "    return \"same\"\n",
        "\n",
        "def input_label(prev_state: Dict[str, Any], action: Dict[str, Any], goal: Tuple[int, int]) -> str:\n",
        "    pos = prev_state[\"agent_pos\"]\n",
        "    d = action.get(\"dir\", \"NONE\") if action.get(\"type\") == \"MOVE\" else \"NONE\"\n",
        "    return f\"pos=({pos[0]},{pos[1]})|dir={d}|goal=({goal[0]},{goal[1]})\"\n",
        "\n",
        "class OutcomePredictor:\n",
        "    \"\"\"\n",
        "    A simple world model that predicts outcomes ignoring ethics (assumes the move executes).\n",
        "    It learns blocked transitions from contradictions and will predict 'noop' for those next time.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        self.blocked: set[Tuple[Tuple[int,int], str]] = set()\n",
        "\n",
        "    def _parse(self, x: str) -> Tuple[Tuple[int,int], str, Tuple[int,int]]:\n",
        "        try:\n",
        "            parts = x.split(\"|\")\n",
        "            pos_s = parts[0].split(\"=\")[1].strip(\"()\")\n",
        "            dir_s = parts[1].split(\"=\")[1]\n",
        "            goal_s = parts[2].split(\"=\")[1].strip(\"()\")\n",
        "            px, py = map(int, pos_s.split(\",\"))\n",
        "            gx, gy = map(int, goal_s.split(\",\"))\n",
        "            return (px, py), dir_s, (gx, gy)\n",
        "        except Exception:\n",
        "            return (0, 0), \"NONE\", (0, 0)\n",
        "\n",
        "    def simulate(self, x: Any) -> str:\n",
        "        pos, direction, goal = self._parse(str(x))\n",
        "        if (pos, direction) in self.blocked:\n",
        "            return \"noop\"\n",
        "        dx, dy = 0, 0\n",
        "        if direction == \"UP\": dy = -1\n",
        "        elif direction == \"DOWN\": dy = 1\n",
        "        elif direction == \"LEFT\": dx = -1\n",
        "        elif direction == \"RIGHT\": dx = 1\n",
        "        new_pos = (pos[0] + dx, pos[1] + dy)\n",
        "        if new_pos == goal:\n",
        "            return \"reached\"\n",
        "        pd = manhattan(pos, goal)\n",
        "        nd = manhattan(new_pos, goal)\n",
        "        if nd < pd:\n",
        "            return \"closer\"\n",
        "        if nd > pd:\n",
        "            return \"farther\"\n",
        "        return \"same\"\n",
        "\n",
        "    def update(self, contradictory: List[Episode]) -> None:\n",
        "        to_add: set[Tuple[Tuple[int,int], str]] = set()\n",
        "        for ep in contradictory:\n",
        "            pos, direction, _ = self._parse(ep.input)\n",
        "            if ep.outcome == \"noop\" and direction != \"NONE\":\n",
        "                to_add.add((pos, direction))\n",
        "        new = to_add - self.blocked\n",
        "        self.blocked |= to_add\n",
        "        if new:\n",
        "            print(f\"[WorldModel] Learned blocked transitions: {sorted(new)}\")\n",
        "\n",
        "\n",
        "class ReflectiveSelfModel:\n",
        "    def __init__(self):\n",
        "        self.learned_count = 0\n",
        "\n",
        "    def adapt(self, contradictory: List[Episode]) -> None:\n",
        "        self.learned_count += len(contradictory)\n",
        "        print(f\"[SelfModel] Adapted on {len(contradictory)} contradictions (total={self.learned_count}).\")\n",
        "\n",
        "\n",
        "# =========================\n",
        "# Similarity function\n",
        "# =========================\n",
        "\n",
        "def basic_similarity(a: str, b: str) -> float:\n",
        "    return 1.0 if a == b else 0.0\n",
        "\n",
        "\n",
        "# =========================\n",
        "# Feedback utility\n",
        "# =========================\n",
        "\n",
        "def make_feedback(prev_state: Dict[str, Any], new_state: Dict[str, Any]) -> Dict[str, Any]:\n",
        "    prev_d = manhattan(prev_state[\"agent_pos\"], prev_state[\"goal_pos\"])\n",
        "    new_d = manhattan(new_state[\"agent_pos\"], new_state[\"goal_pos\"])\n",
        "    reward = (prev_d - new_d)  # +1 closer, -1 farther, 0 same/noop\n",
        "    if new_state[\"agent_pos\"] == new_state[\"goal_pos\"]:\n",
        "        reward += 10.0\n",
        "    return {\"reward\": reward, \"reached_goal\": new_state[\"agent_pos\"] == new_state[\"goal_pos\"]}\n",
        "\n",
        "\n",
        "# =========================\n",
        "# End-to-end main routine\n",
        "# =========================\n",
        "\n",
        "def run_simulation_and_training():\n",
        "    # World and agent\n",
        "    physics_rules = {\"friction\": 0.0}\n",
        "    ethics = SimpleEthics()\n",
        "    world = WorldSimulator(\n",
        "        physics_rules,\n",
        "        ethics,\n",
        "        initial_state=init_world(\n",
        "            width=5, height=5, agent_pos=(0, 0), goal_pos=(3, 3), forbidden=[(1, 1), (2, 2)]\n",
        "        ),\n",
        "    )\n",
        "\n",
        "    agent = SimulatedAgent(PathfindingCore(), ListMemoryForAgent(), values={\"risk\": 0.0})\n",
        "    epo_mem = EpisodicMemory()\n",
        "\n",
        "    print(\"=== Rollout ===\")\n",
        "    max_steps = 20\n",
        "    for t in range(max_steps):\n",
        "        prev = dict(world.state)\n",
        "        agent.observe(prev)\n",
        "        action = agent.act()\n",
        "        new_state = world.step({\"agent\": action})\n",
        "        fb = make_feedback(prev, new_state)\n",
        "        agent.update(fb)\n",
        "\n",
        "        lbl_in = input_label(prev, action, new_state[\"goal_pos\"])\n",
        "        lbl_out = outcome_label(prev, new_state)\n",
        "        epo_mem.store(Episode(lbl_in, lbl_out))\n",
        "\n",
        "        print(\n",
        "            f\"t={t:02d} pos={new_state['agent_pos']} action={action} outcome={lbl_out} \"\n",
        "            f\"reward={fb['reward']} goal={new_state['goal_pos']} reached={fb['reached_goal']}\"\n",
        "        )\n",
        "        if fb[\"reached_goal\"]:\n",
        "            print(\"Goal reached. Stopping rollout.\")\n",
        "            break\n",
        "\n",
        "    # Reflective updater\n",
        "    updater = ReflectiveUpdater(\n",
        "        episodic_memory=epo_mem,\n",
        "        world_model=OutcomePredictor(),\n",
        "        self_model=ReflectiveSelfModel(),\n",
        "        similarity_fn=basic_similarity,\n",
        "        threshold=0.8,\n",
        "    )\n",
        "\n",
        "    print(\"\\n=== Audited contradictions ===\")\n",
        "    contradictions = updater.detect_contradictions_detailed(limit=512, batch_size=128)\n",
        "    # Deduplicate identical lines and count occurrences\n",
        "    counts = Counter((c.episode.input, c.prediction, c.episode.outcome) for c in contradictions)\n",
        "    for (inp, pred, act), cnt in counts.items():\n",
        "        print(f\"[Contradiction x{cnt}] input='{inp}' pred='{pred}' actual='{act}' score=0.0\")\n",
        "\n",
        "    print(\"\\n=== Batched revise ===\")\n",
        "    n = updater.revise_model_batched(batch_size=128)\n",
        "    print(f\"[Revision] Contradictions processed: {n}\")\n",
        "\n",
        "    print(\"\\n=== Curriculum training ===\")\n",
        "    sched = LinearThresholdScheduler(start=0.7, end=0.95, total_steps=3)\n",
        "    for _ in range(3):\n",
        "        report = updater.training_step(max_episodes=512, batch_size=128, scheduler=sched)\n",
        "        print(\n",
        "            f\"[Training] step={report.step} processed={report.processed} \"\n",
        "            f\"contradictory={report.contradictory} threshold={report.threshold:.2f} \"\n",
        "            f\"avg_all={report.avg_score_all:.3f} avg_contra={report.avg_score_contradictions:.3f}\"\n",
        "        )\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    run_simulation_and_training()"
      ],
      "metadata": {
        "id": "aTRIVLeRNTQ2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}