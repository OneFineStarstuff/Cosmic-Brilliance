{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyNVL1vi9QijNwfJSEEPHOf2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OneFineStarstuff/Cosmic-Brilliance/blob/main/casimir_train_log_fix_py.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hhqM7oFsij1m"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.colors import LogNorm\n",
        "\n",
        "# 1. True Casimir force (always negative)\n",
        "def casimir_force(d, ε):\n",
        "    return - (np.pi**2) / (240.0 * d**4) * ε\n",
        "\n",
        "# 2. Build synthetic dataset\n",
        "n_dist, n_perm = 100, 100\n",
        "distances = np.linspace(1e-8, 1e-6, n_dist)      # 10 nm → 1 µm\n",
        "permittivities = np.linspace(1.0, 10.0, n_perm)  # εr = 1 → 10\n",
        "D, E = np.meshgrid(distances, permittivities)\n",
        "X = np.stack([D.ravel(), E.ravel()], axis=1)\n",
        "y = casimir_force(X[:,0], X[:,1]).reshape(-1,1)\n",
        "\n",
        "# 3. Log-transform and normalize inputs\n",
        "log_d = np.log(X[:,0])\n",
        "log_d_norm = (log_d - log_d.mean()) / log_d.std()\n",
        "ε_norm     = (X[:,1] - X[:,1].mean()) / X[:,1].std()\n",
        "X_tensor   = torch.tensor(np.stack([log_d_norm, ε_norm], axis=1),\n",
        "                          dtype=torch.float32)\n",
        "\n",
        "# 4. Log-transform target & normalize\n",
        "y_abs      = -y                                    # absolute force\n",
        "log_y      = np.log(y_abs)                        # log|F|\n",
        "mean_ly, std_ly = log_y.mean(), log_y.std()\n",
        "y_norm     = (log_y - mean_ly) / std_ly\n",
        "y_tensor   = torch.tensor(y_norm, dtype=torch.float32)\n",
        "\n",
        "# 5. DataLoader\n",
        "dataset = TensorDataset(X_tensor, y_tensor)\n",
        "loader  = DataLoader(dataset, batch_size=256, shuffle=True)\n",
        "\n",
        "# 6. MLP definition\n",
        "class NegCasimirMLP(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dims, output_dim):\n",
        "        super().__init__()\n",
        "        layers = []\n",
        "        last_dim = input_dim\n",
        "        for h in hidden_dims:\n",
        "            layers += [\n",
        "                nn.Linear(last_dim, h),\n",
        "                nn.LayerNorm(h),\n",
        "                nn.ReLU(inplace=True)\n",
        "            ]\n",
        "            last_dim = h\n",
        "        layers.append(nn.Linear(last_dim, output_dim))\n",
        "        self.net = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "model     = NegCasimirMLP(input_dim=2, hidden_dims=[64,64,32], output_dim=1)\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-6)\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "# 7. Training loop\n",
        "n_epochs = 200\n",
        "for epoch in range(1, n_epochs+1):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for xb, yb in loader:\n",
        "        optimizer.zero_grad()\n",
        "        y_pred = model(xb)\n",
        "        loss   = criterion(y_pred, yb)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item() * xb.size(0)\n",
        "    running_loss /= len(dataset)\n",
        "    if epoch % 20 == 0:\n",
        "        print(f\"Epoch {epoch:03d}  Normalized-Log MSE: {running_loss:.4f}\")\n",
        "\n",
        "# 8. Inference & back-transform to force\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    y_pred_norm = model(X_tensor).cpu().numpy().flatten()\n",
        "    log_pred     = y_pred_norm * std_ly + mean_ly\n",
        "    y_pred_force = -np.exp(log_pred)  # restore negative sign\n",
        "\n",
        "# 9. Scatter plot (log-log of absolute values)\n",
        "plt.figure(figsize=(6,6))\n",
        "plt.scatter(\n",
        "    np.abs(y),\n",
        "    np.abs(y_pred_force),\n",
        "    s=5, alpha=0.3\n",
        ")\n",
        "plt.plot(\n",
        "    [y.min()*-1, y.max()*-1],\n",
        "    [y.min()*-1, y.max()*-1],\n",
        "    'r--'\n",
        ")\n",
        "plt.xscale('log')\n",
        "plt.yscale('log')\n",
        "plt.xlabel(\"True |Casimir Force|\")\n",
        "plt.ylabel(\"Predicted |Casimir Force|\")\n",
        "plt.title(\"Log-Log Fit of Absolute Forces\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# 10. Surface heatmaps (absolute force with LogNorm)\n",
        "Z_true_abs = (-y).reshape(n_perm, n_dist)\n",
        "Z_pred_abs = (-y_pred_force).reshape(n_perm, n_dist)\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12,4))\n",
        "im1 = ax1.pcolormesh(\n",
        "    distances*1e6, permittivities,\n",
        "    Z_true_abs, norm=LogNorm(), shading='auto'\n",
        ")\n",
        "ax1.set_title(\"True |Force| Surface\")\n",
        "ax1.set_xlabel(\"Distance (µm)\")\n",
        "ax1.set_ylabel(\"Permittivity\")\n",
        "fig.colorbar(im1, ax=ax1)\n",
        "\n",
        "im2 = ax2.pcolormesh(\n",
        "    distances*1e6, permittivities,\n",
        "    Z_pred_abs, norm=LogNorm(), shading='auto'\n",
        ")\n",
        "ax2.set_title(\"Predicted |Force| Surface\")\n",
        "ax2.set_xlabel(\"Distance (µm)\")\n",
        "ax2.set_ylabel(\"Permittivity\")\n",
        "fig.colorbar(im2, ax=ax2)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    }
  ]
}