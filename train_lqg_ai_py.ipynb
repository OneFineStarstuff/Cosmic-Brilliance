{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyPzJ7G3GL0pcEP2/l1WVr5v",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OneFineStarstuff/Cosmic-Brilliance/blob/main/train_lqg_ai_py.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MuOiKYd6p7fG"
      },
      "outputs": [],
      "source": [
        "pip install torch numpy matplotlib"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "train_lqg_ai.py\n",
        "\n",
        "Physics-informed LQGAI pipeline:\n",
        "\n",
        " 1. Synthetic SpinNetworkDataset sampling 5 spin parameters\n",
        " 2. MLP with LayerNorm & Dropout\n",
        " 3. PINN loss: MSE + residual of toy Hamiltonian constraint\n",
        " 4. MC-Dropout inference for uncertainty\n",
        " 5. Training loop with AdamW, ReduceLROnPlateau, early stopping\n",
        " 6. Visualization of losses, true vs predicted scatter, and uncertainty heatmap\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 1. Synthetic Dataset\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def toy_hamiltonian_constraint(spins, evo):\n",
        "    \"\"\"\n",
        "    Toy Hamiltonian constraint residual:\n",
        "      H |ψ> ≈ sum_{i=0}^{output_dim-1} (spin_i^2) * evo_i\n",
        "    (we only use as many spins as evo has dimensions)\n",
        "    \"\"\"\n",
        "    # only take the first evo.size(1) spins\n",
        "    spin_sub = spins[:, :evo.size(1)]\n",
        "    return (spin_sub**2 * evo).sum(dim=1)\n",
        "\n",
        "class SpinNetworkDataset(Dataset):\n",
        "    def __init__(self, n_samples=5000, seed=42):\n",
        "        torch.manual_seed(seed)\n",
        "        # Each sample: 5 spin parameters in [0.5, 2.5]\n",
        "        spins = 0.5 + 2.0 * torch.rand(n_samples, 5)\n",
        "\n",
        "        # Build a toy \"true\" evolution via a random projection + noise\n",
        "        proj = nn.Linear(5, 3)\n",
        "        with torch.no_grad():\n",
        "            true_evo = proj(spins)\n",
        "\n",
        "        Y_raw = true_evo + 0.05 * torch.randn_like(true_evo)\n",
        "\n",
        "        self.X = spins\n",
        "        self.Y = Y_raw\n",
        "\n",
        "        # compute stats for normalization\n",
        "        self.stats = {\n",
        "            'X_mean': self.X.mean(0),\n",
        "            'X_std':  self.X.std(0),\n",
        "            'Y_mean': self.Y.mean(0),\n",
        "            'Y_std':  self.Y.std(0),\n",
        "        }\n",
        "\n",
        "        # normalize\n",
        "        self.X = (self.X - self.stats['X_mean']) / self.stats['X_std']\n",
        "        self.Y = (self.Y - self.stats['Y_mean']) / self.stats['Y_std']\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx], self.Y[idx]\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 2. Model Definition\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "class LQGAI(nn.Module):\n",
        "    def __init__(self, input_dim=5, hidden_dims=(64,64), output_dim=3, p_drop=0.1):\n",
        "        super().__init__()\n",
        "        layers, dim = [], input_dim\n",
        "        for h in hidden_dims:\n",
        "            layers += [\n",
        "                nn.Linear(dim, h),\n",
        "                nn.LayerNorm(h),\n",
        "                nn.ReLU(),\n",
        "                nn.Dropout(p_drop)\n",
        "            ]\n",
        "            dim = h\n",
        "        layers.append(nn.Linear(dim, output_dim))\n",
        "        self.net = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 3. Physics-Informed Loss\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def physics_residual(pred, inp, stats_torch):\n",
        "    \"\"\"\n",
        "    Enforce toy Hamiltonian constraint:\n",
        "      H_residual(spins, evo_pred) ≈ 0\n",
        "    \"\"\"\n",
        "    # denormalize\n",
        "    spins_den = inp * stats_torch['X_std'] + stats_torch['X_mean']\n",
        "    evo_den   = pred * stats_torch['Y_std'] + stats_torch['Y_mean']\n",
        "\n",
        "    resid = toy_hamiltonian_constraint(spins_den, evo_den)\n",
        "    return torch.mean(resid**2)\n",
        "\n",
        "def total_loss(pred, true, inp, stats_torch, lambda_phys=1.0):\n",
        "    mse  = nn.MSELoss()(pred, true)\n",
        "    phys = physics_residual(pred, inp, stats_torch)\n",
        "    return mse + lambda_phys * phys, mse, phys\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 4. MC-Dropout Inference\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def mc_dropout_predict(model, x, n_samples=50):\n",
        "    model.train()  # keep dropout active\n",
        "    preds = []\n",
        "    with torch.no_grad():\n",
        "        for _ in range(n_samples):\n",
        "            preds.append(model(x).cpu().numpy())\n",
        "    arr = np.stack(preds, axis=0)\n",
        "    return arr.mean(axis=0), arr.std(axis=0)\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 5. Training Loop\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def train_model(model, train_loader, val_loader, stats_torch, device,\n",
        "                lr=1e-3, weight_decay=1e-5, lambda_phys=1.0,\n",
        "                max_epochs=200, patience=20):\n",
        "    model.to(device)\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
        "        optimizer, mode='min', factor=0.5, patience=5\n",
        "    )\n",
        "\n",
        "    best_val, wait = float('inf'), 0\n",
        "    history = {'train_loss': [], 'val_loss': []}\n",
        "\n",
        "    for epoch in range(1, max_epochs+1):\n",
        "        # training\n",
        "        model.train()\n",
        "        train_accum = 0.0\n",
        "        for xb, yb in train_loader:\n",
        "            xb, yb = xb.to(device), yb.to(device)\n",
        "            pred = model(xb)\n",
        "            loss, _, _ = total_loss(pred, yb, xb, stats_torch, lambda_phys)\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            train_accum += loss.item() * xb.size(0)\n",
        "        train_loss = train_accum / len(train_loader.dataset)\n",
        "\n",
        "        # validation\n",
        "        model.eval()\n",
        "        val_accum = 0.0\n",
        "        with torch.no_grad():\n",
        "            for xb, yb in val_loader:\n",
        "                xb, yb = xb.to(device), yb.to(device)\n",
        "                pred = model(xb)\n",
        "                loss, _, _ = total_loss(pred, yb, xb, stats_torch, lambda_phys)\n",
        "                val_accum += loss.item() * xb.size(0)\n",
        "        val_loss = val_accum / len(val_loader.dataset)\n",
        "\n",
        "        scheduler.step(val_loss)\n",
        "        history['train_loss'].append(train_loss)\n",
        "        history['val_loss'].append(val_loss)\n",
        "        print(f\"Epoch {epoch:03d} | Train {train_loss:.4e} | Val {val_loss:.4e}\")\n",
        "\n",
        "        if val_loss < best_val - 1e-6:\n",
        "            best_val, wait = val_loss, 0\n",
        "            torch.save(model.state_dict(), \"best_lqg_ai.pth\")\n",
        "        else:\n",
        "            wait += 1\n",
        "            if wait >= patience:\n",
        "                print(f\"Early stopping at epoch {epoch}\")\n",
        "                break\n",
        "\n",
        "    model.load_state_dict(torch.load(\"best_lqg_ai.pth\"))\n",
        "    return history\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 6. Visualization Helpers\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def plot_history(hist):\n",
        "    plt.plot(hist['train_loss'], label='Train Loss')\n",
        "    plt.plot(hist['val_loss'],   label='Val Loss')\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.legend()\n",
        "    plt.title(\"Training Curve\")\n",
        "    plt.show()\n",
        "\n",
        "def plot_scatter(true_vals, pred_vals, name):\n",
        "    plt.scatter(true_vals, pred_vals, s=5, alpha=0.5)\n",
        "    m, M = true_vals.min(), true_vals.max()\n",
        "    plt.plot([m, M], [m, M], 'r--')\n",
        "    plt.xlabel(f\"True {name}\")\n",
        "    plt.ylabel(f\"Pred {name}\")\n",
        "    plt.title(f\"{name}: True vs Pred\")\n",
        "    plt.show()\n",
        "\n",
        "def plot_uncertainty_heatmap(model, stats_np, device, dim_indices=(0,1)):\n",
        "    idx0, idx1 = dim_indices\n",
        "    vals0 = np.linspace(-2, 2, 100)\n",
        "    vals1 = np.linspace(-2, 2, 100)\n",
        "\n",
        "    G0, G1 = np.meshgrid(vals0, vals1)\n",
        "    # fix other dims at zero\n",
        "    grid = np.zeros((G0.size, 5))\n",
        "    grid[:, idx0] = G0.ravel()\n",
        "    grid[:, idx1] = G1.ravel()\n",
        "\n",
        "    Xnp    = (grid - stats_np['X_mean']) / stats_np['X_std']\n",
        "    Xtorch = torch.from_numpy(Xnp).float().to(device)\n",
        "\n",
        "    _, std = mc_dropout_predict(model, Xtorch, n_samples=50)\n",
        "    heat   = std[:,0].reshape(G0.shape)\n",
        "\n",
        "    plt.pcolormesh(vals0, vals1, heat, shading='auto', cmap='magma')\n",
        "    plt.colorbar(label=\"Std of evo_0\")\n",
        "    plt.xlabel(f\"Norm Spin {idx0}\")\n",
        "    plt.ylabel(f\"Norm Spin {idx1}\")\n",
        "    plt.title(\"Uncertainty Heatmap (evo_0)\")\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 7. Main Execution\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # Prepare data\n",
        "    dataset = SpinNetworkDataset(n_samples=5000)\n",
        "    stats_torch = {k: v.to(device)         for k, v in dataset.stats.items()}\n",
        "    stats_np    = {k: v.cpu().numpy()      for k, v in dataset.stats.items()}\n",
        "\n",
        "    # Split dataset\n",
        "    n_val   = int(0.2 * len(dataset))\n",
        "    n_trn   = len(dataset) - n_val\n",
        "    trn_ds, val_ds = random_split(dataset, [n_trn, n_val])\n",
        "    trn_loader     = DataLoader(trn_ds, batch_size=64, shuffle=True)\n",
        "    val_loader     = DataLoader(val_ds,   batch_size=128)\n",
        "\n",
        "    # Build, train, and visualize\n",
        "    model   = LQGAI().to(device)\n",
        "    history = train_model(\n",
        "        model, trn_loader, val_loader,\n",
        "        stats_torch, device,\n",
        "        lr=1e-3, weight_decay=1e-5,\n",
        "        lambda_phys=1.0, max_epochs=200, patience=20\n",
        "    )\n",
        "\n",
        "    plot_history(history)\n",
        "\n",
        "    # True vs Pred for evo_0\n",
        "    Xall = dataset.X.to(device)\n",
        "    with torch.no_grad():\n",
        "        Ypred_norm = model(Xall).cpu().numpy()\n",
        "    Ytrue_norm   = dataset.Y.numpy()\n",
        "    evo0_pred = Ypred_norm[:, 0] * stats_np['Y_std'][0] + stats_np['Y_mean'][0]\n",
        "    evo0_true = Ytrue_norm[:, 0]   * stats_np['Y_std'][0] + stats_np['Y_mean'][0]\n",
        "    plot_scatter(evo0_true, evo0_pred, \"Evolution Factor 0\")\n",
        "\n",
        "    # Uncertainty heatmap over spin dims 0 & 1\n",
        "    plot_uncertainty_heatmap(model, stats_np, device, dim_indices=(0,1))"
      ],
      "metadata": {
        "id": "XhKQqLWyrFT8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}