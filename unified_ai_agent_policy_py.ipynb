{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyNbNmi27Ryah0mAbX6sWDD7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OneFineStarstuff/Cosmic-Brilliance/blob/main/unified_ai_agent_policy_py.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JHoJIZAGcUD-"
      },
      "outputs": [],
      "source": [
        "# unified_ai/agent/policy.py\n",
        "from __future__ import annotations\n",
        "from dataclasses import dataclass\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "@dataclass\n",
        "class PolicyConfig:\n",
        "    obs_dim: int\n",
        "    hidden_dim: int\n",
        "    actions: int\n",
        "    lr: float\n",
        "    gamma: float\n",
        "    epsilon: float\n",
        "    epsilon_min: float\n",
        "    epsilon_decay: float\n",
        "\n",
        "class QNetwork(nn.Module):\n",
        "    def __init__(self, obs_dim: int, hidden: int, actions: int):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(obs_dim, hidden), nn.ReLU(),\n",
        "            nn.Linear(hidden, hidden), nn.ReLU(),\n",
        "            nn.Linear(hidden, actions)\n",
        "        )\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        return self.net(x)\n",
        "\n",
        "class AgentPolicy:\n",
        "    def __init__(self, cfg: PolicyConfig, device: torch.device):\n",
        "        self.cfg = cfg\n",
        "        self.device = device\n",
        "        self.q = QNetwork(cfg.obs_dim, cfg.hidden_dim, cfg.actions).to(device)\n",
        "        self.opt = torch.optim.Adam(self.q.parameters(), lr=cfg.lr)\n",
        "        self.epsilon = cfg.epsilon\n",
        "\n",
        "    def select_action(self, obs: torch.Tensor) -> int:\n",
        "        if torch.rand(()) < self.epsilon:\n",
        "            return int(torch.randint(0, 3, (1,)).item() - 1)  # map {0,1,2} -> {-1,0,1}\n",
        "        with torch.no_grad():\n",
        "            q = self.q(obs.unsqueeze(0))\n",
        "            a_idx = int(q.argmax(dim=-1).item())\n",
        "            return a_idx - 1\n",
        "\n",
        "    def update(self, s: torch.Tensor, a: int, r: float, s2: torch.Tensor, done: bool):\n",
        "        a_idx = a + 1  # map {-1,0,1} -> {0,1,2}\n",
        "        q = self.q(s.unsqueeze(0))\n",
        "        q_sa = q[0, a_idx]\n",
        "        with torch.no_grad():\n",
        "            target = torch.tensor(r, device=self.device)\n",
        "            if not done:\n",
        "                q2 = self.q(s2.unsqueeze(0)).max(dim=-1).values[0]\n",
        "                target = target + self.cfg.gamma * q2\n",
        "        loss = F.smooth_l1_loss(q_sa, target)\n",
        "        self.opt.zero_grad()\n",
        "        loss.backward()\n",
        "        self.opt.step()\n",
        "        return float(loss.item())\n",
        "\n",
        "    def decay_epsilon(self):\n",
        "        self.epsilon = max(self.cfg.epsilon_min, self.epsilon * self.cfg.epsilon_decay)\n",
        "\n",
        "    def boost_epsilon(self, amount: float):\n",
        "        self.epsilon = min(0.9, self.epsilon + amount)"
      ]
    }
  ]
}