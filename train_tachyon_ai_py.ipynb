{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyPzYYLCmiXFt2dvY8LfoDsP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OneFineStarstuff/Cosmic-Brilliance/blob/main/train_tachyon_ai_py.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZnXJt21XpsMq"
      },
      "outputs": [],
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "train_tachyon_ai.py\n",
        "\n",
        "End-to-end training script for TachyonAI:\n",
        "  - Defines an MLP (TachyonAI) for tachyonic field evolution\n",
        "  - Generates synthetic (energy, momentum, field strength) → (next_amplitude, next_rate)\n",
        "  - Trains with MSE, LR scheduler, early stopping\n",
        "  - Evaluates on a held-out validation set\n",
        "  - Plots true vs. predicted field evolution\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 1. Model Definition\n",
        "# ------------------------------------------------------------------------------\n",
        "class TachyonAI(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "        super(TachyonAI, self).__init__()\n",
        "        self.fc1  = nn.Linear(input_dim, hidden_dim)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc2  = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.fc1(x))\n",
        "        return self.fc2(x)\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 2. Synthetic Data Generator\n",
        "#    Toy dynamical rule for tachyonic field evolution:\n",
        "#      Given (E, p, f) at time t, predict (f_{t+Δ}, df/dt_{t+Δ})\n",
        "# ------------------------------------------------------------------------------\n",
        "def generate_synthetic_tachyon_data(n_samples=12000, seed=42):\n",
        "    np.random.seed(seed)\n",
        "    # Sample energy E, momentum p, field strength f in [-1,1]\n",
        "    E = np.random.uniform(-1, 1, size=(n_samples, 1)).astype(np.float32)\n",
        "    p = np.random.uniform(-1, 1, size=(n_samples, 1)).astype(np.float32)\n",
        "    f = np.random.uniform(-1, 1, size=(n_samples, 1)).astype(np.float32)\n",
        "\n",
        "    X = np.hstack([E, p, f])  # shape (n_samples, 3)\n",
        "\n",
        "    # Toy evolution laws:\n",
        "    #   f_next = f + Δt * (p - E * f)\n",
        "    #   rate   = p * f - E\n",
        "    Δt = 0.1\n",
        "    f_next = f + Δt * (p - E * f)\n",
        "    rate   = p * f - E\n",
        "\n",
        "    Y = np.hstack([f_next, rate]).astype(np.float32)  # shape (n_samples, 2)\n",
        "    return X, Y\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 3. Prepare Dataset and DataLoaders\n",
        "# ------------------------------------------------------------------------------\n",
        "# Generate and wrap data\n",
        "X, Y = generate_synthetic_tachyon_data(n_samples=12000)\n",
        "X_tensor = torch.from_numpy(X)\n",
        "Y_tensor = torch.from_numpy(Y)\n",
        "\n",
        "dataset = TensorDataset(X_tensor, Y_tensor)\n",
        "train_size = int(0.8 * len(dataset))\n",
        "val_size   = len(dataset) - train_size\n",
        "\n",
        "train_ds, val_ds = random_split(dataset, [train_size, val_size])\n",
        "train_loader    = DataLoader(train_ds, batch_size=128, shuffle=True)\n",
        "val_loader      = DataLoader(val_ds,   batch_size=256)\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 4. Instantiate Model, Loss, Optimizer, Scheduler\n",
        "# ------------------------------------------------------------------------------\n",
        "device   = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model    = TachyonAI(input_dim=3, hidden_dim=32, output_dim=2).to(device)\n",
        "criterion= nn.MSELoss()\n",
        "optimizer= optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-5)\n",
        "scheduler= optim.lr_scheduler.ReduceLROnPlateau(\n",
        "    optimizer, mode='min', factor=0.5, patience=5\n",
        ")\n",
        "\n",
        "# Early stopping parameters\n",
        "best_val_loss = float('inf')\n",
        "patience      = 0\n",
        "max_patience  = 10\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 5. Training Loop\n",
        "# ------------------------------------------------------------------------------\n",
        "train_losses, val_losses = [], []\n",
        "\n",
        "for epoch in range(1, 201):\n",
        "    # --- Training phase ---\n",
        "    model.train()\n",
        "    running_train_loss = 0.0\n",
        "    for xb, yb in train_loader:\n",
        "        xb, yb = xb.to(device), yb.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        preds = model(xb)\n",
        "        loss  = criterion(preds, yb)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_train_loss += loss.item() * xb.size(0)\n",
        "\n",
        "    train_loss = running_train_loss / len(train_loader.dataset)\n",
        "    train_losses.append(train_loss)\n",
        "\n",
        "    # --- Validation phase ---\n",
        "    model.eval()\n",
        "    running_val_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for xb, yb in val_loader:\n",
        "            xb, yb = xb.to(device), yb.to(device)\n",
        "            preds  = model(xb)\n",
        "            running_val_loss += criterion(preds, yb).item() * xb.size(0)\n",
        "\n",
        "    val_loss = running_val_loss / len(val_loader.dataset)\n",
        "    val_losses.append(val_loss)\n",
        "\n",
        "    scheduler.step(val_loss)\n",
        "\n",
        "    # Early stopping check\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        patience = 0\n",
        "        torch.save(model.state_dict(), \"best_tachyon_ai.pt\")\n",
        "    else:\n",
        "        patience += 1\n",
        "        if patience >= max_patience:\n",
        "            print(f\"Early stopping at epoch {epoch}\")\n",
        "            break\n",
        "\n",
        "    if epoch % 10 == 0 or epoch == 1:\n",
        "        print(f\"Epoch {epoch:03d} | Train MSE: {train_loss:.4f} | Val MSE: {val_loss:.4f}\")\n",
        "\n",
        "print(f\"\\nTraining complete. Best Val MSE: {best_val_loss:.4f}\")\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 6. Load Best Model & Evaluate on Validation Set\n",
        "# ------------------------------------------------------------------------------\n",
        "model.load_state_dict(torch.load(\"best_tachyon_ai.pt\"))\n",
        "model.eval()\n",
        "\n",
        "# Collect predictions and true values\n",
        "all_preds, all_true = [], []\n",
        "with torch.no_grad():\n",
        "    for xb, yb in val_loader:\n",
        "        xb = xb.to(device)\n",
        "        pred = model(xb).cpu().numpy()\n",
        "        all_preds.append(pred)\n",
        "        all_true.append(yb.numpy())\n",
        "\n",
        "all_preds = np.vstack(all_preds)\n",
        "all_true  = np.vstack(all_true)\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 7. Plot True vs. Predicted Evolution\n",
        "# ------------------------------------------------------------------------------\n",
        "plt.figure(figsize=(12,5))\n",
        "\n",
        "for i, label in enumerate([\"f_next\", \"rate\"]):\n",
        "    plt.subplot(1, 2, i+1)\n",
        "    plt.scatter(all_true[:, i], all_preds[:, i], s=5, alpha=0.3)\n",
        "    m = np.min([all_true[:, i].min(), all_preds[:, i].min()])\n",
        "    M = np.max([all_true[:, i].max(), all_preds[:, i].max()])\n",
        "    plt.plot([m, M], [m, M], 'r--')\n",
        "    plt.xlabel(f\"True {label}\")\n",
        "    plt.ylabel(f\"Predicted {label}\")\n",
        "    plt.title(f\"{label}: True vs. Predicted\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 8. Loss Curve Plot\n",
        "# ------------------------------------------------------------------------------\n",
        "plt.figure()\n",
        "plt.plot(train_losses, label=\"Train Loss\")\n",
        "plt.plot(val_losses,   label=\"Val Loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"MSE Loss\")\n",
        "plt.title(\"Training & Validation Loss\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    }
  ]
}