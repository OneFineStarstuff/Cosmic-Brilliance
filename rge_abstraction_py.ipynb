{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyOMHU6Vfqnnu3pmQOqkCkWp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OneFineStarstuff/Cosmic-Brilliance/blob/main/rge_abstraction_py.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install causal-learn"
      ],
      "metadata": {
        "id": "XYhy1PgfLIyM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wv_qGNHRKfNG"
      },
      "outputs": [],
      "source": [
        "# rge/abstraction.py\n",
        "from __future__ import annotations\n",
        "\n",
        "import warnings\n",
        "from dataclasses import dataclass\n",
        "from typing import List, Optional, Any\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from sklearn.cluster import SpectralClustering\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "try:\n",
        "    import networkx as nx  # optional\n",
        "    _HAS_NX = True\n",
        "except Exception:\n",
        "    _HAS_NX = False\n",
        "\n",
        "# causal-learn: pip install causallearn\n",
        "from causallearn.search.ConstraintBased.PC import pc\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class ExtractConfig:\n",
        "    \"\"\"Configuration for invariant extraction.\"\"\"\n",
        "    n_clusters: Optional[int] = None               # if None, auto-select\n",
        "    max_clusters: int = 8\n",
        "    min_cluster_size: int = 20\n",
        "    standardize: bool = True\n",
        "    affinity: str = \"nearest_neighbors\"            # more robust in high-d than rbf\n",
        "    n_neighbors: Optional[int] = None              # auto if None\n",
        "    random_state: Optional[int] = 42\n",
        "    pca_dim: Optional[int] = 16                    # reduce latent dim before PC; None to disable\n",
        "    pc_alpha: float = 0.05\n",
        "    pc_indep_test: str = \"fisherz\"                 # common choice for continuous data\n",
        "    to_networkx: bool = False                      # convert cg.G to networkx if available\n",
        "    verbose: bool = False\n",
        "\n",
        "\n",
        "class LatentInvariantExtractor(nn.Module):\n",
        "    \"\"\"\n",
        "    Autoencoder → latent → spectral clusters → causal graph:\n",
        "        raw → z → {clusters} → PC algorithm → invariant subgraphs\n",
        "    \"\"\"\n",
        "    def __init__(self, in_dim: int = 6, latent_dim: int = 128):\n",
        "        super().__init__()\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Linear(in_dim, 256), nn.ReLU(),\n",
        "            nn.Linear(256, latent_dim)\n",
        "        )\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(latent_dim, 256), nn.ReLU(),\n",
        "            nn.Linear(256, in_dim)\n",
        "        )\n",
        "        self.latent_dim = latent_dim\n",
        "\n",
        "    def forward(self, x: torch.Tensor):\n",
        "        z = self.encoder(x)\n",
        "        x_hat = self.decoder(z)\n",
        "        return z, x_hat\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def extract(self, data: torch.Tensor, cfg: Optional[ExtractConfig] = None) -> List[Any]:\n",
        "        \"\"\"\n",
        "        Returns a list of candidate invariant structures as causal subgraphs.\n",
        "\n",
        "        - data: tensor of shape (n_samples, in_dim)\n",
        "        - cfg: ExtractConfig with clustering and PC options\n",
        "\n",
        "        Each returned item is either:\n",
        "          - causal-learn Graph (cg.G), or\n",
        "          - networkx.DiGraph if cfg.to_networkx=True and networkx is installed\n",
        "        \"\"\"\n",
        "        if cfg is None:\n",
        "            cfg = ExtractConfig()\n",
        "\n",
        "        # Fail-fast validations\n",
        "        if data.dim() != 2:\n",
        "            raise ValueError(f\"data must be 2D (n_samples, in_dim); got shape {tuple(data.shape)}\")\n",
        "        n_samples, _ = data.shape\n",
        "        if n_samples < 4:\n",
        "            raise ValueError(f\"Need at least 4 samples; got {n_samples}\")\n",
        "\n",
        "        self.eval()\n",
        "\n",
        "        # 1) Encode to latent\n",
        "        z, _ = self.forward(data)\n",
        "        z_np = z.detach().cpu().numpy()\n",
        "\n",
        "        # 2) Optional standardization (recommended)\n",
        "        if cfg.standardize:\n",
        "            z_np = StandardScaler().fit_transform(z_np)\n",
        "\n",
        "        # 3) Decide number of clusters\n",
        "        if cfg.n_clusters is None:\n",
        "            # Heuristic: about one cluster per ~100 samples, clipped\n",
        "            auto_k = max(2, min(cfg.max_clusters, n_samples // 100 or 2))\n",
        "        else:\n",
        "            auto_k = int(cfg.n_clusters)\n",
        "        if auto_k > n_samples:\n",
        "            warnings.warn(f\"Reducing n_clusters from {auto_k} to {n_samples} (n_samples limit).\")\n",
        "            auto_k = n_samples\n",
        "        if auto_k < 2:\n",
        "            auto_k = 2  # SpectralClustering requires at least 2\n",
        "\n",
        "        # 4) Spectral clustering\n",
        "        # Choose neighbors if using nearest_neighbors\n",
        "        n_neighbors = cfg.n_neighbors\n",
        "        if cfg.affinity == \"nearest_neighbors\":\n",
        "            # A small, safe default; ensure < n_samples\n",
        "            if n_neighbors is None:\n",
        "                n_neighbors = max(5, min(10, n_samples - 1))\n",
        "            if n_neighbors >= n_samples:\n",
        "                n_neighbors = n_samples - 1\n",
        "\n",
        "        sc = SpectralClustering(\n",
        "            n_clusters=auto_k,\n",
        "            affinity=cfg.affinity,\n",
        "            n_neighbors=n_neighbors if cfg.affinity == \"nearest_neighbors\" else None,\n",
        "            random_state=cfg.random_state,\n",
        "            assign_labels=\"kmeans\"\n",
        "        )\n",
        "        clusters = sc.fit_predict(z_np)\n",
        "\n",
        "        # 5) Optional dimensionality reduction before PC (keeps PC tractable)\n",
        "        X_for_pc = z_np\n",
        "        if cfg.pca_dim is not None and cfg.pca_dim < X_for_pc.shape[1]:\n",
        "            pca = PCA(n_components=cfg.pca_dim, random_state=cfg.random_state)\n",
        "            X_for_pc = pca.fit_transform(X_for_pc)\n",
        "\n",
        "        # 6) Causal discovery inside each cluster\n",
        "        invariants: List[Any] = []\n",
        "        unique_clusters = np.unique(clusters)\n",
        "        for c in unique_clusters:\n",
        "            idx = clusters == c\n",
        "            size = int(idx.sum())\n",
        "            if size < cfg.min_cluster_size:\n",
        "                if cfg.verbose:\n",
        "                    print(f\"[SKIP] cluster {c}: size {size} < min_cluster_size {cfg.min_cluster_size}\")\n",
        "                continue\n",
        "\n",
        "            Xc = X_for_pc[idx, :]\n",
        "            # causal-learn expects shape (n_samples, n_features)\n",
        "            cg = pc(\n",
        "                Xc,\n",
        "                alpha=cfg.pc_alpha,\n",
        "                indep_test=cfg.pc_indep_test,\n",
        "                verbose=cfg.verbose\n",
        "            )\n",
        "            G = cg.G\n",
        "            if cfg.to_networkx:\n",
        "                if not _HAS_NX:\n",
        "                    warnings.warn(\"networkx not installed; returning causal-learn Graph instead.\")\n",
        "                    invariants.append(G)\n",
        "                else:\n",
        "                    # causal-learn Graph has utility to convert; fallback to manual conversion if needed\n",
        "                    try:\n",
        "                        G_nx = G.to_nx_graph()  # available in recent versions\n",
        "                    except Exception:\n",
        "                        # Manual conversion as directed graph\n",
        "                        G_nx = nx.DiGraph()\n",
        "                        nodes = list(range(G.node_num))\n",
        "                        G_nx.add_nodes_from(nodes)\n",
        "                        for i in nodes:\n",
        "                            for j in nodes:\n",
        "                                if i != j and G.is_directed(i, j):\n",
        "                                    G_nx.add_edge(i, j)\n",
        "                    invariants.append(G_nx)\n",
        "            else:\n",
        "                invariants.append(G)\n",
        "\n",
        "        if cfg.verbose:\n",
        "            kept = len(invariants)\n",
        "            total = len(unique_clusters)\n",
        "            print(f\"[DONE] clusters processed: {total}, invariants kept: {kept}\")\n",
        "\n",
        "        return invariants"
      ]
    }
  ]
}