{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyOu2ALdhOoTn/kLSh5kYHxG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OneFineStarstuff/Cosmic-Brilliance/blob/main/casimir_train_py.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uvKWehy9d3Vr"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 1. True Casimir force (negative sign included)\n",
        "def casimir_force(d, ε):\n",
        "    return - (np.pi**2) / (240.0 * d**4) * ε\n",
        "\n",
        "# 2. Create synthetic dataset\n",
        "#    distances: 10 nm → 1 µm, permittivities: 1 → 10\n",
        "n_dist, n_perm = 100, 100\n",
        "distances = np.linspace(1e-8, 1e-6, n_dist)\n",
        "permittivities = np.linspace(1.0, 10.0, n_perm)\n",
        "D, E = np.meshgrid(distances, permittivities)\n",
        "X = np.stack([D.ravel(), E.ravel()], axis=1)\n",
        "y = casimir_force(X[:,0], X[:,1]).reshape(-1,1)\n",
        "\n",
        "# 3. Convert to tensors and normalize\n",
        "#    log-distance helps the network learn the 1/d^4 law\n",
        "log_d = np.log(X[:,0])\n",
        "log_d_norm = (log_d - log_d.mean()) / log_d.std()\n",
        "ε_norm     = (X[:,1] - X[:,1].mean()) / X[:,1].std()\n",
        "\n",
        "X_tensor = torch.tensor(np.stack([log_d_norm, ε_norm], axis=1), dtype=torch.float32)\n",
        "y_tensor = torch.tensor(y, dtype=torch.float32)\n",
        "\n",
        "dataset = TensorDataset(X_tensor, y_tensor)\n",
        "loader  = DataLoader(dataset, batch_size=256, shuffle=True)\n",
        "\n",
        "# 4. Define the MLP with LayerNorm\n",
        "class NegativeEnergyAI(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dims, output_dim):\n",
        "        super().__init__()\n",
        "        layers = []\n",
        "        last_dim = input_dim\n",
        "        for h in hidden_dims:\n",
        "            layers += [\n",
        "                nn.Linear(last_dim, h),\n",
        "                nn.LayerNorm(h),\n",
        "                nn.ReLU(inplace=True)\n",
        "            ]\n",
        "            last_dim = h\n",
        "        layers.append(nn.Linear(last_dim, output_dim))\n",
        "        self.net = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "model = NegativeEnergyAI(input_dim=2, hidden_dims=[64,64,32], output_dim=1)\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "# 5. Training loop\n",
        "n_epochs = 200\n",
        "for epoch in range(1, n_epochs+1):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    for xb, yb in loader:\n",
        "        optimizer.zero_grad()\n",
        "        preds = model(xb)\n",
        "        loss  = criterion(preds, yb)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item() * xb.size(0)\n",
        "    total_loss /= len(dataset)\n",
        "    if epoch % 20 == 0:\n",
        "        print(f\"Epoch {epoch:03d}  MSE Loss: {total_loss:.3e}\")\n",
        "\n",
        "# 6. Evaluate on grid and plot\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    preds = model(X_tensor).numpy().flatten()\n",
        "\n",
        "# Plot predicted vs true force\n",
        "plt.figure(figsize=(6,6))\n",
        "plt.scatter(y, preds, s=5, alpha=0.3)\n",
        "plt.plot([y.min(), y.max()], [y.min(), y.max()], 'r--')\n",
        "plt.xlabel(\"True Casimir Force\")\n",
        "plt.ylabel(\"Predicted Force\")\n",
        "plt.title(\"Model Fit\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Surface visualization (heatmap)\n",
        "Z_true = y.reshape(n_perm, n_dist)\n",
        "Z_pred = preds.reshape(n_perm, n_dist)\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots(1,2, figsize=(12,4))\n",
        "im1 = ax1.pcolormesh(distances*1e6, permittivities, Z_true, shading='auto')\n",
        "ax1.set_title(\"True Force Surface\")\n",
        "ax1.set_xlabel(\"Distance (µm)\")\n",
        "ax1.set_ylabel(\"Permittivity\")\n",
        "fig.colorbar(im1, ax=ax1)\n",
        "\n",
        "im2 = ax2.pcolormesh(distances*1e6, permittivities, Z_pred, shading='auto')\n",
        "ax2.set_title(\"Predicted Force Surface\")\n",
        "ax2.set_xlabel(\"Distance (µm)\")\n",
        "ax2.set_ylabel(\"Permittivity\")\n",
        "fig.colorbar(im2, ax=ax2)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    }
  ]
}