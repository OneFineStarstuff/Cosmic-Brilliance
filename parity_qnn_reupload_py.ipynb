{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyMpKhHnHicbCGRx3KKgT+0P",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OneFineStarstuff/Cosmic-Brilliance/blob/main/parity_qnn_reupload_py.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9FgV_ZTRJXGF"
      },
      "outputs": [],
      "source": [
        "pip install pennylane torch tqdm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "\n",
        "import argparse\n",
        "import numpy as np\n",
        "import pennylane as qml\n",
        "\n",
        "# Try EfficientSU2, otherwise fall back\n",
        "try:\n",
        "    from pennylane.templates.layers import EfficientSU2 as AnsatzLayer\n",
        "    ANSATZ_NAME = \"EfficientSU2\"\n",
        "    HAS_EFFICIENT = True\n",
        "except ImportError:\n",
        "    from pennylane.templates.layers import StronglyEntanglingLayers as AnsatzLayer\n",
        "    ANSATZ_NAME = \"StronglyEntanglingLayers\"\n",
        "    HAS_EFFICIENT = False\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.calibration import calibration_curve\n",
        "\n",
        "def parse_args():\n",
        "    parser = argparse.ArgumentParser(description=\"Parity QNN (analytic, backprop)\")\n",
        "    parser.add_argument(\"--n_qubits\",   type=int,   default=3)\n",
        "    parser.add_argument(\"--epochs\",     type=int,   default=15)\n",
        "    parser.add_argument(\"--batch_size\", type=int,   default=16)\n",
        "    parser.add_argument(\"--lr\",         type=float, default=1e-2)\n",
        "    parser.add_argument(\"--mc_runs\",    type=int,   default=50)\n",
        "    parser.add_argument(\"--reps\",       type=int,   default=2)\n",
        "    args, _ = parser.parse_known_args()\n",
        "    return args\n",
        "\n",
        "def generate_parity_dataset(n_qubits):\n",
        "    X = np.array([list(map(int, np.binary_repr(i, n_qubits)))\n",
        "                  for i in range(2**n_qubits)], dtype=np.float32)\n",
        "    y = X.sum(axis=1) % 2\n",
        "    return X, y.astype(np.int64)\n",
        "\n",
        "class HybridQNN(nn.Module):\n",
        "    def __init__(self, qlayer, n_qubits, hidden_dim=4, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.pre  = nn.Sequential(nn.Linear(n_qubits, n_qubits), nn.ReLU())\n",
        "        self.q    = qlayer\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "        self.post = nn.Sequential(\n",
        "            nn.Linear(n_qubits, hidden_dim), nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, 2)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x_enc = self.pre(x)\n",
        "        q_out = self.q(x_enc)\n",
        "        return self.post(self.drop(q_out))\n",
        "\n",
        "def main():\n",
        "    args = parse_args()\n",
        "    print(f\"Using ansatz: {ANSATZ_NAME}\")\n",
        "\n",
        "    # Prepare data\n",
        "    X, y = generate_parity_dataset(args.n_qubits)\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y, test_size=0.2, random_state=42, stratify=y\n",
        "    )\n",
        "    train_ds = torch.utils.data.TensorDataset(torch.tensor(X_train), torch.tensor(y_train))\n",
        "    test_ds  = torch.utils.data.TensorDataset(torch.tensor(X_test),  torch.tensor(y_test))\n",
        "    train_loader = torch.utils.data.DataLoader(train_ds, batch_size=args.batch_size, shuffle=True)\n",
        "    test_loader  = torch.utils.data.DataLoader(test_ds,  batch_size=args.batch_size)\n",
        "\n",
        "    # Analytic device: supports backprop\n",
        "    dev = qml.device(\"default.qubit\", wires=args.n_qubits)\n",
        "\n",
        "    @qml.qnode(dev, interface=\"torch\", diff_method=\"backprop\")\n",
        "    def circuit(inputs, weights):\n",
        "        qml.templates.AngleEmbedding(inputs, wires=range(args.n_qubits), rotation=\"Y\")\n",
        "        if HAS_EFFICIENT:\n",
        "            AnsatzLayer(weights, wires=range(args.n_qubits),\n",
        "                        reps=args.reps, entanglement=\"full\")\n",
        "        else:\n",
        "            AnsatzLayer(weights, wires=range(args.n_qubits))\n",
        "        return [qml.expval(qml.PauliZ(i)) for i in range(args.n_qubits)]\n",
        "\n",
        "    # Weight shapes depend on ansatz\n",
        "    if HAS_EFFICIENT:\n",
        "        weight_shapes = {\"weights\": (args.reps, args.n_qubits)}\n",
        "    else:\n",
        "        weight_shapes = {\"weights\": (args.reps, args.n_qubits, 3)}\n",
        "\n",
        "    # Build hybrid model\n",
        "    qlayer   = qml.qnn.TorchLayer(circuit, weight_shapes)\n",
        "    model    = HybridQNN(qlayer, args.n_qubits)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=args.lr)\n",
        "    loss_fn   = nn.CrossEntropyLoss()\n",
        "\n",
        "    # Training loop\n",
        "    for epoch in range(1, args.epochs + 1):\n",
        "        model.train()\n",
        "        total_loss, correct = 0.0, 0\n",
        "        for xb, yb in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            logits = model(xb)\n",
        "            loss   = loss_fn(logits, yb)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item() * xb.size(0)\n",
        "            correct    += (logits.argmax(1) == yb).sum().item()\n",
        "        train_acc = correct / len(train_loader.dataset)\n",
        "\n",
        "        model.eval()\n",
        "        correct = 0\n",
        "        with torch.no_grad():\n",
        "            for xb, yb in test_loader:\n",
        "                correct += (model(xb).argmax(1) == yb).sum().item()\n",
        "        test_acc = correct / len(test_loader.dataset)\n",
        "\n",
        "        print(f\"Epoch {epoch:2d} | Loss {total_loss/len(train_loader.dataset):.4f}\"\n",
        "              f\" | Train {train_acc:.3f} | Test {test_acc:.3f}\")\n",
        "\n",
        "    # MC-Dropout for uncertainty\n",
        "    model.train()\n",
        "    all_probs = []\n",
        "    with torch.no_grad():\n",
        "        for _ in range(args.mc_runs):\n",
        "            batch_p = [torch.softmax(model(xb), dim=1).detach().cpu().numpy()\n",
        "                       for xb, _ in test_loader]\n",
        "            all_probs.append(np.vstack(batch_p))\n",
        "    var_probs = np.stack(all_probs).var(axis=0).mean(axis=1)\n",
        "    plt.hist(var_probs, bins=20)\n",
        "    plt.title(\"MC-Dropout Variance\")\n",
        "    plt.savefig(\"mc_variance.png\")\n",
        "    print(\"Saved MC variance histogram\")\n",
        "\n",
        "    # Temperature scaling on training set\n",
        "    model.eval()\n",
        "    logits_list, labels_list = [], []\n",
        "    with torch.no_grad():\n",
        "        for xb, yb in train_loader:\n",
        "            logits_list.append(model(xb))\n",
        "            labels_list.append(yb)\n",
        "    logits_stack = torch.cat(logits_list)\n",
        "    labels_stack = torch.cat(labels_list)\n",
        "\n",
        "    T = torch.ones(1, requires_grad=True)\n",
        "    def nll(): return loss_fn(logits_stack / T, labels_stack)\n",
        "    optT = optim.LBFGS([T], lr=0.1, max_iter=50)\n",
        "    optT.step(lambda: nll())\n",
        "    print(f\"Optimal temperature T = {T.item():.3f}\")\n",
        "\n",
        "    # Reliability diagram on test set\n",
        "    model.eval()\n",
        "    logits_test = torch.cat([model(xb) for xb, _ in test_loader])\n",
        "    probs_test  = torch.softmax(logits_test / T, dim=1).detach().cpu().numpy()[:, 1]\n",
        "    frac_pos, mean_pred = calibration_curve(y_test, probs_test, n_bins=10)\n",
        "\n",
        "    plt.figure()\n",
        "    plt.plot(mean_pred, frac_pos, \"s-\")\n",
        "    plt.plot([0, 1], [0, 1], \"--\")\n",
        "    plt.xlabel(\"Mean predicted probability\")\n",
        "    plt.ylabel(\"Fraction of positives\")\n",
        "    plt.title(\"Reliability Diagram\")\n",
        "    plt.savefig(\"reliability_diagram.png\")\n",
        "    print(\"Saved reliability diagram\")\n",
        "\n",
        "    # Final test accuracy\n",
        "    final_acc = (logits_test.argmax(1).detach().cpu().numpy() == y_test).mean()\n",
        "    print(f\"Final Test Accuracy: {final_acc:.3f}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "Fo9njmRPK7Ta"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}