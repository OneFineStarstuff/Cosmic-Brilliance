{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyP0Qj46vL6aXUDa9B1QWIX2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OneFineStarstuff/Cosmic-Brilliance/blob/main/Higher_Dimensional_Cognition_in_AI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "class HigherDimensionalAI(tf.keras.Model):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.dense1 = tf.keras.layers.Dense(128, activation='relu')\n",
        "        self.dense2 = tf.keras.layers.Dense(256, activation='relu')\n",
        "        self.dense3 = tf.keras.layers.Dense(512, activation='relu')  # Higher-dim projection\n",
        "        self.output_layer = tf.keras.layers.Dense(10, activation='softmax')\n",
        "\n",
        "    def call(self, inputs):\n",
        "        x = self.dense1(inputs)\n",
        "        x = self.dense2(x)\n",
        "        x = self.dense3(x)\n",
        "        return self.output_layer(x)\n",
        "\n",
        "# Reproducibility (optional, helpful in notebooks)\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "# Generate training data\n",
        "x_train = np.random.rand(1000, 10).astype(np.float32)\n",
        "y_train = np.random.randint(0, 10, size=(1000,)).astype(np.int32)\n",
        "\n",
        "# Train higher-dimensional AI\n",
        "model = HigherDimensionalAI()\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "model.fit(x_train, y_train, epochs=10, batch_size=32, validation_split=0.2, verbose=1)\n",
        "\n",
        "def build_model(input_dim=10, num_classes=10, l2=1e-4, dropout=0.2):\n",
        "    he = tf.keras.initializers.HeNormal()\n",
        "    reg = tf.keras.regularizers.l2(l2)\n",
        "\n",
        "    inputs = tf.keras.Input(shape=(input_dim,), name=\"features\")\n",
        "    x = tf.keras.layers.Dense(128, activation='relu', kernel_initializer=he, kernel_regularizer=reg)(inputs)\n",
        "    x = tf.keras.layers.Dropout(dropout)(x)\n",
        "    x = tf.keras.layers.Dense(256, activation='relu', kernel_initializer=he, kernel_regularizer=reg)(x)\n",
        "    x = tf.keras.layers.Dropout(dropout)(x)\n",
        "    x = tf.keras.layers.Dense(512, activation='relu', kernel_initializer=he, kernel_regularizer=reg)(x)\n",
        "    outputs = tf.keras.layers.Dense(num_classes, activation='softmax')(x)\n",
        "    return tf.keras.Model(inputs, outputs, name=\"HigherDimensionalAI\")\n",
        "\n",
        "def main():\n",
        "    # Reproducibility\n",
        "    np.random.seed(42)\n",
        "    tf.random.set_seed(42)\n",
        "\n",
        "    # Data\n",
        "    x = np.random.rand(1000, 10).astype(np.float32)\n",
        "    y = np.random.randint(0, 10, size=(1000,)).astype(np.int32)\n",
        "\n",
        "    # Model\n",
        "    model = build_model(input_dim=10, num_classes=10, l2=1e-4, dropout=0.2)\n",
        "    model.summary()\n",
        "\n",
        "    # Callbacks\n",
        "    cbs = [\n",
        "        tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=3, verbose=1),\n",
        "        tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=8, restore_best_weights=True, verbose=1),\n",
        "    ]\n",
        "\n",
        "    # Compile + train\n",
        "    model.compile(optimizer=tf.keras.optimizers.Adam(1e-3),\n",
        "                  loss=\"sparse_categorical_crossentropy\",\n",
        "                  metrics=[\"accuracy\"])\n",
        "    model.fit(x, y, epochs=30, batch_size=64, validation_split=0.2, callbacks=cbs, verbose=1)\n",
        "\n",
        "    # Evaluate\n",
        "    loss, acc = model.evaluate(x, y, verbose=0)\n",
        "    print(f\"Final metrics â€” loss: {loss:.4f}, acc: {acc:.4f}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "g0VNMWQKIuB9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wayVqhb5GRGi"
      },
      "outputs": [],
      "source": [
        "#!/usr/bin/env python3\n",
        "# -*- coding: utf-8 -*-\n",
        "\n",
        "import argparse\n",
        "import json\n",
        "import os\n",
        "import sys\n",
        "from dataclasses import asdict, dataclass\n",
        "from datetime import datetime\n",
        "from typing import Any, Dict, List, Optional, Sequence, Tuple\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "# ------------------------------\n",
        "# Utilities\n",
        "# ------------------------------\n",
        "\n",
        "def log(msg: str) -> None:\n",
        "    ts = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "    print(f\"[{ts}] {msg}\", flush=True)\n",
        "\n",
        "\n",
        "def ensure_dir(path: str) -> None:\n",
        "    os.makedirs(path, exist_ok=True)\n",
        "\n",
        "\n",
        "def save_json(path: str, data: Dict[str, Any]) -> None:\n",
        "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(data, f, indent=2, sort_keys=True)\n",
        "\n",
        "\n",
        "def load_json(path: str) -> Dict[str, Any]:\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "        return json.load(f)\n",
        "\n",
        "\n",
        "def sanitize_argv(argv: Sequence[str]) -> List[str]:\n",
        "    \"\"\"Remove Jupyter/Colab stray args like -f and *.json kernel files.\"\"\"\n",
        "    bad_flags = {\"-f\", \"--f\"}\n",
        "    cleaned: List[str] = []\n",
        "    skip_next = False\n",
        "    for i, a in enumerate(argv):\n",
        "        if skip_next:\n",
        "            skip_next = False\n",
        "            continue\n",
        "        if a in bad_flags:\n",
        "            # Skip this flag and its possible value\n",
        "            if i + 1 < len(argv) and not argv[i + 1].startswith(\"-\"):\n",
        "                skip_next = True\n",
        "            continue\n",
        "        if a.endswith(\".json\") and os.path.isfile(a):\n",
        "            continue\n",
        "        cleaned.append(a)\n",
        "    return cleaned\n",
        "\n",
        "\n",
        "# ------------------------------\n",
        "# Configs\n",
        "# ------------------------------\n",
        "\n",
        "@dataclass\n",
        "class DataConfig:\n",
        "    dataset: str = \"moons\"              # moons|circles|blobs\n",
        "    n_samples: int = 1000\n",
        "    noise: float = 0.1\n",
        "    centers: int = 2                    # blobs only\n",
        "    cluster_std: float = 0.6            # blobs only\n",
        "    val_split: float = 0.2\n",
        "    test_split: float = 0.2\n",
        "    seed: int = 42\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class TrainConfig:\n",
        "    run_dir: str = \"runs/exp\"\n",
        "    hidden_sizes: List[int] = None\n",
        "    lr: float = 1e-3\n",
        "    weight_decay: float = 0.0\n",
        "    epochs: int = 30\n",
        "    batch_size: int = 32\n",
        "    device: str = \"auto\"                # auto|cpu|gpu\n",
        "\n",
        "\n",
        "# ------------------------------\n",
        "# Data generation (NumPy-only)\n",
        "# ------------------------------\n",
        "\n",
        "def gen_moons(n: int, noise: float, rng: np.random.Generator) -> Tuple[np.ndarray, np.ndarray]:\n",
        "    n1 = n // 2\n",
        "    n2 = n - n1\n",
        "    # Moon A (upper)\n",
        "    t1 = rng.uniform(0, np.pi, size=n1)\n",
        "    x1 = np.c_[np.cos(t1), np.sin(t1)]\n",
        "    # Moon B (lower, shifted)\n",
        "    t2 = rng.uniform(0, np.pi, size=n2)\n",
        "    x2 = np.c_[1 - np.cos(t2), 1 - np.sin(t2) - 0.5]\n",
        "    X = np.vstack([x1, x2])\n",
        "    y = np.concatenate([np.zeros(n1, dtype=np.int32), np.ones(n2, dtype=np.int32)])\n",
        "    X += rng.normal(scale=noise, size=X.shape)\n",
        "    return X.astype(np.float32), y\n",
        "\n",
        "\n",
        "def gen_circles(n: int, noise: float, rng: np.random.Generator) -> Tuple[np.ndarray, np.ndarray]:\n",
        "    n1 = n // 2\n",
        "    n2 = n - n1\n",
        "    t1 = rng.uniform(0, 2 * np.pi, size=n1)\n",
        "    t2 = rng.uniform(0, 2 * np.pi, size=n2)\n",
        "    r1 = 1.0 + rng.normal(scale=noise, size=n1)\n",
        "    r2 = 0.5 + rng.normal(scale=noise, size=n2)\n",
        "    x1 = np.c_[r1 * np.cos(t1), r1 * np.sin(t1)]\n",
        "    x2 = np.c_[r2 * np.cos(t2), r2 * np.sin(t2)]\n",
        "    X = np.vstack([x1, x2]).astype(np.float32)\n",
        "    y = np.concatenate([np.zeros(n1, dtype=np.int32), np.ones(n2, dtype=np.int32)])\n",
        "    return X, y\n",
        "\n",
        "\n",
        "def gen_blobs(n: int, centers: int, cluster_std: float, rng: np.random.Generator) -> Tuple[np.ndarray, np.ndarray]:\n",
        "    # Place centers in a square and generate Gaussian blobs\n",
        "    c_xy = rng.uniform(-3, 3, size=(centers, 2))\n",
        "    # Distribute samples per center\n",
        "    counts = [n // centers] * centers\n",
        "    for i in range(n % centers):\n",
        "        counts[i] += 1\n",
        "    parts = []\n",
        "    labels = []\n",
        "    for k, cnt in enumerate(counts):\n",
        "        pts = c_xy[k] + rng.normal(scale=cluster_std, size=(cnt, 2))\n",
        "        parts.append(pts)\n",
        "        labels.append(np.full(cnt, k, dtype=np.int32))\n",
        "    X = np.vstack(parts).astype(np.float32)\n",
        "    y = np.concatenate(labels)\n",
        "    return X, y\n",
        "\n",
        "\n",
        "def make_dataset(cfg: DataConfig) -> Tuple[np.ndarray, np.ndarray]:\n",
        "    rng = np.random.default_rng(cfg.seed)\n",
        "    if cfg.dataset == \"moons\":\n",
        "        return gen_moons(cfg.n_samples, cfg.noise, rng)\n",
        "    elif cfg.dataset == \"circles\":\n",
        "        return gen_circles(cfg.n_samples, cfg.noise, rng)\n",
        "    elif cfg.dataset == \"blobs\":\n",
        "        return gen_blobs(cfg.n_samples, cfg.centers, cfg.cluster_std, rng)\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown dataset: {cfg.dataset}\")\n",
        "\n",
        "\n",
        "def train_val_test_split(\n",
        "    X: np.ndarray, y: np.ndarray, val_split: float, test_split: float, seed: int\n",
        ") -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
        "    rng = np.random.default_rng(seed)\n",
        "    n = len(X)\n",
        "    idx = np.arange(n)\n",
        "    rng.shuffle(idx)\n",
        "    X = X[idx]\n",
        "    y = y[idx]\n",
        "    n_test = int(n * test_split)\n",
        "    n_val = int(n * val_split)\n",
        "    X_test, y_test = X[:n_test], y[:n_test]\n",
        "    X_val, y_val = X[n_test:n_test + n_val], y[n_test:n_test + n_val]\n",
        "    X_train, y_train = X[n_test + n_val:], y[n_test + n_val:]\n",
        "    return X_train, y_train, X_val, y_val, X_test, y_test\n",
        "\n",
        "\n",
        "# ------------------------------\n",
        "# Model builders\n",
        "# ------------------------------\n",
        "\n",
        "def build_dense_model(\n",
        "    input_dim: int,\n",
        "    hidden_sizes: List[int],\n",
        "    num_classes: int,\n",
        "    weight_decay: float = 0.0,\n",
        "    lr: float = 1e-3,\n",
        ") -> tf.keras.Model:\n",
        "    reg = tf.keras.regularizers.l2(weight_decay) if weight_decay > 0 else None\n",
        "    he = tf.keras.initializers.HeNormal()\n",
        "\n",
        "    inputs = tf.keras.Input(shape=(input_dim,), name=\"features\")\n",
        "    norm = tf.keras.layers.Normalization(name=\"norm\")\n",
        "    x = norm(inputs)\n",
        "    for i, h in enumerate(hidden_sizes or []):\n",
        "        x = tf.keras.layers.Dense(h, activation=\"relu\", kernel_initializer=he, kernel_regularizer=reg, name=f\"dense_{i+1}\")(x)\n",
        "        x = tf.keras.layers.Dropout(0.1, name=f\"drop_{i+1}\")(x)\n",
        "    outputs = tf.keras.layers.Dense(num_classes, activation=\"softmax\", name=\"logits\")(x)\n",
        "\n",
        "    model = tf.keras.Model(inputs, outputs, name=\"tabular_classifier\")\n",
        "    model.compile(\n",
        "        optimizer=tf.keras.optimizers.Adam(learning_rate=lr),\n",
        "        loss=\"sparse_categorical_crossentropy\",\n",
        "        metrics=[\"accuracy\"],\n",
        "    )\n",
        "    return model\n",
        "\n",
        "\n",
        "# ------------------------------\n",
        "# Train / Eval / Predict / Export / Benchmark\n",
        "# ------------------------------\n",
        "\n",
        "def save_run_artifacts(run_dir: str, data_cfg: DataConfig, train_cfg: TrainConfig) -> None:\n",
        "    ensure_dir(run_dir)\n",
        "    save_json(os.path.join(run_dir, \"data_config.json\"), asdict(data_cfg))\n",
        "    save_json(os.path.join(run_dir, \"train_config.json\"), {\n",
        "        **asdict(train_cfg),\n",
        "        \"hidden_sizes\": train_cfg.hidden_sizes or []\n",
        "    })\n",
        "\n",
        "\n",
        "def train_model(data_cfg: DataConfig, train_cfg: TrainConfig) -> Dict[str, Any]:\n",
        "    log(\"Preparing data...\")\n",
        "    X, y = make_dataset(data_cfg)\n",
        "    num_classes = int(y.max()) + 1\n",
        "    X_tr, y_tr, X_val, y_val, X_te, y_te = train_val_test_split(\n",
        "        X, y, data_cfg.val_split, data_cfg.test_split, seed=data_cfg.seed + 1\n",
        "    )\n",
        "\n",
        "    run_dir = train_cfg.run_dir\n",
        "    ensure_dir(run_dir)\n",
        "    save_run_artifacts(run_dir, data_cfg, train_cfg)\n",
        "\n",
        "    # Build model\n",
        "    log(\"Building model...\")\n",
        "    model = build_dense_model(\n",
        "        input_dim=X.shape[1],\n",
        "        hidden_sizes=train_cfg.hidden_sizes or [64, 64],\n",
        "        num_classes=num_classes,\n",
        "        weight_decay=train_cfg.weight_decay,\n",
        "        lr=train_cfg.lr,\n",
        "    )\n",
        "\n",
        "    # Adapt normalization on training set\n",
        "    norm_layer = model.get_layer(\"norm\")\n",
        "    norm_layer.adapt(X_tr)\n",
        "\n",
        "    ckpt_path = os.path.join(run_dir, \"best.keras\")\n",
        "    callbacks = [\n",
        "        tf.keras.callbacks.ModelCheckpoint(ckpt_path, monitor=\"val_loss\", save_best_only=True, verbose=1),\n",
        "        tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=3, verbose=1),\n",
        "        tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=8, restore_best_weights=True, verbose=1),\n",
        "        tf.keras.callbacks.CSVLogger(os.path.join(run_dir, \"history.csv\")),\n",
        "    ]\n",
        "\n",
        "    log(\"Starting training...\")\n",
        "    history = model.fit(\n",
        "        X_tr, y_tr,\n",
        "        validation_data=(X_val, y_val),\n",
        "        epochs=train_cfg.epochs,\n",
        "        batch_size=train_cfg.batch_size,\n",
        "        verbose=1,\n",
        "        callbacks=callbacks,\n",
        "    )\n",
        "\n",
        "    log(\"Evaluating on test set...\")\n",
        "    test_loss, test_acc = model.evaluate(X_te, y_te, verbose=0)\n",
        "    metrics = {\n",
        "        \"test_loss\": float(test_loss),\n",
        "        \"test_acc\": float(test_acc),\n",
        "        \"n_train\": int(len(X_tr)),\n",
        "        \"n_val\": int(len(X_val)),\n",
        "        \"n_test\": int(len(X_te)),\n",
        "    }\n",
        "    save_json(os.path.join(run_dir, \"metrics.json\"), metrics)\n",
        "\n",
        "    # Save a final copy of the model (best already saved via checkpoint)\n",
        "    model.save(os.path.join(run_dir, \"latest.keras\"))\n",
        "    log(f\"Done. Run dir: {run_dir}\")\n",
        "    return {\"run_dir\": run_dir, \"metrics\": metrics, \"history_keys\": list(history.history.keys())}\n",
        "\n",
        "\n",
        "def reconstruct_configs(run_dir: str) -> Tuple[DataConfig, TrainConfig]:\n",
        "    data_cfg = DataConfig(**load_json(os.path.join(run_dir, \"data_config.json\")))\n",
        "    tcfg_dict = load_json(os.path.join(run_dir, \"train_config.json\"))\n",
        "    # Ensure list type for hidden sizes\n",
        "    tcfg_dict[\"hidden_sizes\"] = list(tcfg_dict.get(\"hidden_sizes\", []))\n",
        "    train_cfg = TrainConfig(**tcfg_dict)\n",
        "    return data_cfg, train_cfg\n",
        "\n",
        "\n",
        "def evaluate_model(run_dir: str) -> Dict[str, Any]:\n",
        "    log(f\"Loading configs from: {run_dir}\")\n",
        "    data_cfg, _ = reconstruct_configs(run_dir)\n",
        "\n",
        "    # Re-generate deterministic dataset and splits\n",
        "    X, y = make_dataset(data_cfg)\n",
        "    X_tr, y_tr, X_val, y_val, X_te, y_te = train_val_test_split(\n",
        "        X, y, data_cfg.val_split, data_cfg.test_split, seed=data_cfg.seed + 1\n",
        "    )\n",
        "\n",
        "    # Load best checkpoint\n",
        "    best_path = os.path.join(run_dir, \"best.keras\")\n",
        "    if not os.path.exists(best_path):\n",
        "        raise FileNotFoundError(f\"Best checkpoint not found: {best_path}\")\n",
        "    model = tf.keras.models.load_model(best_path)\n",
        "\n",
        "    log(\"Evaluating best checkpoint on test set...\")\n",
        "    loss, acc = model.evaluate(X_te, y_te, verbose=0)\n",
        "    out = {\"test_loss\": float(loss), \"test_acc\": float(acc)}\n",
        "    log(f\"Test â€” loss: {loss:.4f}, acc: {acc:.4f}\")\n",
        "    return out\n",
        "\n",
        "\n",
        "def predict_csv(run_dir: str, csv_path: str, output: Optional[str], features: Optional[List[str]], include_proba: bool) -> str:\n",
        "    try:\n",
        "        import pandas as pd\n",
        "    except Exception as e:\n",
        "        raise RuntimeError(\"pandas is required for CSV prediction. Please `pip install pandas`.\") from e\n",
        "\n",
        "    best_path = os.path.join(run_dir, \"best.keras\")\n",
        "    if not os.path.exists(best_path):\n",
        "        raise FileNotFoundError(f\"Best checkpoint not found: {best_path}\")\n",
        "    model = tf.keras.models.load_model(best_path)\n",
        "\n",
        "    df = pd.read_csv(csv_path)\n",
        "    if features:\n",
        "        missing = [c for c in features if c not in df.columns]\n",
        "        if missing:\n",
        "            raise ValueError(f\"Missing feature columns in CSV: {missing}\")\n",
        "        X = df[features].to_numpy(dtype=np.float32)\n",
        "    else:\n",
        "        # Use all numeric columns\n",
        "        num_df = df.select_dtypes(include=[np.number])\n",
        "        if num_df.empty:\n",
        "            raise ValueError(\"No numeric columns found. Provide --features.\")\n",
        "        X = num_df.to_numpy(dtype=np.float32)\n",
        "\n",
        "    preds = model.predict(X, verbose=0)\n",
        "    pred_labels = np.argmax(preds, axis=1)\n",
        "\n",
        "    out_df = df.copy()\n",
        "    out_df[\"prediction\"] = pred_labels\n",
        "    if include_proba:\n",
        "        for i in range(preds.shape[1]):\n",
        "            out_df[f\"proba_{i}\"] = preds[:, i]\n",
        "\n",
        "    if output is None or output == \"\":\n",
        "        base, ext = os.path.splitext(csv_path)\n",
        "        output = base + \".predictions.csv\"\n",
        "    out_df.to_csv(output, index=False)\n",
        "    log(f\"Wrote predictions to: {output}\")\n",
        "    return output\n",
        "\n",
        "\n",
        "def export_onnx(run_dir: str, output: Optional[str]) -> str:\n",
        "    best_path = os.path.join(run_dir, \"best.keras\")\n",
        "    if not os.path.exists(best_path):\n",
        "        raise FileNotFoundError(f\"Best checkpoint not found: {best_path}\")\n",
        "    model = tf.keras.models.load_model(best_path)\n",
        "\n",
        "    if output is None or output == \"\":\n",
        "        output = os.path.join(run_dir, \"model.onnx\")\n",
        "\n",
        "    try:\n",
        "        import tf2onnx\n",
        "        import onnx  # noqa: F401 (ensures ONNX is available)\n",
        "    except Exception as e:\n",
        "        raise RuntimeError(\"ONNX export requires tf2onnx and onnx. Try: pip install tf2onnx onnx\") from e\n",
        "\n",
        "    log(\"Converting to ONNX...\")\n",
        "    spec = (tf.TensorSpec(model.inputs[0].shape, model.inputs[0].dtype, name=model.inputs[0].name),)\n",
        "    model_proto, _ = tf2onnx.convert.from_keras(model, input_signature=spec, opset=13)\n",
        "    with open(output, \"wb\") as f:\n",
        "        f.write(model_proto.SerializeToString())\n",
        "    log(f\"Exported ONNX to: {output}\")\n",
        "    return output\n",
        "\n",
        "\n",
        "def benchmark(data_cfg: DataConfig, train_cfg: TrainConfig, seeds: List[int]) -> Dict[str, Any]:\n",
        "    results = []\n",
        "    log(f\"Benchmarking seeds: {seeds}\")\n",
        "    for s in seeds:\n",
        "        run_tag = f\"{train_cfg.run_dir}_seed{s}\"\n",
        "        tcfg = TrainConfig(\n",
        "            run_dir=run_tag,\n",
        "            hidden_sizes=train_cfg.hidden_sizes,\n",
        "            lr=train_cfg.lr,\n",
        "            weight_decay=train_cfg.weight_decay,\n",
        "            epochs=train_cfg.epochs,\n",
        "            batch_size=train_cfg.batch_size,\n",
        "            device=train_cfg.device,\n",
        "        )\n",
        "        dcfg = DataConfig(\n",
        "            dataset=data_cfg.dataset,\n",
        "            n_samples=data_cfg.n_samples,\n",
        "            noise=data_cfg.noise,\n",
        "            centers=data_cfg.centers,\n",
        "            cluster_std=data_cfg.cluster_std,\n",
        "            val_split=data_cfg.val_split,\n",
        "            test_split=data_cfg.test_split,\n",
        "            seed=s,\n",
        "        )\n",
        "        out = train_model(dcfg, tcfg)\n",
        "        results.append({\"seed\": s, **out[\"metrics\"]})\n",
        "\n",
        "    # Aggregate\n",
        "    test_accs = [r[\"test_acc\"] for r in results]\n",
        "    test_losses = [r[\"test_loss\"] for r in results]\n",
        "    summary = {\n",
        "        \"seeds\": seeds,\n",
        "        \"mean_acc\": float(np.mean(test_accs)),\n",
        "        \"std_acc\": float(np.std(test_accs)),\n",
        "        \"mean_loss\": float(np.mean(test_losses)),\n",
        "        \"std_loss\": float(np.std(test_losses)),\n",
        "        \"n_runs\": len(seeds),\n",
        "    }\n",
        "\n",
        "    ensure_dir(train_cfg.run_dir)\n",
        "    save_json(os.path.join(train_cfg.run_dir, \"benchmark_summary.json\"), summary)\n",
        "    # Save per-seed CSV\n",
        "    try:\n",
        "        import pandas as pd\n",
        "        import csv\n",
        "        pd.DataFrame(results).to_csv(os.path.join(train_cfg.run_dir, \"benchmark_results.csv\"), index=False)\n",
        "    except Exception:\n",
        "        # Minimal CSV writer fallback\n",
        "        p = os.path.join(train_cfg.run_dir, \"benchmark_results.csv\")\n",
        "        keys = list(results[0].keys())\n",
        "        with open(p, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
        "            import csv as _csv\n",
        "            w = _csv.DictWriter(f, fieldnames=keys)\n",
        "            w.writeheader()\n",
        "            w.writerows(results)\n",
        "    log(f\"Benchmark summary: acc={summary['mean_acc']:.4f}Â±{summary['std_acc']:.4f}, \"\n",
        "        f\"loss={summary['mean_loss']:.4f}Â±{summary['std_loss']:.4f}\")\n",
        "    return {\"results\": results, \"summary\": summary}\n",
        "\n",
        "\n",
        "# ------------------------------\n",
        "# Command handlers (CLI)\n",
        "# ------------------------------\n",
        "\n",
        "def cmd_train(args: argparse.Namespace) -> None:\n",
        "    data_cfg = DataConfig(\n",
        "        dataset=args.dataset,\n",
        "        n_samples=args.nsamples,\n",
        "        noise=args.noise,\n",
        "        centers=args.centers,\n",
        "        cluster_std=args.clusterstd,\n",
        "        val_split=args.valsplit,\n",
        "        test_split=args.testsplit,\n",
        "        seed=args.seed,\n",
        "    )\n",
        "    hidden = [int(x) for x in args.hidden_sizes.split(\",\") if x.strip()]\n",
        "    train_cfg = TrainConfig(\n",
        "        run_dir=args.rundir,\n",
        "        hidden_sizes=hidden,\n",
        "        lr=args.lr,\n",
        "        weight_decay=args.weightdecay,\n",
        "        epochs=args.epochs,\n",
        "        batch_size=args.batchsize,\n",
        "        device=args.device,\n",
        "    )\n",
        "    log(\"Starting training...\")\n",
        "    out = train_model(data_cfg, train_cfg)\n",
        "    log(f\"Done. Run dir: {out['run_dir']}\")\n",
        "\n",
        "\n",
        "def cmd_eval(args: argparse.Namespace) -> None:\n",
        "    log(\"Evaluating best checkpoint...\")\n",
        "    evaluate_model(args.rundir)\n",
        "\n",
        "\n",
        "def cmd_predict(args: argparse.Namespace) -> None:\n",
        "    feats = [c.strip() for c in args.features.split(\",\") if c.strip()] if args.features else None\n",
        "    predict_csv(args.rundir, args.csv, output=args.output, features=feats, include_proba=(not args.noproba))\n",
        "\n",
        "\n",
        "def cmd_export_onnx(args: argparse.Namespace) -> None:\n",
        "    export_onnx(args.rundir, args.output)\n",
        "\n",
        "\n",
        "def cmd_benchmark(args: argparse.Namespace) -> None:\n",
        "    data_cfg = DataConfig(\n",
        "        dataset=args.dataset,\n",
        "        n_samples=args.nsamples,\n",
        "        noise=args.noise,\n",
        "        centers=args.centers,\n",
        "        cluster_std=args.clusterstd,\n",
        "        val_split=args.valsplit,\n",
        "        test_split=args.testsplit,\n",
        "        seed=0,  # overridden per run\n",
        "    )\n",
        "    hidden = [int(x) for x in args.hidden_sizes.split(\",\") if x.strip()]\n",
        "    train_cfg = TrainConfig(\n",
        "        run_dir=args.rundir,\n",
        "        hidden_sizes=hidden,\n",
        "        lr=args.lr,\n",
        "        weight_decay=args.weightdecay,\n",
        "        epochs=args.epochs,\n",
        "        batch_size=args.batchsize,\n",
        "        device=args.device,\n",
        "    )\n",
        "    seeds = [int(s) for s in args.seeds.split(\",\") if s.strip()]\n",
        "    benchmark(data_cfg, train_cfg, seeds)\n",
        "\n",
        "\n",
        "# ------------------------------\n",
        "# CLI parser\n",
        "# ------------------------------\n",
        "\n",
        "def build_parser() -> argparse.ArgumentParser:\n",
        "    p = argparse.ArgumentParser(\n",
        "        prog=\"metaintel\",\n",
        "        description=\"MetaIntelligence CLI: train/eval/predict/benchmark/export-onnx\",\n",
        "        formatter_class=argparse.ArgumentDefaultsHelpFormatter,\n",
        "    )\n",
        "    sub = p.add_subparsers(dest=\"command\", required=True)\n",
        "\n",
        "    # Train\n",
        "    t = sub.add_parser(\"train\", help=\"Train a model\", formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n",
        "    t.add_argument(\"--dataset\", type=str, default=\"moons\", choices=[\"moons\", \"circles\", \"blobs\"], help=\"Synthetic dataset\")\n",
        "    t.add_argument(\"--nsamples\", type=int, default=1000, help=\"Total samples\")\n",
        "    t.add_argument(\"--noise\", type=float, default=0.1, help=\"Noise level\")\n",
        "    t.add_argument(\"--centers\", type=int, default=2, help=\"For blobs: number of centers\")\n",
        "    t.add_argument(\"--clusterstd\", type=float, default=0.6, help=\"For blobs: std of clusters\")\n",
        "    t.add_argument(\"--valsplit\", type=float, default=0.2, help=\"Validation split\")\n",
        "    t.add_argument(\"--testsplit\", type=float, default=0.2, help=\"Test split\")\n",
        "    t.add_argument(\"--seed\", type=int, default=42, help=\"Random seed\")\n",
        "    t.add_argument(\"--hidden-sizes\", type=str, default=\"64,64\", help=\"Comma-separated hidden sizes\")\n",
        "    t.add_argument(\"--lr\", type=float, default=1e-3, help=\"Learning rate\")\n",
        "    t.add_argument(\"--weightdecay\", type=float, default=0.0, help=\"L2 weight decay (regularizer)\")\n",
        "    t.add_argument(\"--epochs\", type=int, default=30, help=\"Training epochs\")\n",
        "    t.add_argument(\"--batchsize\", type=int, default=32, help=\"Batch size\")\n",
        "    t.add_argument(\"--device\", type=str, default=\"auto\", choices=[\"auto\", \"cpu\", \"gpu\"], help=\"Device preference (informational)\")\n",
        "    t.add_argument(\"--rundir\", type=str, required=True, help=\"Run directory\")\n",
        "    t.set_defaults(func=cmd_train)\n",
        "\n",
        "    # Eval\n",
        "    e = sub.add_parser(\"eval\", help=\"Evaluate best checkpoint on test set\", formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n",
        "    e.add_argument(\"--rundir\", type=str, required=True, help=\"Run directory\")\n",
        "    e.set_defaults(func=cmd_eval)\n",
        "\n",
        "    # Predict\n",
        "    pr = sub.add_parser(\"predict\", help=\"Predict on CSV (tabular only)\", formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n",
        "    pr.add_argument(\"--rundir\", type=str, required=True, help=\"Run directory with trained model\")\n",
        "    pr.add_argument(\"--csv\", type=str, required=True, help=\"Input CSV path\")\n",
        "    pr.add_argument(\"--output\", type=str, default=\"\", help=\"Output CSV path\")\n",
        "    pr.add_argument(\"--features\", type=str, default=\"\", help=\"Comma-separated feature columns (default: all numeric)\")\n",
        "    pr.add_argument(\"--noproba\", action=\"store_true\", help=\"Do not include class probabilities\")\n",
        "    pr.set_defaults(func=cmd_predict)\n",
        "\n",
        "    # Export ONNX\n",
        "    ex = sub.add_parser(\"export-onnx\", help=\"Export best checkpoint to ONNX\", formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n",
        "    ex.add_argument(\"--rundir\", type=str, required=True, help=\"Run directory\")\n",
        "    ex.add_argument(\"--output\", type=str, default=\"\", help=\"ONNX output path\")\n",
        "    ex.set_defaults(func=cmd_export_onnx)\n",
        "\n",
        "    # Benchmark\n",
        "    b = sub.add_parser(\"benchmark\", help=\"Benchmark multiple seeds\", formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n",
        "    b.add_argument(\"--dataset\", type=str, default=\"moons\", choices=[\"moons\", \"circles\", \"blobs\"], help=\"Synthetic dataset\")\n",
        "    b.add_argument(\"--nsamples\", type=int, default=1000, help=\"Total samples\")\n",
        "    b.add_argument(\"--noise\", type=float, default=0.1, help=\"Noise level\")\n",
        "    b.add_argument(\"--centers\", type=int, default=2, help=\"For blobs: number of centers\")\n",
        "    b.add_argument(\"--clusterstd\", type=float, default=0.6, help=\"For blobs: std of clusters\")\n",
        "    b.add_argument(\"--valsplit\", type=float, default=0.2, help=\"Validation split\")\n",
        "    b.add_argument(\"--testsplit\", type=float, default=0.2, help=\"Test split\")\n",
        "    b.add_argument(\"--hidden-sizes\", type=str, default=\"64,64\", help=\"Comma-separated hidden sizes\")\n",
        "    b.add_argument(\"--lr\", type=float, default=1e-3, help=\"Learning rate\")\n",
        "    b.add_argument(\"--weightdecay\", type=float, default=0.0, help=\"L2 weight decay (regularizer)\")\n",
        "    b.add_argument(\"--epochs\", type=int, default=20, help=\"Training epochs per seed\")\n",
        "    b.add_argument(\"--batchsize\", type=int, default=32, help=\"Batch size\")\n",
        "    b.add_argument(\"--device\", type=str, default=\"auto\", choices=[\"auto\", \"cpu\", \"gpu\"], help=\"Device preference (informational)\")\n",
        "    b.add_argument(\"--rundir\", type=str, required=True, help=\"Root run directory for benchmark summary\")\n",
        "    b.add_argument(\"--seeds\", type=str, default=\"1,2,3,4,5\", help=\"Comma-separated seeds\")\n",
        "    b.set_defaults(func=cmd_benchmark)\n",
        "\n",
        "    return p\n",
        "\n",
        "\n",
        "# ------------------------------\n",
        "# Main (Notebook/Colab-safe)\n",
        "# ------------------------------\n",
        "\n",
        "def main(argv: Optional[List[str]] = None):\n",
        "    import sys\n",
        "    raw_args = argv if argv is not None else sys.argv[1:]\n",
        "    filtered_args = sanitize_argv(raw_args)\n",
        "\n",
        "    parser = build_parser()\n",
        "\n",
        "    if not filtered_args:\n",
        "        parser.print_help()\n",
        "        print(\"\\nðŸ“Œ Example usage (in notebooks):\")\n",
        "        print('main([\"train\", \"--dataset\", \"moons\", \"--epochs\", \"30\", \"--rundir\", \"runs/moonstest\"])')\n",
        "        print('main([\"eval\", \"--rundir\", \"runs/moonstest\"])')\n",
        "        print('main([\"export-onnx\", \"--rundir\", \"runs/moonstest\"])')\n",
        "        return\n",
        "\n",
        "    try:\n",
        "        args = parser.parse_args(filtered_args)\n",
        "    except SystemExit:\n",
        "        parser.print_help()\n",
        "        print(\"\\nðŸ“Œ Example usage (in notebooks):\")\n",
        "        print('main([\"train\", \"--dataset\", \"moons\", \"--epochs\", \"30\", \"--rundir\", \"runs/moonstest\"])')\n",
        "        print('main([\"eval\", \"--rundir\", \"runs/moonstest\"])')\n",
        "        print('main([\"export-onnx\", \"--rundir\", \"runs/moonstest\"])')\n",
        "        return\n",
        "\n",
        "    func = getattr(args, \"func\", None)\n",
        "    if func is None:\n",
        "        parser.print_help()\n",
        "        print(\"\\nðŸ“Œ Example usage (in notebooks):\")\n",
        "        print('main([\"train\", \"--dataset\", \"moons\", \"--epochs\", \"30\", \"--rundir\", \"runs/moonstest\"])')\n",
        "        print('main([\"eval\", \"--rundir\", \"runs/moonstest\"])')\n",
        "        print('main([\"export-onnx\", \"--rundir\", \"runs/moonstest\"])')\n",
        "        return\n",
        "\n",
        "    func(args)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ]
}