{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyMNKaarpYCYVJTNhLyB2FCw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OneFineStarstuff/Cosmic-Brilliance/blob/main/MetaIntelligence_CLI_(utf_8)_py.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rKrTiJv_5J1a"
      },
      "outputs": [],
      "source": [
        "#!/usr/bin/env python3\n",
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "MetaIntelligence CLI: train/eval/predict/benchmark/export-onnx\n",
        "\n",
        "End-to-end, Colab/Notebook-safe launcher:\n",
        "- Train an MLP on synthetic datasets (moons, circles, blobs)\n",
        "- Save best checkpoint, scaler, config, and metrics\n",
        "- Evaluate on test split (reproducible via saved config/seed)\n",
        "- Predict on CSV with saved scaler/model (auto or explicit feature selection)\n",
        "- Export ONNX graph\n",
        "- Benchmark across multiple seeds\n",
        "\n",
        "Dependencies:\n",
        "  pip install torch scikit-learn pandas numpy joblib\n",
        "\n",
        "Optional for convenience (exporting ONNX doesn't require onnx package):\n",
        "  pip install onnx onnxruntime\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import json\n",
        "import time\n",
        "import math\n",
        "import shutil\n",
        "import random\n",
        "import argparse\n",
        "from dataclasses import dataclass, asdict\n",
        "from pathlib import Path\n",
        "from typing import List, Tuple, Optional, Dict, Any\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from joblib import dump as joblib_dump, load as joblib_load\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score, log_loss\n",
        "from sklearn.datasets import make_moons, make_circles, make_blobs\n",
        "\n",
        "\n",
        "# ------------------------------\n",
        "# Utilities\n",
        "# ------------------------------\n",
        "\n",
        "def log(msg: str) -> None:\n",
        "    print(f\"[{time.strftime('%H:%M:%S')}] {msg}\")\n",
        "\n",
        "def now_ts() -> str:\n",
        "    return time.strftime(\"%Y%m%d-%H%M%S\")\n",
        "\n",
        "def ensure_dir(path: str | Path) -> Path:\n",
        "    p = Path(path)\n",
        "    p.mkdir(parents=True, exist_ok=True)\n",
        "    return p\n",
        "\n",
        "def save_json(obj: Dict[str, Any], path: str | Path) -> None:\n",
        "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(obj, f, indent=2, sort_keys=True)\n",
        "\n",
        "def load_json(path: str | Path) -> Dict[str, Any]:\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "        return json.load(f)\n",
        "\n",
        "def seed_everything(seed: int) -> None:\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True  # type: ignore[attr-defined]\n",
        "    torch.backends.cudnn.benchmark = False     # type: ignore[attr-defined]\n",
        "\n",
        "def sanitize_argv(raw_args: List[str]) -> List[str]:\n",
        "    \"\"\"\n",
        "    Remove notebook-injected args (e.g., '-f', '...kernel-XXXX.json') and any trailing .json.\n",
        "    \"\"\"\n",
        "    filtered = []\n",
        "    skip_next = False\n",
        "    for i, a in enumerate(raw_args):\n",
        "        if skip_next:\n",
        "            skip_next = False\n",
        "            continue\n",
        "        if a == \"-f\":\n",
        "            # Skip '-f' and the following json path (Jupyter/IPython)\n",
        "            skip_next = True\n",
        "            continue\n",
        "        if a.endswith(\".json\"):\n",
        "            # Skip any json path injection\n",
        "            continue\n",
        "        filtered.append(a)\n",
        "    return filtered\n",
        "\n",
        "\n",
        "# ------------------------------\n",
        "# Data\n",
        "# ------------------------------\n",
        "\n",
        "@dataclass\n",
        "class DataConfig:\n",
        "    dataset: str = \"moons\"           # moons|circles|blobs\n",
        "    n_samples: int = 2000\n",
        "    noise: float = 0.2\n",
        "    centers: int = 3                 # for blobs only\n",
        "    cluster_std: float = 1.0         # for blobs only\n",
        "    val_split: float = 0.2\n",
        "    test_split: float = 0.2\n",
        "    seed: int = 42\n",
        "\n",
        "def make_dataset(cfg: DataConfig) -> Tuple[np.ndarray, np.ndarray]:\n",
        "    if cfg.dataset == \"moons\":\n",
        "        X, y = make_moons(n_samples=cfg.n_samples, noise=cfg.noise, random_state=cfg.seed)\n",
        "    elif cfg.dataset == \"circles\":\n",
        "        X, y = make_circles(n_samples=cfg.n_samples, noise=cfg.noise, factor=0.5, random_state=cfg.seed)\n",
        "    elif cfg.dataset == \"blobs\":\n",
        "        X, y = make_blobs(n_samples=cfg.n_samples, centers=cfg.centers, cluster_std=cfg.cluster_std, random_state=cfg.seed)\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown dataset: {cfg.dataset}\")\n",
        "    return X.astype(np.float32), y.astype(np.int64)\n",
        "\n",
        "def stratified_split(X: np.ndarray, y: np.ndarray, val_split: float, test_split: float, seed: int) -> Dict[str, np.ndarray]:\n",
        "    \"\"\"\n",
        "    Returns indices dict: {'train_idx','val_idx','test_idx'} with stratification.\n",
        "    \"\"\"\n",
        "    rng = np.random.RandomState(seed)\n",
        "    classes = np.unique(y)\n",
        "    train_idx, val_idx, test_idx = [], [], []\n",
        "    for c in classes:\n",
        "        idx = np.where(y == c)[0]\n",
        "        rng.shuffle(idx)\n",
        "        n = len(idx)\n",
        "        n_test = int(round(n * test_split))\n",
        "        n_val = int(round(n * val_split))\n",
        "        n_train = n - n_test - n_val\n",
        "        # Guarantee minimum presence\n",
        "        n_train = max(n_train, 1)\n",
        "        rest = n - n_train\n",
        "        n_val = min(n_val, rest)\n",
        "        n_test = rest - n_val\n",
        "        t_idx = idx[:n_train]\n",
        "        v_idx = idx[n_train:n_train + n_val]\n",
        "        te_idx = idx[n_train + n_val:]\n",
        "        train_idx.extend(t_idx.tolist())\n",
        "        val_idx.extend(v_idx.tolist())\n",
        "        test_idx.extend(te_idx.tolist())\n",
        "    rng.shuffle(train_idx)\n",
        "    rng.shuffle(val_idx)\n",
        "    rng.shuffle(test_idx)\n",
        "    return {\n",
        "        \"train_idx\": np.array(train_idx, dtype=np.int64),\n",
        "        \"val_idx\": np.array(val_idx, dtype=np.int64),\n",
        "        \"test_idx\": np.array(test_idx, dtype=np.int64),\n",
        "    }\n",
        "\n",
        "\n",
        "# ------------------------------\n",
        "# Model\n",
        "# ------------------------------\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, in_dim: int, hidden: List[int], out_dim: int):\n",
        "        super().__init__()\n",
        "        layers: List[nn.Module] = []\n",
        "        prev = in_dim\n",
        "        for h in hidden:\n",
        "            layers += [nn.Linear(prev, h), nn.ReLU()]\n",
        "            prev = h\n",
        "        layers.append(nn.Linear(prev, out_dim))\n",
        "        self.net = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        return self.net(x)\n",
        "\n",
        "\n",
        "# ------------------------------\n",
        "# Training / Evaluation\n",
        "# ------------------------------\n",
        "\n",
        "@dataclass\n",
        "class TrainConfig:\n",
        "    run_dir: str = \"runs/run\"\n",
        "    hidden_sizes: List[int] = None  # type: ignore[assignment]\n",
        "    lr: float = 1e-3\n",
        "    weight_decay: float = 0.0\n",
        "    epochs: int = 50\n",
        "    batch_size: int = 64\n",
        "    device: str = \"cpu\"\n",
        "\n",
        "    def __post_init__(self):\n",
        "        if self.hidden_sizes is None:\n",
        "            self.hidden_sizes = [64, 64]\n",
        "\n",
        "def build_loaders(X: np.ndarray, y: np.ndarray, idx: Dict[str, np.ndarray], batch_size: int) -> Tuple[DataLoader, DataLoader, DataLoader]:\n",
        "    def make_dl(idxs):\n",
        "        ds = TensorDataset(torch.from_numpy(X[idxs]), torch.from_numpy(y[idxs]))\n",
        "        return DataLoader(ds, batch_size=batch_size, shuffle=True)\n",
        "    return make_dl(idx[\"train_idx\"]), make_dl(idx[\"val_idx\"]), make_dl(idx[\"test_idx\"])\n",
        "\n",
        "def train_model(data_cfg: DataConfig, train_cfg: TrainConfig) -> Dict[str, Any]:\n",
        "    seed_everything(data_cfg.seed)\n",
        "\n",
        "    # Prepare run dir\n",
        "    rd = ensure_dir(train_cfg.run_dir)\n",
        "    (rd / \"artifacts\").mkdir(exist_ok=True)\n",
        "\n",
        "    # Data\n",
        "    X, y = make_dataset(data_cfg)\n",
        "    idx = stratified_split(X, y, data_cfg.val_split, data_cfg.test_split, data_cfg.seed)\n",
        "\n",
        "    # Scaler (fit on train only)\n",
        "    scaler = StandardScaler()\n",
        "    X_train = scaler.fit_transform(X[idx[\"train_idx\"]])\n",
        "    X_val = scaler.transform(X[idx[\"val_idx\"]])\n",
        "    X_test = scaler.transform(X[idx[\"test_idx\"]])\n",
        "\n",
        "    X_scaled = np.zeros_like(X)\n",
        "    X_scaled[idx[\"train_idx\"]] = X_train\n",
        "    X_scaled[idx[\"val_idx\"]] = X_val\n",
        "    X_scaled[idx[\"test_idx\"]] = X_test\n",
        "\n",
        "    # Save artifacts for reproducibility\n",
        "    cfg = {\n",
        "        \"data\": asdict(data_cfg),\n",
        "        \"train\": {\n",
        "            **asdict(train_cfg),\n",
        "            \"hidden_sizes\": train_cfg.hidden_sizes,\n",
        "            \"device\": train_cfg.device,\n",
        "        },\n",
        "    }\n",
        "    save_json(cfg, rd / \"config.json\")\n",
        "    save_json({k: v.tolist() for k, v in idx.items()}, rd / \"indices.json\")\n",
        "    joblib_dump(scaler, rd / \"scaler.pkl\")\n",
        "\n",
        "    # Model\n",
        "    in_dim = X.shape[1]\n",
        "    out_dim = len(np.unique(y))\n",
        "    model = MLP(in_dim, train_cfg.hidden_sizes, out_dim).to(train_cfg.device)\n",
        "    opt = torch.optim.Adam(model.parameters(), lr=train_cfg.lr, weight_decay=train_cfg.weight_decay)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    train_loader, val_loader, test_loader = build_loaders(X_scaled, y, idx, train_cfg.batch_size)\n",
        "\n",
        "    best_val = -math.inf\n",
        "    history = []\n",
        "    best_path = rd / \"best.pt\"\n",
        "\n",
        "    for epoch in range(1, train_cfg.epochs + 1):\n",
        "        model.train()\n",
        "        train_losses = []\n",
        "        train_y_true, train_y_pred = [], []\n",
        "\n",
        "        for xb, yb in train_loader:\n",
        "            xb = xb.to(train_cfg.device)\n",
        "            yb = yb.to(train_cfg.device)\n",
        "            opt.zero_grad()\n",
        "            logits = model(xb)\n",
        "            loss = criterion(logits, yb)\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "            train_losses.append(loss.item())\n",
        "            pred = logits.argmax(dim=1).detach().cpu().numpy()\n",
        "            train_y_true.extend(yb.detach().cpu().numpy())\n",
        "            train_y_pred.extend(pred)\n",
        "\n",
        "        train_acc = accuracy_score(train_y_true, train_y_pred)\n",
        "        train_loss = float(np.mean(train_losses))\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        val_y_true, val_y_pred, val_losses = [], [], []\n",
        "        with torch.no_grad():\n",
        "            for xb, yb in val_loader:\n",
        "                xb = xb.to(train_cfg.device)\n",
        "                yb = yb.to(train_cfg.device)\n",
        "                logits = model(xb)\n",
        "                loss = criterion(logits, yb)\n",
        "                val_losses.append(loss.item())\n",
        "                pred = logits.argmax(dim=1).detach().cpu().numpy()\n",
        "                val_y_true.extend(yb.detach().cpu().numpy())\n",
        "                val_y_pred.extend(pred)\n",
        "\n",
        "        val_acc = accuracy_score(val_y_true, val_y_pred)\n",
        "        val_loss = float(np.mean(val_losses))\n",
        "\n",
        "        history.append({\n",
        "            \"epoch\": epoch,\n",
        "            \"train_loss\": train_loss,\n",
        "            \"train_acc\": train_acc,\n",
        "            \"val_loss\": val_loss,\n",
        "            \"val_acc\": val_acc,\n",
        "        })\n",
        "\n",
        "        log(f\"Epoch {epoch:03d} | train_acc={train_acc:.3f} val_acc={val_acc:.3f} train_loss={train_loss:.4f} val_loss={val_loss:.4f}\")\n",
        "\n",
        "        # Track best by val_acc\n",
        "        if val_acc > best_val:\n",
        "            best_val = val_acc\n",
        "            torch.save({\"model_state\": model.state_dict(), \"in_dim\": in_dim, \"out_dim\": out_dim, \"hidden\": train_cfg.hidden_sizes}, best_path)\n",
        "\n",
        "    # Save history\n",
        "    pd.DataFrame(history).to_csv(rd / \"history.csv\", index=False)\n",
        "    metrics = {\"best_val_acc\": best_val}\n",
        "    save_json(metrics, rd / \"metrics.json\")\n",
        "\n",
        "    # Also evaluate on test split immediately (optional)\n",
        "    test_metrics = evaluate_model(rd)\n",
        "    save_json({\"best_val_acc\": best_val, **test_metrics}, rd / \"metrics_final.json\")\n",
        "\n",
        "    return {\"run_dir\": str(rd), \"best_val_acc\": best_val, **test_metrics}\n",
        "\n",
        "\n",
        "def evaluate_model(run_dir: str | Path) -> Dict[str, Any]:\n",
        "    rd = Path(run_dir)\n",
        "    cfg = load_json(rd / \"config.json\")\n",
        "    idx = {k: np.array(v, dtype=np.int64) for k, v in load_json(rd / \"indices.json\").items()}\n",
        "    scaler: StandardScaler = joblib_load(rd / \"scaler.pkl\")\n",
        "    checkpoint = torch.load(rd / \"best.pt\", map_location=\"cpu\")\n",
        "\n",
        "    data_cfg = DataConfig(**cfg[\"data\"])\n",
        "    X, y = make_dataset(data_cfg)\n",
        "\n",
        "    X_test = scaler.transform(X[idx[\"test_idx\"]])\n",
        "    y_test = y[idx[\"test_idx\"]]\n",
        "\n",
        "    model = MLP(checkpoint[\"in_dim\"], checkpoint[\"hidden\"], checkpoint[\"out_dim\"])\n",
        "    model.load_state_dict(checkpoint[\"model_state\"])\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        logits = model(torch.from_numpy(X_test))\n",
        "        probs = torch.softmax(logits, dim=1).numpy()\n",
        "        preds = probs.argmax(axis=1)\n",
        "\n",
        "    acc = float(accuracy_score(y_test, preds))\n",
        "    ll = float(log_loss(y_test, probs, labels=list(range(checkpoint[\"out_dim\"]))))\n",
        "\n",
        "    results = {\"test_acc\": acc, \"test_log_loss\": ll, \"n_test\": int(len(y_test))}\n",
        "    log(f\"Eval | test_acc={acc:.3f} test_log_loss={ll:.4f} n_test={len(y_test)}\")\n",
        "    save_json(results, rd / \"eval.json\")\n",
        "    return results\n",
        "\n",
        "\n",
        "def predict_csv(run_dir: str | Path, csv_path: str | Path, output: Optional[str] = None,\n",
        "                features: Optional[List[str]] = None, include_proba: bool = True) -> str:\n",
        "    rd = Path(run_dir)\n",
        "    scaler: StandardScaler = joblib_load(rd / \"scaler.pkl\")\n",
        "    checkpoint = torch.load(rd / \"best.pt\", map_location=\"cpu\")\n",
        "\n",
        "    model = MLP(checkpoint[\"in_dim\"], checkpoint[\"hidden\"], checkpoint[\"out_dim\"])\n",
        "    model.load_state_dict(checkpoint[\"model_state\"])\n",
        "    model.eval()\n",
        "\n",
        "    df = pd.read_csv(csv_path)\n",
        "    if features is None:\n",
        "        # Auto-select numeric columns, excluding a column named 'label' if present\n",
        "        num_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
        "        features = [c for c in num_cols if c.lower() != \"label\"]\n",
        "    X = df[features].to_numpy(dtype=np.float32)\n",
        "\n",
        "    # Expect same dimensionality as training\n",
        "    if X.shape[1] != checkpoint[\"in_dim\"]:\n",
        "        raise ValueError(f\"Feature dimension mismatch: model expects {checkpoint['in_dim']} but CSV has {X.shape[1]}\")\n",
        "\n",
        "    Xs = scaler.transform(X)\n",
        "    with torch.no_grad():\n",
        "        logits = model(torch.from_numpy(Xs))\n",
        "        probs = torch.softmax(logits, dim=1).numpy()\n",
        "        preds = probs.argmax(axis=1)\n",
        "\n",
        "    out_df = df.copy()\n",
        "    out_df[\"prediction\"] = preds\n",
        "    if include_proba:\n",
        "        for k in range(probs.shape[1]):\n",
        "            out_df[f\"proba_{k}\"] = probs[:, k]\n",
        "\n",
        "    out_path = output or (str(Path(csv_path).with_suffix(\"\")) + \"_pred.csv\")\n",
        "    out_df.to_csv(out_path, index=False)\n",
        "    log(f\"Wrote predictions to: {out_path}\")\n",
        "    return out_path\n",
        "\n",
        "\n",
        "def export_onnx(run_dir: str | Path, out_path: Optional[str] = None) -> str:\n",
        "    rd = Path(run_dir)\n",
        "    checkpoint = torch.load(rd / \"best.pt\", map_location=\"cpu\")\n",
        "    model = MLP(checkpoint[\"in_dim\"], checkpoint[\"hidden\"], checkpoint[\"out_dim\"])\n",
        "    model.load_state_dict(checkpoint[\"model_state\"])\n",
        "    model.eval()\n",
        "\n",
        "    dummy = torch.randn(1, checkpoint[\"in_dim\"], dtype=torch.float32)\n",
        "    onnx_path = out_path or str(rd / \"model.onnx\")\n",
        "    torch.onnx.export(\n",
        "        model,\n",
        "        dummy,\n",
        "        onnx_path,\n",
        "        input_names=[\"input\"],\n",
        "        output_names=[\"logits\"],\n",
        "        dynamic_axes={\"input\": {0: \"batch\"}, \"logits\": {0: \"batch\"}},\n",
        "        opset_version=13,\n",
        "    )\n",
        "    # Save scaler params for reference\n",
        "    scaler: StandardScaler = joblib_load(rd / \"scaler.pkl\")\n",
        "    save_json({\"mean\": scaler.mean_.tolist(), \"scale\": scaler.scale_.tolist()}, rd / \"scaler_params.json\")\n",
        "    log(f\"Exported ONNX model to: {onnx_path}\")\n",
        "    return onnx_path\n",
        "\n",
        "\n",
        "def benchmark(data_cfg: DataConfig, train_cfg: TrainConfig, seeds: List[int]) -> Dict[str, Any]:\n",
        "    results = []\n",
        "    base = Path(train_cfg.run_dir)\n",
        "    base.mkdir(parents=True, exist_ok=True)\n",
        "    for s in seeds:\n",
        "        run_subdir = base / f\"{data_cfg.dataset}_seed{s}_{now_ts()}\"\n",
        "        dc = DataConfig(**{**asdict(data_cfg), \"seed\": s})\n",
        "        tc = TrainConfig(**{**asdict(train_cfg), \"run_dir\": str(run_subdir)})\n",
        "        log(f\"Benchmark seed={s}\")\n",
        "        out = train_model(dc, tc)\n",
        "        results.append({\"seed\": s, **out})\n",
        "\n",
        "    # Aggregate\n",
        "    accs = [r[\"test_acc\"] for r in results if \"test_acc\" in r]\n",
        "    mean_acc = float(np.mean(accs)) if accs else float(\"nan\")\n",
        "    std_acc = float(np.std(accs)) if accs else float(\"nan\")\n",
        "    agg = {\"mean_test_acc\": mean_acc, \"std_test_acc\": std_acc, \"runs\": results}\n",
        "    save_json(agg, base / \"benchmark_results.json\")\n",
        "    log(f\"Benchmark | mean_test_acc={mean_acc:.3f} std={std_acc:.3f} over {len(accs)} runs\")\n",
        "    return agg\n",
        "\n",
        "\n",
        "# ------------------------------\n",
        "# CLI\n",
        "# ------------------------------\n",
        "\n",
        "def build_parser() -> argparse.ArgumentParser:\n",
        "    parser = argparse.ArgumentParser(\n",
        "        prog=\"metaintel\",\n",
        "        description=\"MetaIntelligence CLI: train/eval/predict/benchmark/export-onnx\",\n",
        "        formatter_class=argparse.ArgumentDefaultsHelpFormatter,\n",
        "    )\n",
        "    subparsers = parser.add_subparsers(dest=\"command\")  # don't require to keep notebook-friendly\n",
        "\n",
        "    # train\n",
        "    p_train = subparsers.add_parser(\"train\", help=\"Train a model\", formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n",
        "    p_train.add_argument(\"--dataset\", choices=[\"moons\", \"circles\", \"blobs\"], default=\"moons\")\n",
        "    p_train.add_argument(\"--n_samples\", type=int, default=2000)\n",
        "    p_train.add_argument(\"--noise\", type=float, default=0.2)\n",
        "    p_train.add_argument(\"--centers\", type=int, default=3, help=\"for blobs\")\n",
        "    p_train.add_argument(\"--cluster_std\", type=float, default=1.0, help=\"for blobs\")\n",
        "    p_train.add_argument(\"--val_split\", type=float, default=0.2)\n",
        "    p_train.add_argument(\"--test_split\", type=float, default=0.2)\n",
        "    p_train.add_argument(\"--seed\", type=int, default=42)\n",
        "\n",
        "    p_train.add_argument(\"--hidden_sizes\", type=str, default=\"64,64\", help=\"Comma-separated hidden sizes\")\n",
        "    p_train.add_argument(\"--lr\", type=float, default=1e-3)\n",
        "    p_train.add_argument(\"--weight_decay\", type=float, default=0.0)\n",
        "    p_train.add_argument(\"--epochs\", type=int, default=50)\n",
        "    p_train.add_argument(\"--batch_size\", type=int, default=64)\n",
        "    p_train.add_argument(\"--device\", type=str, default=\"cpu\")\n",
        "    p_train.add_argument(\"--run_dir\", type=str, default=f\"runs/{now_ts()}\")\n",
        "\n",
        "    # eval\n",
        "    p_eval = subparsers.add_parser(\"eval\", help=\"Evaluate best checkpoint on test set\", formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n",
        "    p_eval.add_argument(\"--run_dir\", type=str, required=True)\n",
        "\n",
        "    # predict\n",
        "    p_pred = subparsers.add_parser(\"predict\", help=\"Predict on CSV (tabular only)\", formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n",
        "    p_pred.add_argument(\"--run_dir\", type=str, required=True)\n",
        "    p_pred.add_argument(\"--csv\", type=str, required=True)\n",
        "    p_pred.add_argument(\"--output\", type=str, default=None, help=\"Output CSV path\")\n",
        "    p_pred.add_argument(\"--features\", type=str, default=None, help=\"Comma-separated feature columns (default: auto numeric cols)\")\n",
        "    p_pred.add_argument(\"--no-proba\", action=\"store_true\", help=\"Do not include probability columns\")\n",
        "\n",
        "    # export-onnx\n",
        "    p_onnx = subparsers.add_parser(\"export-onnx\", help=\"Export best checkpoint to ONNX\", formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n",
        "    p_onnx.add_argument(\"--run_dir\", type=str, required=True)\n",
        "    p_onnx.add_argument(\"--output\", type=str, default=None)\n",
        "\n",
        "    # benchmark\n",
        "    p_bench = subparsers.add_parser(\"benchmark\", help=\"Benchmark multiple seeds\", formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n",
        "    p_bench.add_argument(\"--dataset\", choices=[\"moons\", \"circles\", \"blobs\"], default=\"moons\")\n",
        "    p_bench.add_argument(\"--n_samples\", type=int, default=2000)\n",
        "    p_bench.add_argument(\"--noise\", type=float, default=0.2)\n",
        "    p_bench.add_argument(\"--centers\", type=int, default=3)\n",
        "    p_bench.add_argument(\"--cluster_std\", type=float, default=1.0)\n",
        "    p_bench.add_argument(\"--val_split\", type=float, default=0.2)\n",
        "    p_bench.add_argument(\"--test_split\", type=float, default=0.2)\n",
        "    p_bench.add_argument(\"--hidden_sizes\", type=str, default=\"64,64\")\n",
        "    p_bench.add_argument(\"--lr\", type=float, default=1e-3)\n",
        "    p_bench.add_argument(\"--weight_decay\", type=float, default=0.0)\n",
        "    p_bench.add_argument(\"--epochs\", type=int, default=50)\n",
        "    p_bench.add_argument(\"--batch_size\", type=int, default=64)\n",
        "    p_bench.add_argument(\"--device\", type=str, default=\"cpu\")\n",
        "    p_bench.add_argument(\"--run_dir\", type=str, default=f\"runs/bench_{now_ts()}\")\n",
        "    p_bench.add_argument(\"--seeds\", type=str, default=\"1,2,3,4,5\", help=\"Comma-separated seeds\")\n",
        "\n",
        "    return parser\n",
        "\n",
        "\n",
        "# ------------------------------\n",
        "# Commands\n",
        "# ------------------------------\n",
        "\n",
        "def cmd_train(args: argparse.Namespace) -> None:\n",
        "    data_cfg = DataConfig(\n",
        "        dataset=args.dataset,\n",
        "        n_samples=args.nsamples,\n",
        "        noise=args.noise,\n",
        "        centers=args.centers,\n",
        "        cluster_std=args.clusterstd,\n",
        "        val_split=args.valsplit,\n",
        "        test_split=args.testsplit,\n",
        "        seed=args.seed,\n",
        "    )\n",
        "    hidden = [int(x) for x in args.hidden_sizes.split(\",\") if x.strip()]\n",
        "    train_cfg = TrainConfig(\n",
        "        run_dir=args.rundir,\n",
        "        hidden_sizes=hidden,\n",
        "        lr=args.lr,\n",
        "        weight_decay=args.weightdecay,\n",
        "        epochs=args.epochs,\n",
        "        batch_size=args.batchsize,\n",
        "        device=args.device,\n",
        "    )\n",
        "    log(\"Starting training...\")\n",
        "    out = train_model(data_cfg, train_cfg)\n",
        "    log(f\"Done. Run dir: {out['run_dir']}\")\n",
        "\n",
        "\n",
        "def cmd_eval(args: argparse.Namespace) -> None:\n",
        "    log(\"Evaluating best checkpoint...\")\n",
        "    evaluate_model(args.rundir)\n",
        "\n",
        "\n",
        "def cmd_predict(args: argparse.Namespace) -> None:\n",
        "    feats = [f.strip() for f in args.features.split(\",\") if f.strip()] if args.features else None\n",
        "    predict_csv(args.rundir, args.csv, output=args.output, features=feats, include_proba=not args.noproba)\n",
        "\n",
        "\n",
        "def cmd_export_onnx(args: argparse.Namespace) -> None:\n",
        "    export_onnx(args.rundir, args.output)\n",
        "\n",
        "\n",
        "def cmd_benchmark(args: argparse.Namespace) -> None:\n",
        "    data_cfg = DataConfig(\n",
        "        dataset=args.dataset,\n",
        "        n_samples=args.nsamples,\n",
        "        noise=args.noise,\n",
        "        centers=args.centers,\n",
        "        cluster_std=args.clusterstd,\n",
        "        val_split=args.valsplit,\n",
        "        test_split=args.testsplit,\n",
        "        seed=0,\n",
        "    )\n",
        "    hidden = [int(x) for x in args.hidden_sizes.split(\",\") if x.strip()]\n",
        "    train_cfg = TrainConfig(\n",
        "        run_dir=args.rundir,\n",
        "        hidden_sizes=hidden,\n",
        "        lr=args.lr,\n",
        "        weight_decay=args.weightdecay,\n",
        "        epochs=args.epochs,\n",
        "        batch_size=args.batchsize,\n",
        "        device=args.device,\n",
        "    )\n",
        "    seeds = [int(s) for s in args.seeds.split(\",\") if s.strip()]\n",
        "    benchmark(data_cfg, train_cfg, seeds)\n",
        "\n",
        "\n",
        "# ------------------------------\n",
        "# Main (Notebook/Colab-safe)\n",
        "# ------------------------------\n",
        "\n",
        "def main(argv: Optional[List[str]] = None):\n",
        "    import sys\n",
        "\n",
        "    raw_args = argv if argv is not None else sys.argv[1:]\n",
        "    filtered_args = sanitize_argv(raw_args)\n",
        "\n",
        "    parser = build_parser()\n",
        "\n",
        "    # Early exit in notebooks/empty arg cases\n",
        "    if not filtered_args:\n",
        "        parser.print_help()\n",
        "        print(\"\\nðŸ“Œ Example usage (in notebooks):\")\n",
        "        print('main([\"train\", \"--dataset\", \"moons\", \"--epochs\", \"30\", \"--run_dir\", \"runs/moonstest\"])')\n",
        "        print('main([\"eval\", \"--run_dir\", \"runs/moonstest\"])')\n",
        "        print('main([\"export-onnx\", \"--run_dir\", \"runs/moonstest\"])')\n",
        "        return\n",
        "\n",
        "    try:\n",
        "        args = parser.parse_args(filtered_args)\n",
        "    except SystemExit:\n",
        "        parser.print_help()\n",
        "        print(\"\\nðŸ“Œ Example usage (in notebooks):\")\n",
        "        print('main([\"train\", \"--dataset\", \"moons\", \"--epochs\", \"30\", \"--run_dir\", \"runs/moonstest\"])')\n",
        "        print('main([\"eval\", \"--run_dir\", \"runs/moonstest\"])')\n",
        "        print('main([\"export-onnx\", \"--run_dir\", \"runs/moonstest\"])')\n",
        "        return\n",
        "\n",
        "    cmd = getattr(args, \"command\", None)\n",
        "    if cmd == \"train\":\n",
        "        cmd_train(args)\n",
        "    elif cmd == \"eval\":\n",
        "        cmd_eval(args)\n",
        "    elif cmd == \"predict\":\n",
        "        cmd_predict(args)\n",
        "    elif cmd == \"export-onnx\":\n",
        "        cmd_export_onnx(args)\n",
        "    elif cmd == \"benchmark\":\n",
        "        cmd_benchmark(args)\n",
        "    else:\n",
        "        parser.print_help()\n",
        "        print(\"\\nðŸ“Œ Example usage (in notebooks):\")\n",
        "        print('main([\"train\", \"--dataset\", \"moons\", \"--epochs\", \"30\", \"--run_dir\", \"runs/moonstest\"])')\n",
        "        print('main([\"eval\", \"--run_dir\", \"runs/moonstest\"])')\n",
        "        print('main([\"export-onnx\", \"--run_dir\", \"runs/moonstest\"])')\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ]
}