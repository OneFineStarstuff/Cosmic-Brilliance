{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyPer7j+q/OJ+OefFt7O7Uet",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OneFineStarstuff/Cosmic-Brilliance/blob/main/train_omniversal_ai_py.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5ir3ciFYaVLp"
      },
      "outputs": [],
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "train_omniversal_ai.py\n",
        "\n",
        "End-to-end pipeline for OmniversalAI:\n",
        "1. Synthetic “omniversal” dataset of 6 inputs → 3 outputs\n",
        "2. Standardization, noise injection\n",
        "3. MLP with ReLU & optional Dropout (accepts int hidden_dims)\n",
        "4. Physics-informed residual enforcing toy “omniverse” laws\n",
        "5. MC-Dropout for uncertainty quantification\n",
        "6. Training loop with AdamW, ReduceLROnPlateau, gradient clipping, NaN checks, early stopping\n",
        "7. Safe checkpoint loading\n",
        "8. Visualizations: loss curves, true vs. predicted scatter, uncertainty heatmap\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 1. Synthetic Omniversal Dataset\n",
        "# ------------------------------------------------------------------------------\n",
        "class OmniversalDataset(Dataset):\n",
        "    def __init__(self, n_samples=6000, seed=0):\n",
        "        np.random.seed(seed)\n",
        "        # Features: Reality coherence (RC), cross-dimensional entropy (CDE),\n",
        "        # omniversal stability (OS), transdimensional flux (TF),\n",
        "        # quantum synchrony (QS), causal coupling (CC)\n",
        "        RC  = np.random.uniform(0.1, 1.0, (n_samples,1))\n",
        "        CDE = np.random.uniform(0.0, 5.0, (n_samples,1))\n",
        "        OS  = np.random.uniform(0.5, 2.0, (n_samples,1))\n",
        "        TF  = np.random.uniform(0.0, 1.0, (n_samples,1))\n",
        "        QS  = np.random.uniform(0.0, 1.0, (n_samples,1))\n",
        "        CC  = np.random.uniform(0.1, 1.0, (n_samples,1))\n",
        "\n",
        "        X_raw = np.hstack([RC, CDE, OS, TF, QS, CC]).astype(np.float64)\n",
        "\n",
        "        # Toy “omniversal laws” targets:\n",
        "        eps = 1e-6\n",
        "        # 1. intelligence_alignment = RC * QS / (CDE + eps)\n",
        "        IA = RC * QS / (CDE + eps)\n",
        "        # 2. causality_integrity = OS * CC / (TF + eps)\n",
        "        CI = OS * CC / (TF + eps)\n",
        "        # 3. entropy_balance = CDE * CC * OS\n",
        "        EB = CDE * CC * OS\n",
        "\n",
        "        Y_raw = np.hstack([IA, CI, EB]).astype(np.float64)\n",
        "        # 1% relative noise\n",
        "        Y_raw += 0.01 * Y_raw.std(axis=0) * np.random.randn(*Y_raw.shape)\n",
        "\n",
        "        # Compute normalization stats\n",
        "        self.X_mean = X_raw.mean(axis=0)\n",
        "        self.X_std  = X_raw.std(axis=0) + 1e-8\n",
        "        self.Y_mean = Y_raw.mean(axis=0)\n",
        "        self.Y_std  = Y_raw.std(axis=0) + 1e-8\n",
        "\n",
        "        # Standardize\n",
        "        self.X = ((X_raw - self.X_mean) / self.X_std).astype(np.float32)\n",
        "        self.Y = ((Y_raw - self.Y_mean) / self.Y_std).astype(np.float32)\n",
        "\n",
        "        # Debug range prints\n",
        "        print(f\"X range: {self.X.min():.3e}–{self.X.max():.3e}\")\n",
        "        print(f\"Y range: {self.Y.min():.3e}–{self.Y.max():.3e}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return torch.from_numpy(self.X[idx]), torch.from_numpy(self.Y[idx])\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 2. Model Definition (accept int hidden_dims)\n",
        "# ------------------------------------------------------------------------------\n",
        "class OmniversalAI(nn.Module):\n",
        "    def __init__(self, input_dim=6, hidden_dims=(64,64), output_dim=3, p_drop=0.1):\n",
        "        super().__init__()\n",
        "        if isinstance(hidden_dims, int):\n",
        "            hidden_dims = (hidden_dims,)\n",
        "        layers, dim = [], input_dim\n",
        "        for h in hidden_dims:\n",
        "            layers += [\n",
        "                nn.Linear(dim, h),\n",
        "                nn.ReLU(),\n",
        "                nn.Dropout(p_drop)\n",
        "            ]\n",
        "            dim = h\n",
        "        layers.append(nn.Linear(dim, output_dim))\n",
        "        self.net = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 3. Physics-Informed Residual & Loss\n",
        "# ------------------------------------------------------------------------------\n",
        "def physics_residual(pred, X, stats):\n",
        "    X_den = X * stats['X_std'] + stats['X_mean']\n",
        "    RC, CDE, OS, TF, QS, CC = X_den.t()\n",
        "    eps = 1e-6\n",
        "\n",
        "    IA_t = RC * QS / (CDE + eps)\n",
        "    CI_t = OS * CC / (TF + eps)\n",
        "    EB_t = CDE * CC * OS\n",
        "    Yt = torch.stack([IA_t, CI_t, EB_t], dim=1)\n",
        "\n",
        "    Yt_norm = (Yt - stats['Y_mean']) / stats['Y_std']\n",
        "    return nn.MSELoss()(pred, Yt_norm)\n",
        "\n",
        "def total_loss(pred, true, X, stats, lam=1.0):\n",
        "    mse  = nn.MSELoss()(pred, true)\n",
        "    phys = physics_residual(pred, X, stats)\n",
        "    return mse + lam * phys, mse, phys\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 4. MC-Dropout for Uncertainty Quantification\n",
        "# ------------------------------------------------------------------------------\n",
        "def mc_dropout_predict(model, X, T=50):\n",
        "    model.train()\n",
        "    samples = []\n",
        "    with torch.no_grad():\n",
        "        for _ in range(T):\n",
        "            samples.append(model(X))\n",
        "    stacked = torch.stack(samples, dim=0)\n",
        "    return stacked.mean(0), stacked.std(0)\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 5. Training Loop with NaN Protection & Safe Checkpoint\n",
        "# ------------------------------------------------------------------------------\n",
        "def train(model, tr_loader, va_loader, stats, device,\n",
        "          lr=1e-4, wd=1e-5, lam=1.0, epochs=100, patience=10):\n",
        "    model.to(device)\n",
        "    opt = optim.AdamW(model.parameters(), lr=lr, weight_decay=wd)\n",
        "    sched = optim.lr_scheduler.ReduceLROnPlateau(opt, mode='min', factor=0.5, patience=5)\n",
        "\n",
        "    best_val, wait = float('inf'), 0\n",
        "    history = {'train': [], 'val': []}\n",
        "\n",
        "    for ep in range(1, epochs+1):\n",
        "        # Training\n",
        "        model.train()\n",
        "        run = 0.0\n",
        "        for xb, yb in tr_loader:\n",
        "            xb, yb = xb.to(device), yb.to(device)\n",
        "            pred = model(xb)\n",
        "            loss, mse, phys = total_loss(pred, yb, xb, stats, lam)\n",
        "\n",
        "            if torch.isnan(loss):\n",
        "                print(f\"NaN loss at epoch {ep}, aborting.\")\n",
        "                return history\n",
        "\n",
        "            opt.zero_grad()\n",
        "            loss.backward()\n",
        "            nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            opt.step()\n",
        "            run += loss.item() * xb.size(0)\n",
        "        tr_loss = run / len(tr_loader.dataset)\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        run = 0.0\n",
        "        with torch.no_grad():\n",
        "            for xb, yb in va_loader:\n",
        "                xb, yb = xb.to(device), yb.to(device)\n",
        "                pred = model(xb)\n",
        "                loss, _, _ = total_loss(pred, yb, xb, stats, lam)\n",
        "                run += loss.item() * xb.size(0)\n",
        "        va_loss = run / len(va_loader.dataset)\n",
        "\n",
        "        sched.step(va_loss)\n",
        "        history['train'].append(tr_loss)\n",
        "        history['val'].append(va_loss)\n",
        "        print(f\"Epoch {ep:03d} ┃ Train {tr_loss:.4e} ┃ Val {va_loss:.4e}\")\n",
        "\n",
        "        # Checkpoint\n",
        "        if va_loss < best_val - 1e-8:\n",
        "            best_val, wait = va_loss, 0\n",
        "            torch.save(model.state_dict(), \"best_omniversal_ai.pth\")\n",
        "        else:\n",
        "            wait += 1\n",
        "            if wait >= patience:\n",
        "                print(\"Early stopping.\")\n",
        "                break\n",
        "\n",
        "    # Safe load\n",
        "    if os.path.exists(\"best_omniversal_ai.pth\"):\n",
        "        model.load_state_dict(torch.load(\"best_omniversal_ai.pth\", map_location=device))\n",
        "    else:\n",
        "        print(\"No best checkpoint found; using last epoch.\")\n",
        "\n",
        "    return history\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 6. Visualization Helpers\n",
        "# ------------------------------------------------------------------------------\n",
        "def plot_losses(hist):\n",
        "    plt.figure()\n",
        "    plt.plot(hist['train'], label='Train')\n",
        "    plt.plot(hist['val'],   label='Val')\n",
        "    plt.xlabel(\"Epoch\"); plt.ylabel(\"Loss\"); plt.legend(); plt.show()\n",
        "\n",
        "def plot_scatter(y_true, y_pred, name):\n",
        "    plt.figure()\n",
        "    plt.scatter(y_true, y_pred, s=5, alpha=0.6)\n",
        "    m, M = y_true.min(), y_true.max()\n",
        "    plt.plot([m, M],[m, M], 'r--')\n",
        "    plt.title(name); plt.xlabel(\"True\"); plt.ylabel(\"Predicted\"); plt.show()\n",
        "\n",
        "def plot_uncertainty_heatmap(model, stats, device):\n",
        "    # Vary CDE vs. OS; others at mean\n",
        "    grid = 100\n",
        "    CDE = np.linspace(0,5,grid, dtype=np.float32)\n",
        "    OS  = np.linspace(0.5,2,grid, dtype=np.float32)\n",
        "    G1, G2 = np.meshgrid(CDE, OS)\n",
        "    pts = G1.size\n",
        "\n",
        "    Xg = torch.zeros((pts,6), device=device)\n",
        "    # RC, TF, QS, CC = means\n",
        "    for i in (0,3,4,5):\n",
        "        Xg[:,i] = stats['X_mean'][i]\n",
        "    Xg[:,1] = torch.from_numpy(G1.ravel()).to(device)\n",
        "    Xg[:,2] = torch.from_numpy(G2.ravel()).to(device)\n",
        "\n",
        "    Xn = (Xg - stats['X_mean']) / stats['X_std']\n",
        "    _, std = mc_dropout_predict(model, Xn, T=100)\n",
        "    U = std[:,0].cpu().reshape(G1.shape)\n",
        "\n",
        "    plt.figure(figsize=(5,4))\n",
        "    plt.pcolormesh(G1, G2, U, cmap='magma', shading='auto')\n",
        "    plt.colorbar(label='Std(Alignment)')\n",
        "    plt.xlabel(\"Cross-Dimensional Entropy\")\n",
        "    plt.ylabel(\"Omniversal Stability\")\n",
        "    plt.title(\"Uncertainty Heatmap: Intelligence Alignment\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 7. Main Execution\n",
        "# ------------------------------------------------------------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    ds = OmniversalDataset(n_samples=6000, seed=0)\n",
        "    stats = {\n",
        "        'X_mean': torch.tensor(ds.X_mean, dtype=torch.float32, device=device),\n",
        "        'X_std' : torch.tensor(ds.X_std,  dtype=torch.float32, device=device),\n",
        "        'Y_mean': torch.tensor(ds.Y_mean, dtype=torch.float32, device=device),\n",
        "        'Y_std' : torch.tensor(ds.Y_std,  dtype=torch.float32, device=device),\n",
        "    }\n",
        "\n",
        "    n_val = int(0.2 * len(ds))\n",
        "    tr_ds, va_ds = random_split(ds, [len(ds)-n_val, n_val])\n",
        "    tr_ld = DataLoader(tr_ds, batch_size=128, shuffle=True)\n",
        "    va_ld = DataLoader(va_ds, batch_size=256, shuffle=False)\n",
        "\n",
        "    model = OmniversalAI(input_dim=6, hidden_dims=32, output_dim=3, p_drop=0.1).to(device)\n",
        "    history = train(model, tr_ld, va_ld, stats, device,\n",
        "                    lr=1e-4, wd=1e-5, lam=1.0, epochs=100, patience=10)\n",
        "\n",
        "    plot_losses(history)\n",
        "\n",
        "    # Scatter true vs. predicted\n",
        "    X_all = torch.from_numpy(ds.X).to(device)\n",
        "    with torch.no_grad():\n",
        "        Yp_norm = model(X_all).cpu().numpy()\n",
        "    Yp = Yp_norm * ds.Y_std + ds.Y_mean\n",
        "    Yt = ds.Y * ds.Y_std + ds.Y_mean\n",
        "\n",
        "    names = [\"Intelligence Alignment\", \"Causality Integrity\", \"Entropy Balance\"]\n",
        "    for i, nm in enumerate(names):\n",
        "        plot_scatter(Yt[:,i], Yp[:,i], nm)\n",
        "\n",
        "    # Uncertainty heatmap\n",
        "    plot_uncertainty_heatmap(model, stats, device)"
      ]
    }
  ]
}