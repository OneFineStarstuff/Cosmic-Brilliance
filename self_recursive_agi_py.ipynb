{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyMTYy1UdxtYiigWaHhQ+zcc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OneFineStarstuff/Cosmic-Brilliance/blob/main/self_recursive_agi_py.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CAIkVV1W5TB4"
      },
      "outputs": [],
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "self_recursive_agi.py\n",
        "\n",
        "A self-contained PyTorch script demonstrating:\n",
        "- Synthetic regression dataset\n",
        "- Feed-forward AGI model with outer-loop training and inner-loop self-improvement\n",
        "- Gradient clipping, LR scheduling, checkpointing, and GPU support\n",
        "- Progress logging with tqdm\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# 1. Synthetic Regression Dataset\n",
        "# -----------------------------------------------------------------------------\n",
        "class SyntheticRegressionDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Generates random (X, y) pairs for a linear regression task\n",
        "    with additive Gaussian noise.\n",
        "    \"\"\"\n",
        "    def __init__(self, num_samples=2000, input_size=10):\n",
        "        super().__init__()\n",
        "        self.X = torch.randn(num_samples, input_size)\n",
        "        true_w = torch.randn(input_size, 1)\n",
        "        self.y = self.X @ true_w + 0.1 * torch.randn(num_samples, 1)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.X.size(0)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx], self.y[idx]\n",
        "\n",
        "\n",
        "def get_dataloader(batch_size=32, input_size=10, num_samples=2000):\n",
        "    dataset = SyntheticRegressionDataset(num_samples, input_size)\n",
        "    return DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# 2. Model Definition\n",
        "# -----------------------------------------------------------------------------\n",
        "class SelfRecursiveAGI(nn.Module):\n",
        "    \"\"\"\n",
        "    Simple feed-forward net with:\n",
        "      - outer-loop optimizer for standard training\n",
        "      - inner-loop optimizer for self-improvement on the same batch\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                 input_size: int,\n",
        "                 hidden_size: int,\n",
        "                 output_size: int,\n",
        "                 lr_main: float = 1e-3,\n",
        "                 lr_self: float = 1e-3):\n",
        "        super().__init__()\n",
        "        self.hidden = nn.Linear(input_size, hidden_size)\n",
        "        self.output = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "        # Outer‐loop optimizer\n",
        "        self.main_optimizer = optim.Adam(self.parameters(), lr=lr_main)\n",
        "        # Inner‐loop (self‐improve) optimizer\n",
        "        self.self_optimizer = optim.Adam(self.parameters(), lr=lr_self)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        h = torch.relu(self.hidden(x))\n",
        "        return self.output(h)\n",
        "\n",
        "    def self_improvement(self,\n",
        "                         loss_fn,\n",
        "                         x: torch.Tensor,\n",
        "                         y: torch.Tensor,\n",
        "                         steps: int = 5,\n",
        "                         clip_grad_norm: float = 1.0) -> float:\n",
        "        \"\"\"\n",
        "        Performs `steps` mini‐updates on the same batch to\n",
        "        simulate recursive self-improvement.\n",
        "        Returns the last self-improvement loss.\n",
        "        \"\"\"\n",
        "        last_loss = 0.0\n",
        "        for _ in range(steps):\n",
        "            self.self_optimizer.zero_grad()\n",
        "            preds = self.forward(x)\n",
        "            loss = loss_fn(preds, y)\n",
        "            loss.backward()\n",
        "            nn.utils.clip_grad_norm_(self.parameters(), clip_grad_norm)\n",
        "            self.self_optimizer.step()\n",
        "            last_loss = loss.item()\n",
        "        return last_loss\n",
        "\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# 3. Training + Self-Improvement Loop\n",
        "# -----------------------------------------------------------------------------\n",
        "def train_and_self_improve(model: SelfRecursiveAGI,\n",
        "                           dataloader: DataLoader,\n",
        "                           loss_fn,\n",
        "                           device: torch.device = torch.device('cpu'),\n",
        "                           epochs: int = 20,\n",
        "                           self_steps: int = 3,\n",
        "                           clip_grad_norm: float = 1.0,\n",
        "                           scheduler_step: int = 5,\n",
        "                           scheduler_gamma: float = 0.5,\n",
        "                           checkpoint_dir: str = 'checkpoints',\n",
        "                           checkpoint_interval: int = 5):\n",
        "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "    model.to(device)\n",
        "    scheduler = StepLR(model.main_optimizer, step_size=scheduler_step, gamma=scheduler_gamma)\n",
        "\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        model.train()\n",
        "        epoch_loss = 0.0\n",
        "\n",
        "        progress = tqdm(dataloader, desc=f\"[Epoch {epoch}/{epochs}]\")\n",
        "        for X_batch, y_batch in progress:\n",
        "            X_batch = X_batch.to(device)\n",
        "            y_batch = y_batch.to(device)\n",
        "\n",
        "            # Outer-loop update\n",
        "            model.main_optimizer.zero_grad()\n",
        "            preds = model(X_batch)\n",
        "            loss_main = loss_fn(preds, y_batch)\n",
        "            loss_main.backward()\n",
        "            nn.utils.clip_grad_norm_(model.parameters(), clip_grad_norm)\n",
        "            model.main_optimizer.step()\n",
        "\n",
        "            # Inner-loop self-improvement\n",
        "            loss_self = model.self_improvement(\n",
        "                loss_fn,\n",
        "                X_batch,\n",
        "                y_batch,\n",
        "                steps=self_steps,\n",
        "                clip_grad_norm=clip_grad_norm\n",
        "            )\n",
        "\n",
        "            epoch_loss += loss_main.item()\n",
        "            progress.set_postfix({\n",
        "                'loss_main': f\"{loss_main.item():.4f}\",\n",
        "                'loss_self': f\"{loss_self:.4f}\"\n",
        "            })\n",
        "\n",
        "        scheduler.step()\n",
        "        avg_loss = epoch_loss / len(dataloader)\n",
        "        print(f\"Epoch {epoch} completed. Avg outer-loop loss: {avg_loss:.6f}\")\n",
        "\n",
        "        # Checkpointing\n",
        "        if epoch % checkpoint_interval == 0:\n",
        "            ckpt_path = os.path.join(checkpoint_dir, f\"agi_epoch{epoch}.pt\")\n",
        "            torch.save({\n",
        "                'epoch': epoch,\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': model.main_optimizer.state_dict(),\n",
        "                'scheduler_state_dict': scheduler.state_dict()\n",
        "            }, ckpt_path)\n",
        "            print(f\"Checkpoint saved: {ckpt_path}\")\n",
        "\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# 4. Entry Point\n",
        "# -----------------------------------------------------------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    # Hyperparameters\n",
        "    BATCH_SIZE = 32\n",
        "    INPUT_SIZE = 10\n",
        "    HIDDEN_SIZE = 64\n",
        "    OUTPUT_SIZE = 1\n",
        "    LR_MAIN = 1e-3\n",
        "    LR_SELF = 1e-3\n",
        "    EPOCHS = 20\n",
        "    SELF_STEPS = 3\n",
        "    CLIP_GRAD_NORM = 1.0\n",
        "    SCHED_STEP = 5\n",
        "    SCHED_GAMMA = 0.5\n",
        "    CHECKPOINT_DIR = 'checkpoints'\n",
        "    CHECKPOINT_INTERVAL = 5\n",
        "\n",
        "    # Setup\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    dataloader = get_dataloader(batch_size=BATCH_SIZE, input_size=INPUT_SIZE)\n",
        "\n",
        "    model = SelfRecursiveAGI(\n",
        "        input_size=INPUT_SIZE,\n",
        "        hidden_size=HIDDEN_SIZE,\n",
        "        output_size=OUTPUT_SIZE,\n",
        "        lr_main=LR_MAIN,\n",
        "        lr_self=LR_SELF\n",
        "    )\n",
        "    loss_fn = nn.MSELoss()\n",
        "\n",
        "    # Train\n",
        "    train_and_self_improve(\n",
        "        model=model,\n",
        "        dataloader=dataloader,\n",
        "        loss_fn=loss_fn,\n",
        "        device=device,\n",
        "        epochs=EPOCHS,\n",
        "        self_steps=SELF_STEPS,\n",
        "        clip_grad_norm=CLIP_GRAD_NORM,\n",
        "        scheduler_step=SCHED_STEP,\n",
        "        scheduler_gamma=SCHED_GAMMA,\n",
        "        checkpoint_dir=CHECKPOINT_DIR,\n",
        "        checkpoint_interval=CHECKPOINT_INTERVAL\n",
        "    )"
      ]
    }
  ]
}