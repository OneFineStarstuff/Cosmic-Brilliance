{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyOhR91XKJ068/wndWOEZxar",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OneFineStarstuff/Cosmic-Brilliance/blob/main/reward_model_train_py.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DldlHOE6Wqtz"
      },
      "outputs": [],
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "reward_model_train.py\n",
        "\n",
        "Trains a simple reward model (single scalar output) on scored contextâ€“question pairs.\n",
        "\n",
        "Expected JSONL file format (scored_questions.jsonl):\n",
        "{\"context\": \"...\", \"question\": \"...\", \"quality\": 0.85}\n",
        "{\"context\": \"...\", \"question\": \"...\", \"quality\": 0.42}\n",
        "...\n",
        "\"\"\"\n",
        "\n",
        "from datasets import load_dataset\n",
        "from transformers import (\n",
        "    AutoModelForSequenceClassification,\n",
        "    AutoTokenizer,\n",
        "    Trainer,\n",
        "    TrainingArguments,\n",
        ")\n",
        "import torch\n",
        "import random\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# -----------------------\n",
        "# Config\n",
        "# -----------------------\n",
        "MODEL_ID = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "DATA_FILE = \"scored_questions.jsonl\"\n",
        "OUTPUT_DIR = \"rm_ckpt\"\n",
        "FINAL_DIR = \"rm_final\"\n",
        "MAX_LENGTH = 128\n",
        "BATCH_SIZE = 32\n",
        "LR = 2e-5\n",
        "EPOCHS = 4\n",
        "WEIGHT_DECAY = 0.01\n",
        "SEED = 42\n",
        "\n",
        "# -----------------------\n",
        "# Reproducibility\n",
        "# -----------------------\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "# -----------------------\n",
        "# Load tokenizer & model\n",
        "# -----------------------\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    MODEL_ID, num_labels=1\n",
        ")\n",
        "\n",
        "# -----------------------\n",
        "# Preprocess function\n",
        "# -----------------------\n",
        "def preprocess(batch):\n",
        "    \"\"\"\n",
        "    Concatenate context and question, tokenize, and attach label.\n",
        "    \"\"\"\n",
        "    text = [f\"{c} ||| {q}\" for c, q in zip(batch[\"context\"], batch[\"question\"])]\n",
        "    enc = tokenizer(text, truncation=True, padding=\"max_length\", max_length=MAX_LENGTH)\n",
        "    enc[\"labels\"] = batch[\"quality\"]\n",
        "    return enc\n",
        "\n",
        "# -----------------------\n",
        "# Load & prepare dataset\n",
        "# -----------------------\n",
        "raw_dataset = load_dataset(\"json\", data_files=DATA_FILE)[\"train\"]\n",
        "dataset = raw_dataset.map(preprocess, batched=True, remove_columns=raw_dataset.column_names)\n",
        "\n",
        "# -----------------------\n",
        "# Training args\n",
        "# -----------------------\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=OUTPUT_DIR,\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    per_device_train_batch_size=BATCH_SIZE,\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    learning_rate=LR,\n",
        "    num_train_epochs=EPOCHS,\n",
        "    weight_decay=WEIGHT_DECAY,\n",
        "    logging_dir=os.path.join(OUTPUT_DIR, \"logs\"),\n",
        "    logging_steps=50,\n",
        "    save_strategy=\"epoch\",\n",
        "    save_total_limit=2,\n",
        "    load_best_model_at_end=True,\n",
        "    report_to=\"none\",  # disable W&B/MLflow unless you want them\n",
        ")\n",
        "\n",
        "# -----------------------\n",
        "# Trainer\n",
        "# -----------------------\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=dataset,\n",
        ")\n",
        "\n",
        "# -----------------------\n",
        "# Train & save\n",
        "# -----------------------\n",
        "trainer.train()\n",
        "model.save_pretrained(FINAL_DIR)\n",
        "tokenizer.save_pretrained(FINAL_DIR)\n",
        "\n",
        "print(f\"Training complete. Model & tokenizer saved to '{FINAL_DIR}'.\")"
      ]
    }
  ]
}