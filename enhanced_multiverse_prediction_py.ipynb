{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyOZcCKb6WkBxErlH0RnLNQ7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OneFineStarstuff/Cosmic-Brilliance/blob/main/enhanced_multiverse_prediction_py.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "movCGmnm-eeH"
      },
      "outputs": [],
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "enhanced_multiverse_prediction.py\n",
        "\n",
        "Train a deep neural network to predict multiverse topology classes\n",
        "from synthetic string-theory compactification parameters.\n",
        "Improvements include class-balanced loss, batchnorm, dropout,\n",
        "ReduceLROnPlateau, early stopping, and checkpointing.\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "\n",
        "# 1. Synthetic Dataset Generation\n",
        "def sample_compactifications(n_samples=15000, input_dim=5):\n",
        "    \"\"\"\n",
        "    Generate synthetic compactification parameters and discrete labels.\n",
        "    Uses np.ptp() for NumPy 2.0 compatibility.\n",
        "    \"\"\"\n",
        "    params = np.random.uniform(0, 2*np.pi, size=(n_samples, input_dim)).astype(np.float32)\n",
        "    scores = np.sum(np.sin(params), axis=1)\n",
        "    span = np.ptp(scores) + 1e-6\n",
        "    labels = np.floor((scores - scores.min()) / span * 10).astype(int)\n",
        "    labels = np.clip(labels, 0, 9)\n",
        "    return params, labels\n",
        "\n",
        "# 2. Model Definition\n",
        "class MultiverseAI(nn.Module):\n",
        "    def __init__(self, input_dim=5, hidden_dims=[64, 32], output_dim=10, drop_p=0.3):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dims[0])\n",
        "        self.bn1 = nn.BatchNorm1d(hidden_dims[0])\n",
        "        self.fc2 = nn.Linear(hidden_dims[0], hidden_dims[1])\n",
        "        self.bn2 = nn.BatchNorm1d(hidden_dims[1])\n",
        "        self.fc3 = nn.Linear(hidden_dims[1], output_dim)\n",
        "        self.drop = nn.Dropout(drop_p)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.bn1(self.fc1(x)))\n",
        "        x = self.drop(x)\n",
        "        x = F.relu(self.bn2(self.fc2(x)))\n",
        "        x = self.drop(x)\n",
        "        return F.softmax(self.fc3(x), dim=-1)\n",
        "\n",
        "# 3. Training & Validation Loop\n",
        "def train_and_validate(\n",
        "    model, train_loader, val_loader,\n",
        "    class_weights, epochs=100, lr=1e-3, patience=10, device=None\n",
        "):\n",
        "    device = device or torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "\n",
        "    # Weighted cross-entropy\n",
        "    weight_tensor = torch.tensor(class_weights, dtype=torch.float32, device=device)\n",
        "    criterion = nn.CrossEntropyLoss(weight=weight_tensor)\n",
        "\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=1e-5)\n",
        "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer,\n",
        "                                                     mode=\"max\",\n",
        "                                                     factor=0.5,\n",
        "                                                     patience=5,\n",
        "                                                     verbose=True)\n",
        "\n",
        "    best_val_acc = 0.0\n",
        "    epochs_no_improve = 0\n",
        "    all_true, all_preds = [], []\n",
        "\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        # Training\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        correct_train = 0\n",
        "        total_train = 0\n",
        "\n",
        "        for xb, yb in train_loader:\n",
        "            xb, yb = xb.to(device), yb.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            preds = model(xb)\n",
        "            loss = criterion(preds, yb)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item() * xb.size(0)\n",
        "            correct_train += (preds.argmax(1) == yb).sum().item()\n",
        "            total_train += xb.size(0)\n",
        "\n",
        "        train_loss = running_loss / total_train\n",
        "        train_acc = correct_train / total_train\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        correct_val = 0\n",
        "        total_val = 0\n",
        "        epoch_true, epoch_pred = [], []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for xb, yb in val_loader:\n",
        "                xb, yb = xb.to(device), yb.to(device)\n",
        "                preds = model(xb)\n",
        "                correct_val += (preds.argmax(1) == yb).sum().item()\n",
        "                total_val += xb.size(0)\n",
        "\n",
        "                epoch_true.append(yb.cpu().numpy())\n",
        "                epoch_pred.append(preds.argmax(1).cpu().numpy())\n",
        "\n",
        "        val_acc = correct_val / total_val\n",
        "        scheduler.step(val_acc)\n",
        "\n",
        "        # Checkpointing & Early Stopping\n",
        "        if val_acc > best_val_acc:\n",
        "            best_val_acc = val_acc\n",
        "            epochs_no_improve = 0\n",
        "            torch.save(model.state_dict(), \"best_multiverse_ai.pth\")\n",
        "        else:\n",
        "            epochs_no_improve += 1\n",
        "\n",
        "        # Log progress\n",
        "        if epoch == 1 or epoch % 10 == 0 or val_acc == best_val_acc:\n",
        "            print(f\"Epoch {epoch:03d} ─ \"\n",
        "                  f\"Train Loss: {train_loss:.4f} ─ \"\n",
        "                  f\"Train Acc: {train_acc:.3f} ─ \"\n",
        "                  f\"Val Acc: {val_acc:.3f}\")\n",
        "\n",
        "        if epochs_no_improve >= patience:\n",
        "            print(f\"Early stopping at epoch {epoch}. \"\n",
        "                  f\"Best Val Acc: {best_val_acc:.3f}\")\n",
        "            all_true = np.concatenate(epoch_true)\n",
        "            all_preds = np.concatenate(epoch_pred)\n",
        "            break\n",
        "\n",
        "        all_true = np.concatenate(epoch_true)\n",
        "        all_preds = np.concatenate(epoch_pred)\n",
        "\n",
        "    # Final metrics\n",
        "    print(\"\\nClassification Report:\")\n",
        "    print(classification_report(all_true, all_preds, digits=4))\n",
        "    print(\"Confusion Matrix:\")\n",
        "    print(confusion_matrix(all_true, all_preds))\n",
        "\n",
        "# 4. Main Execution\n",
        "if __name__ == \"__main__\":\n",
        "    # Hyperparameters\n",
        "    INPUT_DIM   = 5\n",
        "    OUTPUT_DIM  = 10\n",
        "    N_SAMPLES   = 15000\n",
        "    TEST_SIZE   = 0.2\n",
        "    BATCH_SIZE  = 256\n",
        "    EPOCHS      = 100\n",
        "    LR          = 1e-3\n",
        "    PATIENCE    = 10\n",
        "\n",
        "    # Generate and balance dataset\n",
        "    X, y = sample_compactifications(N_SAMPLES, INPUT_DIM)\n",
        "    classes, counts = np.unique(y, return_counts=True)\n",
        "    inv_counts = 1.0 / (counts + 1e-6)\n",
        "    class_weights = inv_counts / inv_counts.sum() * len(inv_counts)\n",
        "\n",
        "    # Split data\n",
        "    X_train, X_val, y_train, y_val = train_test_split(\n",
        "        X, y, test_size=TEST_SIZE, random_state=42\n",
        "    )\n",
        "\n",
        "    # Create DataLoaders\n",
        "    train_ds = TensorDataset(torch.from_numpy(X_train), torch.from_numpy(y_train))\n",
        "    val_ds   = TensorDataset(torch.from_numpy(X_val),   torch.from_numpy(y_val))\n",
        "    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
        "    val_loader   = DataLoader(val_ds,   batch_size=BATCH_SIZE)\n",
        "\n",
        "    # Initialize model\n",
        "    model = MultiverseAI(\n",
        "        input_dim=INPUT_DIM,\n",
        "        hidden_dims=[64, 32],\n",
        "        output_dim=OUTPUT_DIM,\n",
        "        drop_p=0.3\n",
        "    )\n",
        "\n",
        "    # Train and validate\n",
        "    train_and_validate(\n",
        "        model, train_loader, val_loader,\n",
        "        class_weights, epochs=EPOCHS, lr=LR, patience=PATIENCE\n",
        "    )"
      ]
    }
  ]
}