{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyOl+unlKpVJhKnNVfBWV2hR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OneFineStarstuff/Cosmic-Brilliance/blob/main/train_tachyon_pinn_py.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5fpwGi7nuNEb"
      },
      "outputs": [],
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "train_tachyon_pinn.py\n",
        "\n",
        "Physics‐informed TachyonAI pipeline with:\n",
        "  - Synthetic data generator for (E, p, f) → (f_next, rate)\n",
        "  - PINN loss enforcing δf/δt = p − E·f\n",
        "  - Residual‐skip MLP with LayerNorm & Dropout\n",
        "  - Early stopping & ReduceLROnPlateau scheduler\n",
        "  - MC‐Dropout uncertainty estimation\n",
        "  - Field‐evolution animation saved as GIF\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.animation as animation\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Hyperparameters\n",
        "# ------------------------------------------------------------------------------\n",
        "INPUT_DIM     = 3\n",
        "HIDDEN_DIM    = 64\n",
        "OUTPUT_DIM    = 2\n",
        "DROPOUT_P     = 0.1\n",
        "DT            = 0.1\n",
        "LAMBDA_PHY    = 1.0\n",
        "LR            = 1e-3\n",
        "BATCH_SIZE    = 128\n",
        "EPOCHS        = 200\n",
        "PATIENCE      = 15\n",
        "MC_SAMPLES    = 50\n",
        "GIF_STEPS     = 80\n",
        "GIF_CURVES    = 4\n",
        "GIF_PATH      = \"tachyon_evolution.gif\"\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 1. Synthetic Data Generator\n",
        "# ------------------------------------------------------------------------------\n",
        "def generate_synthetic_tachyon_data(n_samples=12000, dt=DT):\n",
        "    np.random.seed(42)\n",
        "    E = np.random.uniform(-1,1,(n_samples,1)).astype(np.float32)\n",
        "    p = np.random.uniform(-1,1,(n_samples,1)).astype(np.float32)\n",
        "    f = np.random.uniform(-1,1,(n_samples,1)).astype(np.float32)\n",
        "    X = np.hstack([E,p,f])\n",
        "    f_next = f + dt*(p - E*f)\n",
        "    rate   = p*f - E\n",
        "    Y = np.hstack([f_next, rate]).astype(np.float32)\n",
        "    return X, Y\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 2. Physics‐Informed Residual MLP with Residual Skip & Dropout\n",
        "# ------------------------------------------------------------------------------\n",
        "class TachyonPINN(nn.Module):\n",
        "    def __init__(self, input_dim=INPUT_DIM, hidden_dim=HIDDEN_DIM,\n",
        "                 output_dim=OUTPUT_DIM, dropout=DROPOUT_P):\n",
        "        super().__init__()\n",
        "        self.fc1   = nn.Linear(input_dim, hidden_dim)\n",
        "        self.norm1 = nn.LayerNorm(hidden_dim)\n",
        "        self.relu  = nn.ReLU()\n",
        "        self.drop  = nn.Dropout(dropout)\n",
        "        self.fc2   = nn.Linear(hidden_dim, output_dim)\n",
        "        self.skip  = nn.Linear(input_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h   = self.fc1(x)\n",
        "        h   = self.norm1(h)\n",
        "        h   = self.relu(h)\n",
        "        h   = self.drop(h)\n",
        "        out = self.fc2(h)\n",
        "        return out + self.skip(x)\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 3. PINN Loss: supervised MSE + physical constraint δf/δt = p − E·f\n",
        "# ------------------------------------------------------------------------------\n",
        "def pinn_loss(pred, true, inputs, dt=DT, lambda_phys=LAMBDA_PHY):\n",
        "    mse_loss  = nn.MSELoss()(pred, true)\n",
        "    f         = inputs[:,2]\n",
        "    p         = inputs[:,1]\n",
        "    E         = inputs[:,0]\n",
        "    f_phys    = f + dt*(p - E*f)\n",
        "    phys_loss = nn.MSELoss()(pred[:,0], f_phys)\n",
        "    return mse_loss + lambda_phys*phys_loss, mse_loss, phys_loss\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 4. MC‐Dropout Uncertainty Estimation\n",
        "# ------------------------------------------------------------------------------\n",
        "def mc_dropout_predict(model, x, n_samples=MC_SAMPLES, device='cpu'):\n",
        "    model.train()  # keep dropout active\n",
        "    preds = []\n",
        "    with torch.no_grad():\n",
        "        for _ in range(n_samples):\n",
        "            preds.append(model(x.to(device)).cpu().numpy())\n",
        "    preds = np.stack(preds, axis=0)          # (n_samples, batch, 2)\n",
        "    mean  = preds.mean(axis=0)               # (batch, 2)\n",
        "    std   = preds.std(axis=0)                # (batch, 2)\n",
        "    model.eval()\n",
        "    return mean, std\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 5. DataLoaders\n",
        "# ------------------------------------------------------------------------------\n",
        "def prepare_dataloaders(batch_size=BATCH_SIZE, val_frac=0.2):\n",
        "    X, Y        = generate_synthetic_tachyon_data()\n",
        "    X_tensor    = torch.from_numpy(X)\n",
        "    Y_tensor    = torch.from_numpy(Y)\n",
        "    dataset     = TensorDataset(X_tensor, Y_tensor)\n",
        "    n_val       = int(len(dataset) * val_frac)\n",
        "    n_train     = len(dataset) - n_val\n",
        "    train_ds, val_ds = random_split(dataset, [n_train, n_val])\n",
        "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
        "    val_loader   = DataLoader(val_ds, batch_size=batch_size)\n",
        "    return train_loader, val_loader\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 6. Training Loop with Early Stopping & Scheduler\n",
        "# ------------------------------------------------------------------------------\n",
        "def train_model(model, train_loader, val_loader,\n",
        "                epochs=EPOCHS, lr=LR, weight_decay=1e-5,\n",
        "                lambda_phys=LAMBDA_PHY, patience_max=PATIENCE,\n",
        "                device='cpu'):\n",
        "\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr,\n",
        "                           weight_decay=weight_decay)\n",
        "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
        "        optimizer, mode='min', factor=0.5, patience=5\n",
        "    )\n",
        "\n",
        "    best_val_loss = float('inf')\n",
        "    patience_cnt  = 0\n",
        "    history       = {'train': [], 'val': []}\n",
        "\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        # -- Train --\n",
        "        model.train()\n",
        "        total_train_loss = 0.0\n",
        "        for xb, yb in train_loader:\n",
        "            xb, yb = xb.to(device), yb.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            pred        = model(xb)\n",
        "            loss, _, _ = pinn_loss(pred, yb, xb, lambda_phys=lambda_phys)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_train_loss += loss.item() * xb.size(0)\n",
        "\n",
        "        train_loss = total_train_loss / len(train_loader.dataset)\n",
        "\n",
        "        # -- Validate --\n",
        "        model.eval()\n",
        "        total_val_loss = 0.0\n",
        "        with torch.no_grad():\n",
        "            for xb, yb in val_loader:\n",
        "                xb, yb = xb.to(device), yb.to(device)\n",
        "                pred       = model(xb)\n",
        "                loss, _, _ = pinn_loss(pred, yb, xb, lambda_phys=lambda_phys)\n",
        "                total_val_loss += loss.item() * xb.size(0)\n",
        "\n",
        "        val_loss = total_val_loss / len(val_loader.dataset)\n",
        "        scheduler.step(val_loss)\n",
        "\n",
        "        history['train'].append(train_loss)\n",
        "        history['val'].append(val_loss)\n",
        "\n",
        "        # Check for best\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            torch.save(model.state_dict(), \"best_tachyon_pinn.pt\")\n",
        "            patience_cnt = 0\n",
        "        else:\n",
        "            patience_cnt += 1\n",
        "            if patience_cnt >= patience_max:\n",
        "                print(f\"Early stopping at epoch {epoch}\")\n",
        "                break\n",
        "\n",
        "        if epoch % 10 == 0 or epoch == 1:\n",
        "            print(f\"Epoch {epoch:03d} | Train Loss {train_loss:.4e} | Val Loss {val_loss:.4e}\")\n",
        "\n",
        "    # Load best\n",
        "    model.load_state_dict(torch.load(\"best_tachyon_pinn.pt\"))\n",
        "    return model, history, best_val_loss\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 7. Field Evolution Animation\n",
        "# ------------------------------------------------------------------------------\n",
        "def animate_evolution(model, n_steps=GIF_STEPS, n_curves=GIF_CURVES,\n",
        "                      dt=DT, save_path=GIF_PATH, device='cpu'):\n",
        "\n",
        "    # Random initial conditions (E, p, f)\n",
        "    X0 = np.random.uniform(-1,1,(n_curves,3)).astype(np.float32)\n",
        "    x  = torch.from_numpy(X0).to(device)\n",
        "\n",
        "    # Record f over time\n",
        "    paths = np.zeros((n_curves, n_steps+1), dtype=np.float32)\n",
        "    paths[:,0] = X0[:,2]\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for t in range(1, n_steps+1):\n",
        "            pred   = model(x).cpu().numpy()\n",
        "            f_next = pred[:,0]\n",
        "            paths[:,t] = f_next\n",
        "            # update input: keep E,p fixed, update f\n",
        "            x = torch.stack([x[:,0], x[:,1], torch.from_numpy(f_next).to(device)], dim=1)\n",
        "\n",
        "    # Plot & animate\n",
        "    fig, ax = plt.subplots()\n",
        "    lines = [ax.plot([], [], lw=2)[0] for _ in range(n_curves)]\n",
        "    ax.set_xlim(0, n_steps*dt)\n",
        "    ax.set_ylim(paths.min()-0.1, paths.max()+0.1)\n",
        "    ax.set_xlabel('t'); ax.set_ylabel('f(t)')\n",
        "    ax.set_title('Tachyonic Field Evolution')\n",
        "\n",
        "    def init():\n",
        "        for ln in lines:\n",
        "            ln.set_data([], [])\n",
        "        return lines\n",
        "\n",
        "    def update(frame):\n",
        "        t_vals = np.linspace(0, frame*dt, frame+1)\n",
        "        for i, ln in enumerate(lines):\n",
        "            ln.set_data(t_vals, paths[i,:frame+1])\n",
        "        return lines\n",
        "\n",
        "    anim = animation.FuncAnimation(fig, update, frames=n_steps+1,\n",
        "                                   init_func=init, blit=True)\n",
        "    anim.save(save_path, writer='pillow', fps=10)\n",
        "    plt.close(fig)\n",
        "    print(f\"Animation saved to {save_path}\")\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 8. Main Execution\n",
        "# ------------------------------------------------------------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # Prepare data\n",
        "    train_loader, val_loader = prepare_dataloaders()\n",
        "\n",
        "    # Build and train model\n",
        "    model = TachyonPINN().to(device)\n",
        "    model, history, best_val = train_model(model, train_loader, val_loader, device=device)\n",
        "    print(f\"\\nTraining complete. Best Val Loss: {best_val:.4e}\")\n",
        "\n",
        "    # Plot training history\n",
        "    plt.figure()\n",
        "    plt.plot(history['train'], label=\"Train Loss\")\n",
        "    plt.plot(history['val'],   label=\"Val Loss\")\n",
        "    plt.xlabel(\"Epoch\"); plt.ylabel(\"Loss\")\n",
        "    plt.legend(); plt.show()\n",
        "\n",
        "    # MC‐Dropout example on validation batch\n",
        "    xb, yb = next(iter(val_loader))\n",
        "    mean_p, std_p = mc_dropout_predict(model, xb, device=device)\n",
        "    print(\"\\nMC‐Dropout sample predictions (mean ± std):\")\n",
        "    for i in range(min(5, len(xb))):\n",
        "        print(xb[i].numpy(), \"→\", mean_p[i], \"±\", std_p[i])\n",
        "\n",
        "    # Animate field evolution and save GIF\n",
        "    animate_evolution(model, device=device)"
      ]
    }
  ]
}