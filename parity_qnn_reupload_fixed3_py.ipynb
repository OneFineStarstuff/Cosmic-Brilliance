{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyPDP0YwSR7tAmzkwAJe3u0e",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OneFineStarstuff/Cosmic-Brilliance/blob/main/parity_qnn_reupload_fixed3_py.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OXFf0U4vVni3"
      },
      "outputs": [],
      "source": [
        "pip install pennylane torch tqdm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "parity_qnn_reupload_fixed3.py\n",
        "\n",
        "Full parity QNN training pipeline with:\n",
        "- Data re-uploading via AngleEmbedding + StronglyEntanglingLayers\n",
        "- Proper DataLoader unpacking\n",
        "- MC-Dropout variance estimation\n",
        "- Temperature scaling & reliability diagram\n",
        "\"\"\"\n",
        "\n",
        "import argparse\n",
        "import numpy as np\n",
        "import pennylane as qml\n",
        "from pennylane.templates.layers import StronglyEntanglingLayers\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.calibration import calibration_curve\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "def parse_args():\n",
        "    parser = argparse.ArgumentParser(\n",
        "        description=\"Parity QNN with data re-uploading\"\n",
        "    )\n",
        "    parser.add_argument(\"--n_qubits\",   type=int,   default=3)\n",
        "    parser.add_argument(\"--n_layers\",   type=int,   default=3)\n",
        "    parser.add_argument(\"--hidden_dim\", type=int,   default=8)\n",
        "    parser.add_argument(\"--dropout\",    type=float, default=0.1)\n",
        "    parser.add_argument(\"--mc_runs\",    type=int,   default=50)\n",
        "    parser.add_argument(\"--epochs\",     type=int,   default=15)\n",
        "    parser.add_argument(\"--lr\",         type=float, default=1e-2)\n",
        "\n",
        "    # Ignore unknown flags (e.g., '-f ...json')\n",
        "    args, _ = parser.parse_known_args()\n",
        "    return args\n",
        "\n",
        "def generate_parity_dataset(n_qubits):\n",
        "    X = np.array(\n",
        "        [list(map(int, np.binary_repr(i, width=n_qubits)))\n",
        "         for i in range(2**n_qubits)],\n",
        "        dtype=np.float32,\n",
        "    )\n",
        "    y = X.sum(axis=1) % 2\n",
        "    return X, y.astype(np.int64)\n",
        "\n",
        "class HybridParityReupload(nn.Module):\n",
        "    def __init__(self, qlayer, n_qubits, hidden_dim, dropout_p):\n",
        "        super().__init__()\n",
        "        self.pre_net  = nn.Sequential(\n",
        "            nn.Linear(n_qubits, n_qubits), nn.ReLU()\n",
        "        )\n",
        "        self.qlayer   = qlayer\n",
        "        self.dropout  = nn.Dropout(dropout_p)\n",
        "        self.post_net = nn.Sequential(\n",
        "            nn.Linear(n_qubits, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, 2),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x_pre  = self.pre_net(x)\n",
        "        q_out  = self.qlayer(x_pre)\n",
        "        q_drop = self.dropout(q_out)\n",
        "        return self.post_net(q_drop)\n",
        "\n",
        "def main():\n",
        "    args = parse_args()\n",
        "\n",
        "    # 1) Data preparation\n",
        "    X, y = generate_parity_dataset(args.n_qubits)\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y, test_size=0.2, random_state=42, stratify=y\n",
        "    )\n",
        "\n",
        "    train_ds = TensorDataset(\n",
        "        torch.tensor(X_train, dtype=torch.float32),\n",
        "        torch.tensor(y_train, dtype=torch.int64),\n",
        "    )\n",
        "    test_ds  = TensorDataset(\n",
        "        torch.tensor(X_test, dtype=torch.float32),\n",
        "        torch.tensor(y_test, dtype=torch.int64),\n",
        "    )\n",
        "    train_loader = DataLoader(train_ds, batch_size=32, shuffle=True)\n",
        "    test_loader  = DataLoader(test_ds,  batch_size=32)\n",
        "\n",
        "    # 2) Quantum device & QNode\n",
        "    dev = qml.device(\"default.qubit\", wires=args.n_qubits)\n",
        "\n",
        "    @qml.qnode(dev, interface=\"torch\", diff_method=\"backprop\")\n",
        "    def circuit(inputs, weights):\n",
        "        for layer in range(args.n_layers):\n",
        "            qml.templates.AngleEmbedding(\n",
        "                inputs, wires=range(args.n_qubits), rotation=\"Y\"\n",
        "            )\n",
        "            w = weights[layer : layer + 1]  # shape (1, n_qubits, 3)\n",
        "            StronglyEntanglingLayers(w, wires=range(args.n_qubits))\n",
        "        qml.templates.AngleEmbedding(\n",
        "            inputs, wires=range(args.n_qubits), rotation=\"Y\"\n",
        "        )\n",
        "        return [qml.expval(qml.PauliZ(i)) for i in range(args.n_qubits)]\n",
        "\n",
        "    weight_shapes = {\"weights\": (args.n_layers, args.n_qubits, 3)}\n",
        "    qlayer = qml.qnn.TorchLayer(circuit, weight_shapes)\n",
        "\n",
        "    # 3) Hybrid model, optimizer, loss\n",
        "    model    = HybridParityReupload(\n",
        "        qlayer, args.n_qubits, args.hidden_dim, args.dropout\n",
        "    )\n",
        "    optimizer = optim.Adam(model.parameters(), lr=args.lr)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # 4) Training\n",
        "    for epoch in range(1, args.epochs + 1):\n",
        "        model.train()\n",
        "        total_loss, correct = 0.0, 0\n",
        "        for xb, yb in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            logits = model(xb)\n",
        "            loss   = criterion(logits, yb)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item() * xb.size(0)\n",
        "            correct    += (logits.argmax(1) == yb).sum().item()\n",
        "        train_acc = correct / len(train_loader.dataset)\n",
        "\n",
        "        model.eval()\n",
        "        correct = 0\n",
        "        with torch.no_grad():\n",
        "            for xb, yb in test_loader:\n",
        "                correct += (model(xb).argmax(1) == yb).sum().item()\n",
        "        test_acc = correct / len(test_loader.dataset)\n",
        "\n",
        "        print(\n",
        "            f\"Epoch {epoch:2d} | \"\n",
        "            f\"Loss {total_loss/len(train_loader.dataset):.4f} | \"\n",
        "            f\"Train {train_acc:.3f} | Test {test_acc:.3f}\"\n",
        "        )\n",
        "\n",
        "    # 5) MC-Dropout variance\n",
        "    model.train()\n",
        "    all_probs = []\n",
        "    with torch.no_grad():\n",
        "        for _ in range(args.mc_runs):\n",
        "            batch_ps = []\n",
        "            for xb, _ in test_loader:\n",
        "                ps = torch.softmax(model(xb), dim=1).cpu().numpy()\n",
        "                batch_ps.append(ps)\n",
        "            all_probs.append(np.vstack(batch_ps))\n",
        "    var_est = np.stack(all_probs).var(axis=0).mean(axis=1)\n",
        "    plt.hist(var_est, bins=20)\n",
        "    plt.title(\"MC-Dropout Variance\")\n",
        "    plt.savefig(\"mc_variance.png\")\n",
        "    print(\"Saved MC-Dropout variance histogram\")\n",
        "\n",
        "    # 6) Temperature scaling\n",
        "    model.eval()\n",
        "    logits_stack, labels_stack = [], []\n",
        "    with torch.no_grad():\n",
        "        for xb, yb in train_loader:\n",
        "            logits_stack.append(model(xb))\n",
        "            labels_stack.append(yb)\n",
        "    logits_stack = torch.cat(logits_stack)\n",
        "    labels_stack = torch.cat(labels_stack)\n",
        "\n",
        "    T = torch.ones(1, requires_grad=True)\n",
        "    def loss_T(): return criterion(logits_stack / T, labels_stack)\n",
        "    optim.LBFGS([T], lr=0.1, max_iter=50).step(lambda: loss_T())\n",
        "    T = T.detach()  # freeze temperature\n",
        "    print(f\"Optimal temperature T = {T.item():.3f}\")\n",
        "\n",
        "    # 7) Reliability diagram\n",
        "    model.eval()\n",
        "    logits_test = torch.cat([model(xb) for xb, _ in test_loader])\n",
        "    probs_test  = (\n",
        "        torch.softmax(logits_test / T, dim=1)\n",
        "        .detach()\n",
        "        .cpu()\n",
        "        .numpy()[:, 1]\n",
        "    )\n",
        "    frac_pos, mean_pred = calibration_curve(y_test, probs_test, n_bins=10)\n",
        "    plt.figure()\n",
        "    plt.plot(mean_pred, frac_pos, \"s-\", label=\"Model\")\n",
        "    plt.plot([0, 1], [0, 1], \"--\", label=\"Ideal\")\n",
        "    plt.xlabel(\"Mean predicted probability\")\n",
        "    plt.ylabel(\"Fraction of positives\")\n",
        "    plt.title(\"Reliability Diagram\")\n",
        "    plt.legend()\n",
        "    plt.savefig(\"reliability_diagram.png\")\n",
        "    print(\"Saved reliability diagram\")\n",
        "\n",
        "    # 8) Final accuracy\n",
        "    final_acc = (logits_test.argmax(1).cpu().numpy() == y_test).mean()\n",
        "    print(f\"Final Test Accuracy: {final_acc:.3f}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "Cr1n3z4YWFqa"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}