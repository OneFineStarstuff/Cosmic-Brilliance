{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyP5EqvR+yjV6Miym6CSsQAu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OneFineStarstuff/Cosmic-Brilliance/blob/main/train_planetary_ai_py.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "train_planetary_ai.py\n",
        "\n",
        "Full pipeline for PlanetaryAI:\n",
        "1. Synthetic “planetary” dataset of 6 inputs → 3 outputs\n",
        "2. Standardization, noise injection\n",
        "3. MLP with residual head & dropout\n",
        "4. Training loop with gradient clipping, LR scheduler, checkpointing\n",
        "5. Validation and loss reporting\n",
        "6. Optional scatter plots of true vs. predicted\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 1. Synthetic Planetary Dataset\n",
        "# ------------------------------------------------------------------------------\n",
        "class PlanetaryDataset(Dataset):\n",
        "    def __init__(self, n_samples=5000, seed=42):\n",
        "        torch.manual_seed(seed)\n",
        "        data = torch.rand(n_samples, 6)  # CNS, CER, ICL, PRA, ECI, CDF\n",
        "        CNS, CER, ICL, PRA, ECI, CDF = data.t()\n",
        "        ε = 1e-6\n",
        "\n",
        "        # Toy “planetary laws”\n",
        "        coherence  = (CNS * CER) / (ICL + ε)\n",
        "        efficiency = PRA / (ICL + 1.0)\n",
        "        retention  = CDF * torch.exp(-ECI)\n",
        "\n",
        "        targets = torch.stack([coherence, efficiency, retention], dim=1)\n",
        "        targets += 0.01 * targets.std(0) * torch.randn_like(targets)  # 1% noise\n",
        "\n",
        "        # Standardize\n",
        "        self.X_mean, self.X_std = data.mean(0), data.std(0) + ε\n",
        "        self.Y_mean, self.Y_std = targets.mean(0), targets.std(0) + ε\n",
        "\n",
        "        self.X = ((data - self.X_mean) / self.X_std).float()\n",
        "        self.Y = ((targets - self.Y_mean) / self.Y_std).float()\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx], self.Y[idx]\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 2. PlanetaryAI Model with Residuals & Dropout\n",
        "# ------------------------------------------------------------------------------\n",
        "class PlanetaryAI(nn.Module):\n",
        "    def __init__(self, input_dim=6, hidden_dim=32, output_dim=3, p_drop=0.1):\n",
        "        super().__init__()\n",
        "        self.fc1      = nn.Linear(input_dim, hidden_dim)\n",
        "        self.res_head = nn.Linear(input_dim, hidden_dim)\n",
        "        self.fc2      = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.fc3      = nn.Linear(hidden_dim, output_dim)\n",
        "        self.relu     = nn.ReLU()\n",
        "        self.drop     = nn.Dropout(p_drop)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h1 = self.relu(self.fc1(x))\n",
        "        r  = self.res_head(x)\n",
        "        h2 = self.drop(self.relu(self.fc2(h1 + r)))\n",
        "        out = self.fc3(h2)\n",
        "        return out\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 3. Training & Validation Helpers\n",
        "# ------------------------------------------------------------------------------\n",
        "def train_one_epoch(model, loader, optimizer, device):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    criterion = nn.MSELoss()\n",
        "\n",
        "    for Xb, Yb in loader:\n",
        "        Xb, Yb = Xb.to(device), Yb.to(device)\n",
        "        pred = model(Xb)\n",
        "        loss = criterion(pred, Yb)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item() * Xb.size(0)\n",
        "\n",
        "    return total_loss / len(loader.dataset)\n",
        "\n",
        "def validate(model, loader, device):\n",
        "    model.eval()\n",
        "    total_loss = 0.0\n",
        "    criterion = nn.MSELoss()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for Xb, Yb in loader:\n",
        "            Xb, Yb = Xb.to(device), Yb.to(device)\n",
        "            pred = model(Xb)\n",
        "            total_loss += criterion(pred, Yb).item() * Xb.size(0)\n",
        "\n",
        "    return total_loss / len(loader.dataset)\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 4. Main Execution\n",
        "# ------------------------------------------------------------------------------\n",
        "def main():\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # Prepare data\n",
        "    dataset = PlanetaryDataset(n_samples=5000, seed=42)\n",
        "    val_size = int(0.2 * len(dataset))\n",
        "    train_ds, val_ds = random_split(dataset, [len(dataset)-val_size, val_size])\n",
        "    train_loader = DataLoader(train_ds, batch_size=128, shuffle=True)\n",
        "    val_loader   = DataLoader(val_ds, batch_size=256, shuffle=False)\n",
        "\n",
        "    # Model, optimizer, scheduler\n",
        "    model     = PlanetaryAI().to(device)\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-5)\n",
        "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min',\n",
        "                                                     factor=0.5, patience=5)\n",
        "\n",
        "    best_val = float('inf')\n",
        "    os.makedirs(\"checkpoints\", exist_ok=True)\n",
        "\n",
        "    # Training loop\n",
        "    for epoch in range(1, 51):\n",
        "        tr_loss = train_one_epoch(model, train_loader, optimizer, device)\n",
        "        va_loss = validate(model, val_loader, device)\n",
        "        scheduler.step(va_loss)\n",
        "        print(f\"Epoch {epoch:02d} | Train Loss: {tr_loss:.4f} | Val Loss: {va_loss:.4f}\")\n",
        "\n",
        "        # Save best\n",
        "        if va_loss < best_val - 1e-5:\n",
        "            best_val = va_loss\n",
        "            torch.save(model.state_dict(), \"checkpoints/planetary_best.pth\")\n",
        "\n",
        "    # Load the best model\n",
        "    model.load_state_dict(torch.load(\"checkpoints/planetary_best.pth\", map_location=device))\n",
        "    print(\"Best model loaded.\")\n",
        "\n",
        "    # Optional: scatter plots of True vs. Predicted\n",
        "    X_all, Y_all = dataset.X.to(device), dataset.Y.to(device)\n",
        "    with torch.no_grad():\n",
        "        Y_pred = model(X_all)\n",
        "\n",
        "    Y_true = (Y_all * dataset.Y_std.to(device)) + dataset.Y_mean.to(device)\n",
        "    Y_est  = (Y_pred * dataset.Y_std.to(device)) + dataset.Y_mean.to(device)\n",
        "\n",
        "    for i, name in enumerate([\"Coherence\", \"Efficiency\", \"Retention\"]):\n",
        "        plt.figure(figsize=(4,4))\n",
        "        plt.scatter(Y_true[:,i].cpu(), Y_est[:,i].cpu(), s=5, alpha=0.6)\n",
        "        m, M = Y_true[:,i].min(), Y_true[:,i].max()\n",
        "        plt.plot([m, M], [m, M], 'r--')\n",
        "        plt.title(name)\n",
        "        plt.xlabel(\"True\")\n",
        "        plt.ylabel(\"Predicted\")\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "JjvtLVm7yCJI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}