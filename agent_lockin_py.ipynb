{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyPgZgdwWlYKY6UmUhes77mI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OneFineStarstuff/Cosmic-Brilliance/blob/main/agent_lockin_py.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IznJy_Zz_PNy"
      },
      "outputs": [],
      "source": [
        "#!/usr/bin/env python3\n",
        "# -*- coding: utf-8 -*-\n",
        "\n",
        "\"\"\"\n",
        "End-to-end agent with:\n",
        "- requires_human_review decorator (forwards *args, **kwargs)\n",
        "- Adaptive temperature based on score\n",
        "- Lock-in: once GT is matched (or fully covered with high score), snap to GT and freeze exploration\n",
        "- Optional blended peer feedback per step\n",
        "- Audit history export (JSON/CSV)\n",
        "\n",
        "Run examples:\n",
        "  python agent_lockin.py\n",
        "  python agent_lockin.py --blend-feedback\n",
        "  python agent_lockin.py --steps 8 --seed 7 --history runs/history.json\n",
        "\"\"\"\n",
        "\n",
        "from __future__ import annotations\n",
        "\n",
        "import argparse\n",
        "import csv\n",
        "import json\n",
        "import os\n",
        "import random\n",
        "import sys\n",
        "import time\n",
        "from dataclasses import dataclass\n",
        "from functools import wraps\n",
        "from typing import Any, Dict, List, Optional, Tuple\n",
        "\n",
        "# -------------------------------\n",
        "# Configuration\n",
        "# -------------------------------\n",
        "\n",
        "@dataclass\n",
        "class Config:\n",
        "    human_review_thresh: float = 0.20\n",
        "    coverage_lock_score: float = 0.95\n",
        "    min_temperature: float = 0.01\n",
        "    novelty_increment: float = 0.05\n",
        "    seed: int = 42\n",
        "    steps: int = 5\n",
        "    history_path: str = \"runs/history.json\"\n",
        "    blend_feedback: bool = False\n",
        "\n",
        "CFG = Config()\n",
        "\n",
        "# -------------------------------\n",
        "# Decorators\n",
        "# -------------------------------\n",
        "\n",
        "def requires_human_review(if_score_below: float):\n",
        "    \"\"\"Decorator that prints a review notice for low scores, forwarding all args/kwargs.\"\"\"\n",
        "    def deco(fn):\n",
        "        @wraps(fn)\n",
        "        def wrapped(self, kernel, reason, *args, **kwargs):\n",
        "            if reason < if_score_below:\n",
        "                print(f\"[HUMAN-REVIEW] score={reason:.3f} < threshold={if_score_below:.3f} — flagging before adapt()\")\n",
        "            return fn(self, kernel, reason, *args, **kwargs)\n",
        "        return wrapped\n",
        "    return deco\n",
        "\n",
        "# -------------------------------\n",
        "# Data structures\n",
        "# -------------------------------\n",
        "\n",
        "@dataclass\n",
        "class Feedback:\n",
        "    from_peer: str\n",
        "    t: int\n",
        "    comment: str\n",
        "    delta_hint: Optional[str] = None\n",
        "    weight: float = 1.0\n",
        "\n",
        "# -------------------------------\n",
        "# Engines and components\n",
        "# -------------------------------\n",
        "\n",
        "class SimilarityEngine:\n",
        "    \"\"\"\n",
        "    Set-based Jaccard similarity:\n",
        "      - Tokens are whitespace-lowered unique sets (duplicate words collapse)\n",
        "      - overlap = |T ∩ G|\n",
        "      - union   = |T ∪ G|\n",
        "      - jaccard = overlap / union\n",
        "    \"\"\"\n",
        "    def test(self, theory: str, gt: str) -> Dict[str, Any]:\n",
        "        tset = set(theory.lower().split())\n",
        "        gset = set(gt.lower().split())\n",
        "        inter = len(tset & gset)\n",
        "        union = max(1, len(tset | gset))\n",
        "        return {\n",
        "            \"tokens_theory\": len(tset),\n",
        "            \"tokens_gt\": len(gset),\n",
        "            \"jaccard\": inter / union,\n",
        "            \"overlap\": inter,\n",
        "            \"union\": union,\n",
        "        }\n",
        "\n",
        "class ScoringEngine:\n",
        "    def evaluate(self, theory: str, sim_res: Dict[str, Any], gt: str) -> float:\n",
        "        return float(sim_res.get(\"jaccard\", 0.0))\n",
        "\n",
        "class TheoryGenerator:\n",
        "    \"\"\"\n",
        "    Interleaves a fixed 'core' phrase with sampled vocabulary.\n",
        "    Keeps duplicated 'the' in core to enable exact GT reconstruction.\n",
        "    \"\"\"\n",
        "    def __init__(self, vocabulary: Optional[List[str]] = None):\n",
        "        self.vocab = vocabulary or [\n",
        "            \"the\", \"quick\", \"brown\", \"fox\", \"jumps\", \"over\", \"lazy\", \"dog\",\n",
        "            \"smart\", \"agile\", \"beyond\", \"robust\", \"audit\", \"traceable\", \"reliable\",\n",
        "            \"policy\", \"signal\", \"contract\", \"governed\", \"reviewed\", \"clear\"\n",
        "        ]\n",
        "\n",
        "    def generate(self, abstraction: Dict[str, Any]) -> str:\n",
        "        # Snap-to-GT if requested by upstream lock-in\n",
        "        force_gt = abstraction.get(\"force_gt\")\n",
        "        if isinstance(force_gt, str) and force_gt:\n",
        "            return force_gt\n",
        "\n",
        "        temperature = float(abstraction.get(\"temperature\", 0.2))\n",
        "        nudge = abstraction.get(\"nudge\")\n",
        "\n",
        "        # Sample k tokens based on temperature\n",
        "        k = min(9, max(3, int(3 + temperature * 6)))\n",
        "        tokens = random.sample(self.vocab, k=k)\n",
        "        if isinstance(nudge, str) and nudge:\n",
        "            # Ensure the nudge appears early in the sequence\n",
        "            tokens = [nudge] + [tok for tok in tokens if tok != nudge]\n",
        "\n",
        "        # Core phrase (duplicate 'the' intentionally preserved)\n",
        "        core = [\"the\", \"quick\", \"brown\", \"fox\", \"jumps\", \"over\", \"the\", \"lazy\", \"dog\"]\n",
        "\n",
        "        # Interleave: after every second core token, insert one sampled token if available\n",
        "        mix: List[str] = []\n",
        "        token_idx = 0\n",
        "        for i, tok in enumerate(core):\n",
        "            mix.append(tok)\n",
        "            if i % 2 == 1 and token_idx < len(tokens):\n",
        "                mix.append(tokens[token_idx])\n",
        "                token_idx += 1\n",
        "\n",
        "        return \" \".join(mix)\n",
        "\n",
        "class MemoryModelReviser:\n",
        "    \"\"\"\n",
        "    Translates scores + feedback into generator abstraction changes.\n",
        "    Logs every adaptation and lock-in for auditability.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        self.history: List[Dict[str, Any]] = []\n",
        "\n",
        "    @requires_human_review(if_score_below=CFG.human_review_thresh)\n",
        "    def adapt(self, kernel: \"Agent\", reason: float, feedback: Optional[Feedback] = None) -> None:\n",
        "        temperature = max(CFG.min_temperature, min(1.0, 1.0 - reason))\n",
        "        nudge = feedback.delta_hint if feedback and feedback.delta_hint else None\n",
        "\n",
        "        kernel.last_abstraction = {\n",
        "            \"temperature\": temperature,\n",
        "            \"nudge\": nudge,\n",
        "            \"last_score\": reason,\n",
        "            \"last_feedback_peer\": getattr(feedback, \"from_peer\", None),\n",
        "            \"t\": getattr(feedback, \"t\", None),\n",
        "        }\n",
        "\n",
        "        self.history.append({\n",
        "            \"event\": \"adapt\",\n",
        "            \"ts\": time.time(),\n",
        "            \"score\": reason,\n",
        "            \"temperature\": temperature,\n",
        "            \"nudge\": nudge,\n",
        "            \"peer\": getattr(feedback, \"from_peer\", None),\n",
        "            \"t\": getattr(feedback, \"t\", None),\n",
        "        })\n",
        "\n",
        "        print(f\"[ADAPT] score={reason:.3f} -> temp={temperature:.2f} nudge={repr(nudge)} \"\n",
        "              f\"peer={getattr(feedback, 'from_peer', None)} t={getattr(feedback, 't', None)}\")\n",
        "\n",
        "    def record_lock_in(self, theory: str, gt: str) -> None:\n",
        "        self.history.append({\n",
        "            \"event\": \"lock_in\",\n",
        "            \"ts\": time.time(),\n",
        "            \"score\": 1.0 if theory.strip().lower() == gt.strip().lower() else None,\n",
        "            \"note\": \"Snapped to canonical GT and froze exploration.\"\n",
        "        })\n",
        "\n",
        "# -------------------------------\n",
        "# Agent\n",
        "# -------------------------------\n",
        "\n",
        "class Agent:\n",
        "    \"\"\"\n",
        "    Orchestrates:\n",
        "      - similarity → score\n",
        "      - lock-in detection (exact match or full coverage with high score)\n",
        "      - adapt with feedback\n",
        "      - generate next theory\n",
        "      - freeze once locked\n",
        "    \"\"\"\n",
        "    def __init__(self, tg: TheoryGenerator, simi: SimilarityEngine, ste: ScoringEngine,\n",
        "                 mmr: MemoryModelReviser, gt: str) -> None:\n",
        "        self.tg = tg\n",
        "        self.simi = simi\n",
        "        self.ste = ste\n",
        "        self.mmr = mmr\n",
        "        self.gt = gt\n",
        "        self.last_abstraction: Optional[Dict[str, Any]] = None\n",
        "        self.locked: bool = False\n",
        "\n",
        "    def _snap_to_gt_and_lock(self) -> None:\n",
        "        self.locked = True\n",
        "        self.last_abstraction = {\"temperature\": CFG.min_temperature, \"nudge\": None, \"force_gt\": self.gt, \"last_score\": 1.0}\n",
        "        self.mmr.record_lock_in(self.gt, self.gt)\n",
        "        print(\"[LOCK-IN] Snapping to canonical GT and freezing exploration.\")\n",
        "\n",
        "    def revise(self, theory: str, feedback: Optional[Feedback]) -> Tuple[str, float, Dict[str, Any]]:\n",
        "        # If already locked, keep returning canonical GT\n",
        "        if self.locked:\n",
        "            sim_res = self.simi.test(theory, self.gt)\n",
        "            score = 1.0 if theory.strip().lower() == self.gt.strip().lower() else self.ste.evaluate(theory, sim_res, self.gt)\n",
        "            return self.gt, score, {\"sim_res\": sim_res, \"locked_in\": True}\n",
        "\n",
        "        sim_res = self.simi.test(theory, self.gt)\n",
        "        score = self.ste.evaluate(theory, sim_res, self.gt)\n",
        "\n",
        "        # Exact match → lock\n",
        "        if theory.strip().lower() == self.gt.strip().lower():\n",
        "            self._snap_to_gt_and_lock()\n",
        "            return self.gt, 1.0, {\"sim_res\": sim_res, \"locked_in\": True}\n",
        "\n",
        "        # Full unique-token coverage with high score and no conflicting hint → lock\n",
        "        full_coverage = (sim_res[\"overlap\"] == sim_res[\"tokens_gt\"])\n",
        "        if full_coverage and score >= CFG.coverage_lock_score and (not feedback or not feedback.delta_hint):\n",
        "            self._snap_to_gt_and_lock()\n",
        "            return self.gt, 1.0, {\"sim_res\": sim_res, \"locked_in\": True}\n",
        "\n",
        "        # Otherwise adapt and generate\n",
        "        self.mmr.adapt(self, reason=score, feedback=feedback)\n",
        "        new_theory = self.tg.generate(self.last_abstraction or {})\n",
        "\n",
        "        # If generation naturally hits exact GT, lock immediately\n",
        "        if new_theory.strip().lower() == self.gt.strip().lower():\n",
        "            self._snap_to_gt_and_lock()\n",
        "            return self.gt, 1.0, {\"sim_res\": sim_res, \"locked_in\": True}\n",
        "\n",
        "        return new_theory, score, {\"sim_res\": sim_res, \"locked_in\": False}\n",
        "\n",
        "# -------------------------------\n",
        "# Peer validation and blending\n",
        "# -------------------------------\n",
        "\n",
        "def peer_validate(peer: str, theory: str, gt: str, t: int) -> Optional[Feedback]:\n",
        "    gt_tokens = gt.lower().split()\n",
        "    thy_tokens = set(theory.lower().split())\n",
        "    missing = [tok for tok in gt_tokens if tok not in thy_tokens]\n",
        "    if not missing:\n",
        "        return Feedback(peer, t, \"Looks aligned; keep consolidating.\", None, 0.8)\n",
        "\n",
        "    candidate = None\n",
        "    for tok in [\"quick\", \"brown\", \"fox\", \"jumps\", \"lazy\", \"dog\"]:\n",
        "        if tok in missing:\n",
        "            candidate = tok\n",
        "            break\n",
        "    candidate = candidate or random.choice(missing)\n",
        "    weight = max(0.4, 1.2 - 0.1 * t)\n",
        "    return Feedback(peer, t, f\"Missing token '{candidate}'. Consider incorporating it.\", candidate, weight)\n",
        "\n",
        "def blend_feedback(feedbacks: List[Feedback]) -> Optional[Feedback]:\n",
        "    if not feedbacks:\n",
        "        return None\n",
        "    # Weighted vote for delta_hint tokens\n",
        "    weights: Dict[str, float] = {}\n",
        "    for fb in feedbacks:\n",
        "        if fb.delta_hint:\n",
        "            weights[fb.delta_hint] = weights.get(fb.delta_hint, 0.0) + fb.weight\n",
        "    top_hint = max(weights, key=weights.get) if weights else None\n",
        "    avg_w = sum(fb.weight for fb in feedbacks) / len(feedbacks)\n",
        "    return Feedback(\n",
        "        from_peer=\"blend\",\n",
        "        t=max(fb.t for fb in feedbacks),\n",
        "        comment=f\"Blended hint={top_hint}\",\n",
        "        delta_hint=top_hint,\n",
        "        weight=avg_w\n",
        "    )\n",
        "\n",
        "# -------------------------------\n",
        "# History export\n",
        "# -------------------------------\n",
        "\n",
        "def export_history(mmr: \"MemoryModelReviser\", path: str) -> None:\n",
        "    try:\n",
        "        os.makedirs(os.path.dirname(path) or \".\", exist_ok=True)\n",
        "        base, ext = os.path.splitext(path)\n",
        "        if ext.lower() == \".json\":\n",
        "            with open(path, \"w\", encoding=\"utf-8\") as f:\n",
        "                json.dump(mmr.history, f, ensure_ascii=False, indent=2)\n",
        "            print(f\"[EXPORT] Wrote history JSON -> {path}\")\n",
        "        elif ext.lower() == \".csv\":\n",
        "            if not mmr.history:\n",
        "                with open(path, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
        "                    pass\n",
        "                print(f\"[EXPORT] No history; created empty CSV -> {path}\")\n",
        "                return\n",
        "            keys = sorted({k for row in mmr.history for k in row.keys()})\n",
        "            with open(path, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
        "                w = csv.DictWriter(f, fieldnames=keys)\n",
        "                w.writeheader()\n",
        "                for row in mmr.history:\n",
        "                    w.writerow(row)\n",
        "            print(f\"[EXPORT] Wrote history CSV -> {path}\")\n",
        "        else:\n",
        "            print(f\"[EXPORT] Unsupported extension for {path}; skipping.\")\n",
        "    except Exception as e:\n",
        "        print(f\"[EXPORT] Failed to write history: {e}\")\n",
        "\n",
        "# -------------------------------\n",
        "# Main\n",
        "# -------------------------------\n",
        "\n",
        "def parse_args(argv: Optional[List[str]] = None) -> argparse.Namespace:\n",
        "    p = argparse.ArgumentParser(description=\"Agent with lock-in, human-review decorator, and audit export.\")\n",
        "    p.add_argument(\"--steps\", type=int, default=CFG.steps, help=\"Number of time steps\")\n",
        "    p.add_argument(\"--seed\", type=int, default=CFG.seed, help=\"Random seed\")\n",
        "    p.add_argument(\"--blend-feedback\", action=\"store_true\", help=\"Blend peer feedback per step instead of sequential revises\")\n",
        "    p.add_argument(\"--history\", type=str, default=CFG.history_path, help=\"Path to write history (.json or .csv)\")\n",
        "    return p.parse_args(argv)\n",
        "\n",
        "def main(argv: Optional[List[str]] = None):\n",
        "    args = parse_args(argv)\n",
        "    random.seed(args.seed)\n",
        "\n",
        "    # Apply CLI overrides to CFG-like behavior (kept simple)\n",
        "    steps = int(args.steps)\n",
        "    history_path = args.history\n",
        "    use_blend = bool(args.blend_feedback)\n",
        "\n",
        "    X_true = \"the quick brown fox jumps over the lazy dog\"\n",
        "\n",
        "    tg = TheoryGenerator()\n",
        "    simi = SimilarityEngine()\n",
        "    ste = ScoringEngine()\n",
        "    mmr = MemoryModelReviser()\n",
        "    agent = Agent(tg=tg, simi=simi, ste=ste, mmr=mmr, gt=X_true)\n",
        "\n",
        "    theory = \"brown fox jumps beyond policy\"\n",
        "    peers = [\"alpha\", \"beta\", \"gamma\"]\n",
        "\n",
        "    print(\"[INIT] gt:\", X_true)\n",
        "    print(\"[INIT] theory0:\", theory)\n",
        "    print(\"-\" * 72)\n",
        "\n",
        "    for t in range(1, steps + 1):\n",
        "        print(f\"\\n[STEP] t={t}\")\n",
        "\n",
        "        if use_blend:\n",
        "            # Gather peer feedback first, blend, then revise once\n",
        "            batch = [peer_validate(peer, theory, X_true, t) for peer in peers]\n",
        "            batch = [b for b in batch if b]\n",
        "            for b in batch:\n",
        "                print(f\"[PEER] {b.from_peer}: {b.comment} (w={b.weight:.2f}, t={b.t}, hint={repr(b.delta_hint)})\")\n",
        "            fb = blend_feedback(batch)\n",
        "            theory, score, audit = agent.revise(theory, fb)\n",
        "            sim = audit[\"sim_res\"]\n",
        "            print(f\"[SCORE] jaccard={sim['jaccard']:.3f} overlap={sim['overlap']}/{sim['union']} theory:\")\n",
        "            print(\"        \", theory)\n",
        "        else:\n",
        "            # Sequential peer feedback with immediate revise after each\n",
        "            for peer in peers:\n",
        "                fb = peer_validate(peer, theory, X_true, t)\n",
        "                if not fb:\n",
        "                    print(f\"[PEER] {peer}: no feedback\")\n",
        "                    continue\n",
        "                print(f\"[PEER] {peer}: {fb.comment} (w={fb.weight:.2f}, t={fb.t}, hint={repr(fb.delta_hint)})\")\n",
        "                theory, score, audit = agent.revise(theory, fb)\n",
        "                sim = audit[\"sim_res\"]\n",
        "                print(f\"[SCORE] jaccard={sim['jaccard']:.3f} overlap={sim['overlap']}/{sim['union']} theory:\")\n",
        "                print(\"        \", theory)\n",
        "\n",
        "        # Skip novelty if locked to preserve exact GT\n",
        "        if agent.locked:\n",
        "            print(\"[LOCK-IN] Skipping novelty injection — theory matches GT.\")\n",
        "            continue\n",
        "\n",
        "        # Optional novelty/broadcast step between rounds\n",
        "        agent.last_abstraction = {\n",
        "            **(agent.last_abstraction or {}),\n",
        "            \"temperature\": min(1.0, (agent.last_abstraction or {}).get(\"temperature\", 0.2) + CFG.novelty_increment),\n",
        "        }\n",
        "        theory = agent.tg.generate(agent.last_abstraction or {})\n",
        "        print(f\"[NOVELTY] t={t} broadcast theory:\\n          \", theory)\n",
        "\n",
        "    print(\"\\n\" + \"-\" * 72)\n",
        "    print(\"[DONE] Final theory:\", theory)\n",
        "    print(\"[HISTORY] adapt events:\", len([h for h in mmr.history if h.get(\"event\") == \"adapt\"]))\n",
        "    print(\"[HISTORY] lock-in events:\", len([h for h in mmr.history if h.get(\"event\") == \"lock_in\"]))\n",
        "    if mmr.history:\n",
        "        last = mmr.history[-1]\n",
        "        if last.get(\"event\") == \"adapt\":\n",
        "            print(f\"         last adapt: score={last['score']:.3f} temp={last['temperature']:.2f} nudge={repr(last['nudge'])}\")\n",
        "        else:\n",
        "            print(\"         last event:\", last.get(\"event\"))\n",
        "\n",
        "    # Export audit history\n",
        "    export_history(mmr, history_path)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    sys.exit(main())"
      ]
    }
  ]
}