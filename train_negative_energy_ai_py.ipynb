{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyNWBPi4SV3FxvCWisVHabl+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OneFineStarstuff/Cosmic-Brilliance/blob/main/train_negative_energy_ai_py.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1vEY4m6y-AuZ"
      },
      "outputs": [],
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "train_negative_energy_ai.py\n",
        "\n",
        "Full pipeline for NegativeEnergyAI:\n",
        "  1. Synthetic dataset for Casimir force: (distance, permittivity) → force\n",
        "  2. Physics-informed loss: match true Casimir force in log-space\n",
        "  3. MLP with LayerNorm & Dropout for uncertainty\n",
        "  4. MC-Dropout inference to quantify predictive variance\n",
        "  5. Training loop with Adam, ReduceLROnPlateau, and early stopping\n",
        "  6. Evaluation: scatter, surface plots, and uncertainty heatmap\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.colors import LogNorm\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 1. True Casimir Force (vectorized)\n",
        "# ------------------------------------------------------------------------------\n",
        "def casimir_force(d, eps):\n",
        "    \"\"\"\n",
        "    Compute Casimir force for arrays of distances and permittivities.\n",
        "    Negative by definition.\n",
        "    \"\"\"\n",
        "    return - (np.pi**2) / (240.0 * d**4) * eps\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 2. Dataset: log-sampled distances, normalized for stability\n",
        "# ------------------------------------------------------------------------------\n",
        "class CasimirDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, n_dist=200, n_eps=200):\n",
        "        # distances in meters: 10nm → 1µm (log-uniform)\n",
        "        d = np.exp(np.linspace(np.log(1e-8), np.log(1e-6), n_dist))\n",
        "        e = np.linspace(1.0, 10.0, n_eps)\n",
        "        D, E = np.meshgrid(d, e)\n",
        "        X = np.stack([D.ravel(), E.ravel()], axis=1).astype(np.float32)\n",
        "        y = casimir_force(X[:,0], X[:,1]).astype(np.float32).reshape(-1,1)\n",
        "\n",
        "        # log-transform distance and force & normalize\n",
        "        log_d = np.log(X[:,0])\n",
        "        log_y = np.log(-y[:,0])  # log(|F|)\n",
        "\n",
        "        self.X = torch.tensor(\n",
        "            np.stack([\n",
        "                (log_d - log_d.mean())/log_d.std(),\n",
        "                (X[:,1] - X[:,1].mean())/X[:,1].std()\n",
        "            ], axis=1),\n",
        "            dtype=torch.float32\n",
        "        )\n",
        "        self.y = torch.tensor(\n",
        "            ((log_y - log_y.mean())/log_y.std()).reshape(-1,1),\n",
        "            dtype=torch.float32\n",
        "        )\n",
        "        self.orig = (D, E, y.reshape(D.shape))  # for plotting\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx], self.y[idx]\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 3. Model: NegativeEnergyAI with Dropout & LayerNorm\n",
        "# ------------------------------------------------------------------------------\n",
        "class NegativeEnergyAI(nn.Module):\n",
        "    def __init__(self, input_dim=2, hidden_dims=[64,64], output_dim=1, p_drop=0.1):\n",
        "        super().__init__()\n",
        "        layers = []\n",
        "        dim = input_dim\n",
        "        for h in hidden_dims:\n",
        "            layers += [\n",
        "                nn.Linear(dim, h),\n",
        "                nn.LayerNorm(h),\n",
        "                nn.ReLU(),\n",
        "                nn.Dropout(p_drop)\n",
        "            ]\n",
        "            dim = h\n",
        "        layers.append(nn.Linear(dim, output_dim))\n",
        "        self.net = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 4. Loss in log-space\n",
        "# ------------------------------------------------------------------------------\n",
        "def pinn_loss(pred, true):\n",
        "    \"\"\"\n",
        "    MSE in normalized log-space of |F|\n",
        "    \"\"\"\n",
        "    return nn.MSELoss()(pred, true)\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 5. MC-Dropout inference for uncertainty\n",
        "# ------------------------------------------------------------------------------\n",
        "def mc_dropout_predict(model, x, n_samples=100):\n",
        "    \"\"\"\n",
        "    Returns mean and std of model(x) in log-space.\n",
        "    \"\"\"\n",
        "    model.train()\n",
        "    preds = []\n",
        "    with torch.no_grad():\n",
        "        for _ in range(n_samples):\n",
        "            preds.append(model(x).cpu().numpy())\n",
        "    arr = np.stack(preds, axis=0)\n",
        "    return arr.mean(axis=0), arr.std(axis=0)\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 6. Training and Evaluation\n",
        "# ------------------------------------------------------------------------------\n",
        "def main():\n",
        "    # Hyperparams\n",
        "    batch_size = 256\n",
        "    lr         = 1e-3\n",
        "    max_epochs = 200\n",
        "    patience   = 15\n",
        "    device     = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # Data\n",
        "    dataset = CasimirDataset(n_dist=200, n_eps=200)\n",
        "    n_val    = int(0.2 * len(dataset))\n",
        "    n_train  = len(dataset) - n_val\n",
        "    train_ds, val_ds = random_split(dataset, [n_train, n_val])\n",
        "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
        "    val_loader   = DataLoader(val_ds,   batch_size=batch_size)\n",
        "\n",
        "    # Model\n",
        "    model = NegativeEnergyAI().to(device)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=1e-5)\n",
        "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
        "        optimizer, mode='min', factor=0.5, patience=5\n",
        "    )\n",
        "\n",
        "    best_val = float('inf')\n",
        "    wait      = 0\n",
        "    history  = {'train': [], 'val': []}\n",
        "\n",
        "    # Training loop\n",
        "    for epoch in range(1, max_epochs+1):\n",
        "        model.train()\n",
        "        train_loss = 0\n",
        "        for xb, yb in train_loader:\n",
        "            xb, yb = xb.to(device), yb.to(device)\n",
        "            pred   = model(xb)\n",
        "            loss   = pinn_loss(pred, yb)\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            train_loss += loss.item() * xb.size(0)\n",
        "\n",
        "        train_loss /= len(train_loader.dataset)\n",
        "\n",
        "        # validation\n",
        "        model.eval()\n",
        "        val_loss = 0\n",
        "        with torch.no_grad():\n",
        "            for xb, yb in val_loader:\n",
        "                xb, yb = xb.to(device), yb.to(device)\n",
        "                pred   = model(xb)\n",
        "                loss   = pinn_loss(pred, yb)\n",
        "                val_loss += loss.item() * xb.size(0)\n",
        "        val_loss /= len(val_loader.dataset)\n",
        "\n",
        "        scheduler.step(val_loss)\n",
        "        history['train'].append(train_loss)\n",
        "        history['val'].append(val_loss)\n",
        "\n",
        "        print(f\"Epoch {epoch:03d} | Train {train_loss:.4e} | Val {val_loss:.4e}\")\n",
        "\n",
        "        if val_loss < best_val:\n",
        "            best_val = val_loss\n",
        "            torch.save(model.state_dict(), \"best_neg_energy.pth\")\n",
        "            wait = 0\n",
        "        else:\n",
        "            wait += 1\n",
        "            if wait >= patience:\n",
        "                print(\"Early stopping\")\n",
        "                break\n",
        "\n",
        "    # load best\n",
        "    model.load_state_dict(torch.load(\"best_neg_energy.pth\"))\n",
        "\n",
        "    # Evaluate on full grid\n",
        "    D, E, F_true = dataset.orig\n",
        "    X_full = dataset.X.to(device)\n",
        "    mean_log_f, std_log_f = mc_dropout_predict(model, X_full, n_samples=200)\n",
        "    mean_f = np.exp(mean_log_f * dataset.y.std().item() + dataset.y.mean().item())\n",
        "    std_f  = std_log_f * dataset.y.std().item()  # approx error on log-space\n",
        "\n",
        "    # Scatter plot\n",
        "    F_mod = F_true.ravel()\n",
        "    plt.figure(figsize=(6,6))\n",
        "    plt.scatter(np.abs(F_mod), mean_f.ravel(), s=5, alpha=0.3)\n",
        "    lims = [F_mod.min(), F_mod.max()]\n",
        "    plt.plot(lims, lims, 'r--')\n",
        "    plt.xscale('log'); plt.yscale('log')\n",
        "    plt.xlabel(\"True |Casimir Force|\")\n",
        "    plt.ylabel(\"Predicted |Casimir Force|\")\n",
        "    plt.title(\"Log-Log Fit\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Heatmap of uncertainty\n",
        "    plt.figure(figsize=(6,5))\n",
        "    plt.pcolormesh(\n",
        "        np.log10(D*1e6), E,\n",
        "        std_f.reshape(D.shape).T,\n",
        "        shading='auto', cmap='magma'\n",
        "    )\n",
        "    plt.colorbar(label=\"Std(log|F|)\")\n",
        "    plt.xlabel(\"log10(Distance [µm])\")\n",
        "    plt.ylabel(\"Permittivity\")\n",
        "    plt.title(\"Prediction Uncertainty\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ]
}