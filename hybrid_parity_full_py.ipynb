{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyMGiI8cjwhR3M36XpoYZ6vO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OneFineStarstuff/Cosmic-Brilliance/blob/main/hybrid_parity_full_py.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pennylane torch tqdm"
      ],
      "metadata": {
        "id": "TrSB9qUGdwOC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RVgyUtd2dY7X"
      },
      "outputs": [],
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "hybrid_parity_full.py\n",
        "\n",
        "A self-contained script that trains and evaluates an n-bit parity classifier\n",
        "using a hybrid quantum-classical model with:\n",
        "\n",
        "  1. MC-Dropout for uncertainty quantification\n",
        "  2. Temperature scaling + reliability diagram calibration\n",
        "  3. Noise injection via default.mixed device\n",
        "  4. Scaling to arbitrary n-qubit parity via CLI args\n",
        "\n",
        "Unknown args (like Jupyter’s “-f …json”) are silently ignored.\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import argparse\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "import pennylane as qml\n",
        "from pennylane import numpy as np\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "def parse_args():\n",
        "    p = argparse.ArgumentParser()\n",
        "    p.add_argument(\"--n_qubits\",    type=int,   default=4)\n",
        "    p.add_argument(\"--n_layers\",    type=int,   default=6)\n",
        "    p.add_argument(\"--hidden_dim\",  type=int,   default=32)\n",
        "    p.add_argument(\"--dropout_p\",   type=float, default=0.1)\n",
        "    p.add_argument(\"--mc_samples\",  type=int,   default=20)\n",
        "    p.add_argument(\n",
        "        \"--noise_model\",\n",
        "        type=str,\n",
        "        choices=[\"none\", \"depolarizing\", \"amplitude_damping\"],\n",
        "        default=\"none\",\n",
        "    )\n",
        "    p.add_argument(\"--noise_prob\",  type=float, default=0.01)\n",
        "    p.add_argument(\"--batch_size\",  type=int,   default=32)\n",
        "    p.add_argument(\"--epochs\",      type=int,   default=20)\n",
        "    p.add_argument(\"--lr\",          type=float, default=0.005)\n",
        "    p.add_argument(\"--calibrate\",   action=\"store_true\")\n",
        "    p.add_argument(\"--output_dir\",  type=str,   default=\"results/\")\n",
        "\n",
        "    # parse_known_args(): ignore any extra flags (e.g., Jupyter’s “-f”)\n",
        "    args, _ = p.parse_known_args()\n",
        "    return args\n",
        "\n",
        "\n",
        "def generate_parity_data(n_samples, n_qubits):\n",
        "    X = np.random.randint(0, 2, size=(n_samples, n_qubits))\n",
        "    X = 2 * X - 1  # map {0,1} → {-1,+1}\n",
        "    y = (np.sum(X == 1, axis=1) % 2).astype(int)\n",
        "    return (\n",
        "        torch.tensor(X, dtype=torch.float32),\n",
        "        torch.tensor(y, dtype=torch.long),\n",
        "    )\n",
        "\n",
        "\n",
        "def create_device(n_qubits, noise_model, noise_prob):\n",
        "    if noise_model == \"none\":\n",
        "        return qml.device(\"default.qubit\", wires=n_qubits)\n",
        "    # default.mixed for noise\n",
        "    return qml.device(\n",
        "        \"default.mixed\",\n",
        "        wires=n_qubits,\n",
        "        noise= qml.DepolarizingChannel\n",
        "               if noise_model == \"depolarizing\"\n",
        "               else qml.AmplitudeDamping,\n",
        "        prob=noise_prob,\n",
        "    )\n",
        "\n",
        "\n",
        "def create_qnode(dev, n_qubits, n_layers):\n",
        "    @qml.qnode(dev, interface=\"torch\", diff_method=\"backprop\")\n",
        "    def qnode(inputs, weights):\n",
        "        qml.templates.AngleEmbedding(\n",
        "            inputs, wires=range(n_qubits), rotation=\"X\"\n",
        "        )\n",
        "        # Remove `reps` keyword; the shape of `weights` encodes layers:\n",
        "        qml.templates.BasicEntanglerLayers(\n",
        "            weights, wires=range(n_qubits)\n",
        "        )\n",
        "        return [qml.expval(qml.PauliZ(i)) for i in range(n_qubits)]\n",
        "\n",
        "    return qnode\n",
        "\n",
        "\n",
        "class HybridParityModel(nn.Module):\n",
        "    def __init__(self, n_qubits, n_layers, hidden_dim, dropout_p, dev):\n",
        "        super().__init__()\n",
        "        qnode = create_qnode(dev, n_qubits, n_layers)\n",
        "        weight_shapes = {\"weights\": (n_layers, n_qubits)}\n",
        "        self.qlayer    = qml.qnn.TorchLayer(qnode, weight_shapes)\n",
        "\n",
        "        self.res_fc1    = nn.Linear(n_qubits, hidden_dim)\n",
        "        self.dropout    = nn.Dropout(dropout_p)\n",
        "        self.res_fc2    = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.classifier = nn.Linear(hidden_dim, 2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        q_out = self.qlayer(x)\n",
        "        h     = F.relu(self.res_fc1(q_out))\n",
        "        h     = self.dropout(h)\n",
        "        h_res = F.relu(self.res_fc2(h))\n",
        "        h     = h + h_res            # out-of-place residual\n",
        "        return self.classifier(h)\n",
        "\n",
        "\n",
        "def temperature_scaling(logits, labels, device):\n",
        "    T = torch.ones(1, requires_grad=True, device=device)\n",
        "    nll = nn.CrossEntropyLoss()\n",
        "    opt = optim.LBFGS([T], lr=0.1, max_iter=50)\n",
        "\n",
        "    def closure():\n",
        "        opt.zero_grad()\n",
        "        loss = nll(logits / T, labels)\n",
        "        loss.backward()\n",
        "        return loss\n",
        "\n",
        "    opt.step(closure)\n",
        "    return T.item()\n",
        "\n",
        "\n",
        "def plot_reliability_diagram(probs, labels, n_bins, save_path):\n",
        "    confs = probs.max(1).values.detach().cpu().numpy()\n",
        "    preds = probs.argmax(1).detach().cpu().numpy()\n",
        "    truths = labels.detach().cpu().numpy()\n",
        "\n",
        "    bins        = np.linspace(0, 1, n_bins + 1)\n",
        "    bin_centers = (bins[:-1] + bins[1:]) / 2\n",
        "    accs, avg_conf = [], []\n",
        "\n",
        "    for i in range(n_bins):\n",
        "        mask = (confs >= bins[i]) & (confs < bins[i + 1])\n",
        "        if mask.sum() > 0:\n",
        "            accs.append((preds[mask] == truths[mask]).mean())\n",
        "            avg_conf.append(confs[mask].mean())\n",
        "        else:\n",
        "            accs.append(0.0)\n",
        "            avg_conf.append(0.0)\n",
        "\n",
        "    plt.figure(figsize=(5, 5))\n",
        "    plt.plot(bin_centers, avg_conf, 's-', label=\"Confidence\")\n",
        "    plt.plot(bin_centers, accs, 'o-', label=\"Accuracy\")\n",
        "    plt.plot([0,1], [0,1], '--', color='gray', label=\"Ideal\")\n",
        "    plt.xlabel(\"Confidence\")\n",
        "    plt.ylabel(\"Accuracy\")\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(save_path)\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "def main():\n",
        "    args = parse_args()\n",
        "    os.makedirs(args.output_dir, exist_ok=True)\n",
        "\n",
        "    device = create_device(\n",
        "        args.n_qubits, args.noise_model, args.noise_prob\n",
        "    )\n",
        "\n",
        "    model = HybridParityModel(\n",
        "        n_qubits   = args.n_qubits,\n",
        "        n_layers   = args.n_layers,\n",
        "        hidden_dim = args.hidden_dim,\n",
        "        dropout_p  = args.dropout_p,\n",
        "        dev        = device,\n",
        "    )\n",
        "\n",
        "    optimizer = optim.Adam(model.parameters(), lr=args.lr)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # datasets: train/calibrate/test\n",
        "    X_train, y_train = generate_parity_data(2000, args.n_qubits)\n",
        "    X_calib, y_calib = generate_parity_data(500,  args.n_qubits)\n",
        "    X_test,  y_test  = generate_parity_data(500,  args.n_qubits)\n",
        "\n",
        "    train_loader = DataLoader(\n",
        "        TensorDataset(X_train, y_train),\n",
        "        batch_size=args.batch_size, shuffle=True\n",
        "    )\n",
        "    test_loader  = DataLoader(\n",
        "        TensorDataset(X_test,  y_test),\n",
        "        batch_size=args.batch_size\n",
        "    )\n",
        "\n",
        "    # 1) Train\n",
        "    for epoch in range(1, args.epochs + 1):\n",
        "        model.train()\n",
        "        running_loss, correct = 0.0, 0\n",
        "\n",
        "        for Xb, yb in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            logits = model(Xb)\n",
        "            loss   = criterion(logits, yb)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item() * Xb.size(0)\n",
        "            correct      += (logits.argmax(1) == yb).sum().item()\n",
        "\n",
        "        train_acc  = correct / len(train_loader.dataset)\n",
        "        train_loss = running_loss / len(train_loader.dataset)\n",
        "\n",
        "        # quick test accuracy\n",
        "        model.eval()\n",
        "        test_corr = 0\n",
        "        with torch.no_grad():\n",
        "            for Xb, yb in test_loader:\n",
        "                test_corr += (model(Xb).argmax(1) == yb).sum().item()\n",
        "        test_acc = test_corr / len(test_loader.dataset)\n",
        "\n",
        "        print(\n",
        "            f\"Epoch {epoch:2d} | Loss: {train_loss:.4f} | \"\n",
        "            f\"Train Acc: {train_acc:.3f} | Test Acc: {test_acc:.3f}\"\n",
        "        )\n",
        "\n",
        "    # 2) MC-Dropout\n",
        "    if args.mc_samples > 0:\n",
        "        model.train()  # keep dropout active\n",
        "        mc_probs = []\n",
        "        with torch.no_grad():\n",
        "            for _ in range(args.mc_samples):\n",
        "                batch_probs = []\n",
        "                for Xb, _ in test_loader:\n",
        "                    ps = F.softmax(model(Xb), dim=1)\n",
        "                    batch_probs.append(ps)\n",
        "                mc_probs.append(torch.cat(batch_probs, dim=0))\n",
        "        mc_stack = torch.stack(mc_probs)  # [mc, N, 2]\n",
        "        mean_p   = mc_stack.mean(0)\n",
        "        var_p    = mc_stack.var(0).mean(1)\n",
        "\n",
        "        plt.hist(var_p.cpu(), bins=20)\n",
        "        plt.xlabel(\"Predictive Variance\")\n",
        "        plt.ylabel(\"Count\")\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(os.path.join(args.output_dir, \"mc_variance.png\"))\n",
        "        plt.close()\n",
        "        print(\"Saved MC-Dropout variance histogram\")\n",
        "\n",
        "    # 3) Calibration\n",
        "    if args.calibrate:\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            logits_calib = model(X_calib)\n",
        "        T_opt = temperature_scaling(logits_calib, y_calib, logits_calib.device)\n",
        "        print(f\"Fitted temperature: {T_opt:.3f}\")\n",
        "\n",
        "        with torch.no_grad():\n",
        "            logits_test = model(X_test) / T_opt\n",
        "            probs_test  = F.softmax(logits_test, dim=1)\n",
        "        plot_reliability_diagram(\n",
        "            probs_test, y_test, n_bins=10,\n",
        "            save_path=os.path.join(args.output_dir, \"reliability.png\")\n",
        "        )\n",
        "        print(\"Saved reliability diagram\")\n",
        "\n",
        "    # 4) Final Test Acc\n",
        "    model.eval()\n",
        "    final_corr = 0\n",
        "    with torch.no_grad():\n",
        "        for Xb, yb in test_loader:\n",
        "            final_corr += (model(Xb).argmax(1) == yb).sum().item()\n",
        "    print(f\"Final Test Accuracy: {final_corr / len(test_loader.dataset):.3f}\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ]
}