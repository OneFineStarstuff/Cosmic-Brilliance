{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyMnnV3xRW3qVJ89JDAxtAo0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OneFineStarstuff/Cosmic-Brilliance/blob/main/metaintel_cli_py.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch torchvision torchaudio scikit-learn matplotlib pandas seaborn onnx onnxruntime pyyaml"
      ],
      "metadata": {
        "id": "Et4L76KHSNa-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pua9kY5_5ngx"
      },
      "outputs": [],
      "source": [
        "#!/usr/bin/env python3\n",
        "# -*- coding: utf-8 -*-\n",
        "\n",
        "\"\"\"\n",
        "MetaIntelligence CLI: train, eval, predict, benchmark, and ONNX export\n",
        "Datasets: moons, blobs, circles, iris, mnist\n",
        "Models: MLP (all), CNN (mnist)\n",
        "Notebook/Colab-safe: functions and main guard\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import json\n",
        "import math\n",
        "import time\n",
        "import yaml\n",
        "import argparse\n",
        "import random\n",
        "import datetime as dt\n",
        "from dataclasses import dataclass, asdict\n",
        "from typing import Tuple, Dict, Any, Optional, List\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib\n",
        "# Default to non-interactive backend for CLI; enable live plotting with flag\n",
        "matplotlib.use(\"Agg\")\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score,\n",
        "    precision_recall_fscore_support,\n",
        "    confusion_matrix,\n",
        "    classification_report,\n",
        ")\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import make_moons, make_blobs, make_circles, load_iris\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
        "\n",
        "# Optional ONNX runtime\n",
        "try:\n",
        "    import onnx\n",
        "    import onnxruntime as ort\n",
        "    HAS_ONNX = True\n",
        "except Exception:\n",
        "    HAS_ONNX = False\n",
        "\n",
        "# Optional MNIST\n",
        "try:\n",
        "    from torchvision import datasets, transforms\n",
        "    HAS_TORCHVISION = True\n",
        "except Exception:\n",
        "    HAS_TORCHVISION = False\n",
        "\n",
        "# ---------------------------\n",
        "# Utilities\n",
        "# ---------------------------\n",
        "\n",
        "def set_seed(seed: int, deterministic: bool = True):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    if deterministic:\n",
        "        torch.backends.cudnn.deterministic = True\n",
        "        torch.backends.cudnn.benchmark = False\n",
        "\n",
        "def now_tag():\n",
        "    return dt.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
        "\n",
        "def ensure_dir(path: str):\n",
        "    os.makedirs(path, exist_ok=True)\n",
        "\n",
        "def save_json(path: str, obj: Dict[str, Any]):\n",
        "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(obj, f, indent=2, sort_keys=True)\n",
        "\n",
        "def load_json(path: str) -> Dict[str, Any]:\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "        return json.load(f)\n",
        "\n",
        "def maybe_load_config(path: Optional[str]) -> Dict[str, Any]:\n",
        "    if not path:\n",
        "        return {}\n",
        "    ext = os.path.splitext(path)[1].lower()\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "        if ext in [\".yaml\", \".yml\"]:\n",
        "            return yaml.safe_load(f)\n",
        "        elif ext == \".json\":\n",
        "            return json.load(f)\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported config extension: {ext}\")\n",
        "\n",
        "def pretty_time(s: float) -> str:\n",
        "    if s < 60:\n",
        "        return f\"{s:.2f}s\"\n",
        "    m, sec = divmod(s, 60)\n",
        "    if m < 60:\n",
        "        return f\"{int(m)}m {sec:.1f}s\"\n",
        "    h, m = divmod(m, 60)\n",
        "    return f\"{int(h)}h {int(m)}m {sec:.0f}s\"\n",
        "\n",
        "# ---------------------------\n",
        "# Data loading\n",
        "# ---------------------------\n",
        "\n",
        "@dataclass\n",
        "class DataBundle:\n",
        "    X_train: np.ndarray\n",
        "    y_train: np.ndarray\n",
        "    X_val: np.ndarray\n",
        "    y_val: np.ndarray\n",
        "    X_test: np.ndarray\n",
        "    y_test: np.ndarray\n",
        "    input_shape: Tuple[int, ...]\n",
        "    num_classes: int\n",
        "    class_names: List[str]\n",
        "    scaler_mean: Optional[List[float]] = None\n",
        "    scaler_std: Optional[List[float]] = None\n",
        "    is_image: bool = False  # True for MNIST\n",
        "\n",
        "def make_synthetic(\n",
        "    kind: str,\n",
        "    n_samples: int = 2000,\n",
        "    noise: float = 0.2,\n",
        "    random_state: int = 42,\n",
        ") -> Tuple[np.ndarray, np.ndarray, List[str]]:\n",
        "    if kind == \"moons\":\n",
        "        X, y = make_moons(n_samples=n_samples, noise=noise, random_state=random_state)\n",
        "        classes = [\"class0\", \"class1\"]\n",
        "    elif kind == \"blobs\":\n",
        "        X, y = make_blobs(n_samples=n_samples, centers=3, cluster_std=1.5, random_state=random_state)\n",
        "        classes = [\"c0\", \"c1\", \"c2\"]\n",
        "    elif kind == \"circles\":\n",
        "        X, y = make_circles(n_samples=n_samples, noise=noise, factor=0.5, random_state=random_state)\n",
        "        classes = [\"inner\", \"outer\"]\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown synthetic dataset: {kind}\")\n",
        "    return X.astype(np.float32), y.astype(np.int64), classes\n",
        "\n",
        "def load_iris_dataset() -> Tuple[np.ndarray, np.ndarray, List[str]]:\n",
        "    iris = load_iris()\n",
        "    X = iris[\"data\"].astype(np.float32)\n",
        "    y = iris[\"target\"].astype(np.int64)\n",
        "    classes = list(iris[\"target_names\"])\n",
        "    return X, y, classes\n",
        "\n",
        "def load_mnist_dataset(data_dir: str, download: bool, seed: int) -> Tuple[TensorDataset, TensorDataset, TensorDataset, List[str]]:\n",
        "    if not HAS_TORCHVISION:\n",
        "        raise RuntimeError(\"torchvision not available. Install torchvision to use MNIST.\")\n",
        "    transform = transforms.Compose([transforms.ToTensor()])\n",
        "    train_full = datasets.MNIST(root=data_dir, train=True, download=download, transform=transform)\n",
        "    test_ds = datasets.MNIST(root=data_dir, train=False, download=download, transform=transform)\n",
        "\n",
        "    # Split train into train/val\n",
        "    val_size = int(0.1 * len(train_full))\n",
        "    train_size = len(train_full) - val_size\n",
        "    generator = torch.Generator().manual_seed(seed)\n",
        "    train_ds, val_ds = random_split(train_full, [train_size, val_size], generator=generator)\n",
        "    classes = [str(i) for i in range(10)]\n",
        "    return train_ds, val_ds, test_ds, classes\n",
        "\n",
        "def build_tabular_bundle(\n",
        "    X: np.ndarray,\n",
        "    y: np.ndarray,\n",
        "    class_names: List[str],\n",
        "    test_size: float,\n",
        "    val_size: float,\n",
        "    seed: int,\n",
        "    standardize: bool = True,\n",
        ") -> DataBundle:\n",
        "    X_train, X_hold, y_train, y_hold = train_test_split(\n",
        "        X, y, test_size=test_size, random_state=seed, stratify=y\n",
        "    )\n",
        "    X_val, X_test, y_val, y_test = train_test_split(\n",
        "        X_hold, y_hold, test_size=0.5, random_state=seed, stratify=y_hold\n",
        "    )\n",
        "    scaler_mean = None\n",
        "    scaler_std = None\n",
        "    if standardize:\n",
        "        scaler = StandardScaler()\n",
        "        X_train = scaler.fit_transform(X_train).astype(np.float32)\n",
        "        X_val = scaler.transform(X_val).astype(np.float32)\n",
        "        X_test = scaler.transform(X_test).astype(np.float32)\n",
        "        scaler_mean = scaler.mean_.astype(np.float32).tolist()\n",
        "        scaler_std = scaler.scale_.astype(np.float32).tolist()\n",
        "\n",
        "    input_shape = (X.shape[1],)\n",
        "    num_classes = len(np.unique(y))\n",
        "    return DataBundle(\n",
        "        X_train, y_train, X_val, y_val, X_test, y_test,\n",
        "        input_shape=input_shape, num_classes=num_classes,\n",
        "        class_names=class_names,\n",
        "        scaler_mean=scaler_mean, scaler_std=scaler_std,\n",
        "        is_image=False\n",
        "    )\n",
        "\n",
        "# ---------------------------\n",
        "# Models\n",
        "# ---------------------------\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, in_dim: int, num_classes: int, hidden_sizes: List[int], dropout: float = 0.0, batchnorm: bool = False):\n",
        "        super().__init__()\n",
        "        layers = []\n",
        "        prev = in_dim\n",
        "        for h in hidden_sizes:\n",
        "            layers.append(nn.Linear(prev, h))\n",
        "            if batchnorm:\n",
        "                layers.append(nn.BatchNorm1d(h))\n",
        "            layers.append(nn.ReLU())\n",
        "            if dropout > 0:\n",
        "                layers.append(nn.Dropout(dropout))\n",
        "            prev = h\n",
        "        layers.append(nn.Linear(prev, num_classes))\n",
        "        self.net = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class SimpleCNN(nn.Module):\n",
        "    # For MNIST 1x28x28\n",
        "    def __init__(self, num_classes: int = 10, dropout: float = 0.0):\n",
        "        super().__init__()\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(1, 32, 3, padding=1), nn.ReLU(),\n",
        "            nn.MaxPool2d(2),  # 14x14\n",
        "            nn.Conv2d(32, 64, 3, padding=1), nn.ReLU(),\n",
        "            nn.MaxPool2d(2),  # 7x7\n",
        "        )\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(64 * 7 * 7, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(128, num_classes),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        return self.fc(x)\n",
        "\n",
        "# ---------------------------\n",
        "# Training / Evaluation\n",
        "# ---------------------------\n",
        "\n",
        "def make_device(device_str: str) -> torch.device:\n",
        "    if device_str == \"auto\":\n",
        "        return torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    return torch.device(device_str)\n",
        "\n",
        "def to_tensor(x: np.ndarray) -> torch.Tensor:\n",
        "    return torch.from_numpy(x)\n",
        "\n",
        "def accuracy_from_logits(logits: torch.Tensor, y: torch.Tensor) -> float:\n",
        "    preds = logits.argmax(dim=1)\n",
        "    return (preds == y).float().mean().item()\n",
        "\n",
        "def plot_training_curves(history: pd.DataFrame, out_png: str):\n",
        "    plt.figure(figsize=(10, 4))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(history[\"epoch\"], history[\"train_loss\"], label=\"train\")\n",
        "    plt.plot(history[\"epoch\"], history[\"val_loss\"], label=\"val\")\n",
        "    plt.xlabel(\"epoch\"); plt.ylabel(\"loss\"); plt.legend(); plt.title(\"Loss\")\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(history[\"epoch\"], history[\"train_acc\"], label=\"train\")\n",
        "    plt.plot(history[\"epoch\"], history[\"val_acc\"], label=\"val\")\n",
        "    plt.xlabel(\"epoch\"); plt.ylabel(\"accuracy\"); plt.legend(); plt.title(\"Accuracy\")\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(out_png, dpi=150)\n",
        "    plt.close()\n",
        "\n",
        "def plot_confusion(cm: np.ndarray, class_names: List[str], out_png: str):\n",
        "    plt.figure(figsize=(6, 5))\n",
        "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=class_names, yticklabels=class_names)\n",
        "    plt.ylabel(\"True\"); plt.xlabel(\"Pred\")\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(out_png, dpi=150)\n",
        "    plt.close()\n",
        "\n",
        "def plot_decision_boundary(model: nn.Module, bundle: DataBundle, device: torch.device, out_png: str):\n",
        "    # Only for 2D tabular\n",
        "    if bundle.input_shape[0] != 2 or bundle.is_image:\n",
        "        return\n",
        "    x_min, x_max = bundle.X_train[:, 0].min() - 0.5, bundle.X_train[:, 0].max() + 0.5\n",
        "    y_min, y_max = bundle.X_train[:, 1].min() - 0.5, bundle.X_train[:, 1].max() + 0.5\n",
        "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 300), np.linspace(y_min, y_max, 300))\n",
        "    grid = np.c_[xx.ravel(), yy.ravel()].astype(np.float32)\n",
        "\n",
        "    # If standardized, standardize the grid using training stats\n",
        "    if bundle.scaler_mean is not None and bundle.scaler_std is not None:\n",
        "        grid = (grid - np.array(bundle.scaler_mean)) / np.array(bundle.scaler_std)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        g = to_tensor(grid).to(device)\n",
        "        logits = model(g)\n",
        "        preds = logits.argmax(dim=1).cpu().numpy().reshape(xx.shape)\n",
        "\n",
        "    plt.figure(figsize=(6, 5))\n",
        "    plt.contourf(xx, yy, preds, alpha=0.25, levels=np.arange(bundle.num_classes + 1) - 0.5, cmap=\"Set3\")\n",
        "    # Plot training points (de-standardize back for visualization)\n",
        "    X_plot = bundle.X_train\n",
        "    if bundle.scaler_mean is not None and bundle.scaler_std is not None:\n",
        "        X_plot = X_plot * np.array(bundle.scaler_std) + np.array(bundle.scaler_mean)\n",
        "    scatter = plt.scatter(X_plot[:, 0], X_plot[:, 1], c=bundle.y_train, cmap=\"Set1\", edgecolor=\"k\", s=20)\n",
        "    plt.title(\"Decision boundary (train)\")\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(out_png, dpi=150)\n",
        "    plt.close()\n",
        "\n",
        "def train_one_epoch(model, loader, optimizer, device, loss_fn):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    total_acc = 0.0\n",
        "    count = 0\n",
        "    for xb, yb in loader:\n",
        "        xb, yb = xb.to(device), yb.to(device)\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        out = model(xb)\n",
        "        loss = loss_fn(out, yb)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        acc = accuracy_from_logits(out, yb)\n",
        "        bsz = yb.size(0)\n",
        "        total_loss += loss.item() * bsz\n",
        "        total_acc += acc * bsz\n",
        "        count += bsz\n",
        "    return total_loss / count, total_acc / count\n",
        "\n",
        "@torch.no_grad()\n",
        "def eval_epoch(model, loader, device, loss_fn):\n",
        "    model.eval()\n",
        "    total_loss = 0.0\n",
        "    total_acc = 0.0\n",
        "    count = 0\n",
        "    for xb, yb in loader:\n",
        "        xb, yb = xb.to(device), yb.to(device)\n",
        "        out = model(xb)\n",
        "        loss = loss_fn(out, yb)\n",
        "        acc = accuracy_from_logits(out, yb)\n",
        "        bsz = yb.size(0)\n",
        "        total_loss += loss.item() * bsz\n",
        "        total_acc += acc * bsz\n",
        "        count += bsz\n",
        "    return total_loss / count, total_acc / count\n",
        "\n",
        "def build_model(model_name: str, input_shape: Tuple[int, ...], num_classes: int, hidden_sizes: List[int], dropout: float, batchnorm: bool):\n",
        "    if model_name == \"mlp\":\n",
        "        return MLP(in_dim=input_shape[0], num_classes=num_classes, hidden_sizes=hidden_sizes, dropout=dropout, batchnorm=batchnorm)\n",
        "    elif model_name == \"cnn\":\n",
        "        return SimpleCNN(num_classes=num_classes, dropout=dropout)\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown model: {model_name}\")\n",
        "\n",
        "def make_dataloaders(bundle: DataBundle, is_image: bool, batch_size: int) -> Tuple[DataLoader, DataLoader, DataLoader]:\n",
        "    if not is_image:\n",
        "        X_tr = torch.from_numpy(bundle.X_train)\n",
        "        y_tr = torch.from_numpy(bundle.y_train)\n",
        "        X_va = torch.from_numpy(bundle.X_val)\n",
        "        y_va = torch.from_numpy(bundle.y_val)\n",
        "        X_te = torch.from_numpy(bundle.X_test)\n",
        "        y_te = torch.from_numpy(bundle.y_test)\n",
        "        train_ds = TensorDataset(X_tr, y_tr)\n",
        "        val_ds = TensorDataset(X_va, y_va)\n",
        "        test_ds = TensorDataset(X_te, y_te)\n",
        "    else:\n",
        "        # MNIST: already TensorDataset from torchvision splits\n",
        "        raise RuntimeError(\"For MNIST, use dedicated make_mnist_loaders.\")\n",
        "\n",
        "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, drop_last=False)\n",
        "    val_loader   = DataLoader(val_ds, batch_size=batch_size, shuffle=False)\n",
        "    test_loader  = DataLoader(test_ds, batch_size=batch_size, shuffle=False)\n",
        "    return train_loader, val_loader, test_loader\n",
        "\n",
        "def make_mnist_loaders(train_ds, val_ds, test_ds, batch_size: int) -> Tuple[DataLoader, DataLoader, DataLoader]:\n",
        "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, drop_last=False)\n",
        "    val_loader   = DataLoader(val_ds, batch_size=batch_size, shuffle=False)\n",
        "    test_loader  = DataLoader(test_ds, batch_size=batch_size, shuffle=False)\n",
        "    return train_loader, val_loader, test_loader\n",
        "\n",
        "def collect_preds(model, loader, device) -> Tuple[np.ndarray, np.ndarray]:\n",
        "    model.eval()\n",
        "    all_logits = []\n",
        "    all_targets = []\n",
        "    with torch.no_grad():\n",
        "        for xb, yb in loader:\n",
        "            xb = xb.to(device)\n",
        "            logits = model(xb)\n",
        "            all_logits.append(logits.cpu().numpy())\n",
        "            all_targets.append(yb.numpy())\n",
        "    logits = np.concatenate(all_logits, axis=0)\n",
        "    targets = np.concatenate(all_targets, axis=0)\n",
        "    preds = logits.argmax(axis=1)\n",
        "    return preds, targets\n",
        "\n",
        "# ---------------------------\n",
        "# Config dataclass\n",
        "# ---------------------------\n",
        "\n",
        "@dataclass\n",
        "class TrainConfig:\n",
        "    dataset: str = \"moons\"        # moons|blobs|circles|iris|mnist\n",
        "    model: str = \"mlp\"            # mlp|cnn(mnist only)\n",
        "    data_dir: str = \"./data\"      # for mnist\n",
        "    download: bool = True         # for mnist\n",
        "    test_size: float = 0.2\n",
        "    val_frac: float = 0.5         # internal val split from the hold-out; used only for tabular\n",
        "    batch_size: int = 64\n",
        "    epochs: int = 50\n",
        "    lr: float = 1e-3\n",
        "    weight_decay: float = 0.0\n",
        "    hidden_sizes: Tuple[int, ...] = (64, 64)\n",
        "    dropout: float = 0.0\n",
        "    batchnorm: bool = False\n",
        "    seed: int = 42\n",
        "    deterministic: bool = True\n",
        "    device: str = \"auto\"          # auto|cpu|cuda\n",
        "    live_plot: bool = False\n",
        "    standardize: bool = True\n",
        "    noise: float = 0.2            # for synthetic\n",
        "    n_samples: int = 2000         # for synthetic\n",
        "    # early stopping\n",
        "    early_stop: bool = True\n",
        "    patience: int = 10\n",
        "    min_delta: float = 1e-4\n",
        "\n",
        "    # misc\n",
        "    run_dir: Optional[str] = None\n",
        "\n",
        "    def as_dict(self) -> Dict[str, Any]:\n",
        "        d = asdict(self)\n",
        "        d[\"hidden_sizes\"] = list(self.hidden_sizes)\n",
        "        return d\n",
        "\n",
        "# ---------------------------\n",
        "# Live plotting helper\n",
        "# ---------------------------\n",
        "\n",
        "class LivePlotter:\n",
        "    def __init__(self, enable: bool = False):\n",
        "        self.enable = enable\n",
        "        if self.enable:\n",
        "            matplotlib.use(\"TkAgg\")\n",
        "            plt.ion()\n",
        "            self.fig, (self.ax1, self.ax2) = plt.subplots(1, 2, figsize=(10, 4))\n",
        "            self.ax1.set_title(\"Loss\"); self.ax2.set_title(\"Accuracy\")\n",
        "            self.ax1.set_xlabel(\"epoch\"); self.ax2.set_xlabel(\"epoch\")\n",
        "            self.ax1.grid(True); self.ax2.grid(True)\n",
        "            self.train_loss_line, = self.ax1.plot([], [], label=\"train\")\n",
        "            self.val_loss_line,   = self.ax1.plot([], [], label=\"val\")\n",
        "            self.ax1.legend()\n",
        "            self.train_acc_line, = self.ax2.plot([], [], label=\"train\")\n",
        "            self.val_acc_line,   = self.ax2.plot([], [], label=\"val\")\n",
        "            self.ax2.legend()\n",
        "            self.fig.tight_layout()\n",
        "\n",
        "    def update(self, history: pd.DataFrame):\n",
        "        if not self.enable:\n",
        "            return\n",
        "        x = history[\"epoch\"].values\n",
        "        self.train_loss_line.set_data(x, history[\"train_loss\"].values)\n",
        "        self.val_loss_line.set_data(x, history[\"val_loss\"].values)\n",
        "        self.ax1.relim(); self.ax1.autoscale_view()\n",
        "\n",
        "        self.train_acc_line.set_data(x, history[\"train_acc\"].values)\n",
        "        self.val_acc_line.set_data(x, history[\"val_acc\"].values)\n",
        "        self.ax2.relim(); self.ax2.autoscale_view()\n",
        "\n",
        "        plt.pause(0.001)\n",
        "\n",
        "    def close(self):\n",
        "        if self.enable:\n",
        "            plt.ioff()\n",
        "            plt.close(self.fig)\n",
        "\n",
        "# ---------------------------\n",
        "# Train command\n",
        "# ---------------------------\n",
        "\n",
        "def cmd_train(args: argparse.Namespace):\n",
        "    # 1) Load defaults and optional config file\n",
        "    base_cfg = TrainConfig()\n",
        "    file_cfg = maybe_load_config(getattr(args, \"config\", None))\n",
        "    # Merge: defaults <- file_cfg (if any)\n",
        "    cfg_dict = {**base_cfg.as_dict(), **(file_cfg or {})}\n",
        "\n",
        "    # 2) CLI overrides (only when provided)\n",
        "    override_map = {\n",
        "        \"dataset\": \"dataset\",\n",
        "        \"model\": \"model\",\n",
        "        \"data_dir\": \"data_dir\",\n",
        "        \"download\": \"download\",\n",
        "        \"test_size\": \"test_size\",\n",
        "        \"val_frac\": \"val_frac\",\n",
        "        \"batch_size\": \"batch_size\",\n",
        "        \"epochs\": \"epochs\",\n",
        "        \"lr\": \"lr\",\n",
        "        \"weight_decay\": \"weight_decay\",\n",
        "        \"dropout\": \"dropout\",\n",
        "        \"batchnorm\": \"batchnorm\",\n",
        "        \"seed\": \"seed\",\n",
        "        \"deterministic\": \"deterministic\",\n",
        "        \"device\": \"device\",\n",
        "        \"live_plot\": \"live_plot\",\n",
        "        \"standardize\": \"standardize\",\n",
        "        \"noise\": \"noise\",\n",
        "        \"n_samples\": \"n_samples\",\n",
        "        \"early_stop\": \"early_stop\",\n",
        "        \"patience\": \"patience\",\n",
        "        \"min_delta\": \"min_delta\",\n",
        "        \"run_dir\": \"run_dir\",\n",
        "    }\n",
        "    for cli_key, cfg_key in override_map.items():\n",
        "        if hasattr(args, cli_key):\n",
        "            val = getattr(args, cli_key)\n",
        "            if val is not None:\n",
        "                cfg_dict[cfg_key] = val\n",
        "    # hidden sizes (list)\n",
        "    if hasattr(args, \"hidden_sizes\") and args.hidden_sizes:\n",
        "        cfg_dict[\"hidden_sizes\"] = args.hidden_sizes\n",
        "\n",
        "    # 3) Build TrainConfig\n",
        "    cfg = TrainConfig(**cfg_dict)\n",
        "\n",
        "    # 4) Reproducibility and device\n",
        "    set_seed(cfg.seed, cfg.deterministic)\n",
        "    device = make_device(cfg.device)\n",
        "\n",
        "    # 5) Prepare run directory and save config\n",
        "    run_tag = f\"{now_tag()}_{cfg.dataset}_{cfg.model}\"\n",
        "    run_dir = cfg.run_dir or os.path.join(\"runs\", run_tag)\n",
        "    ensure_dir(run_dir)\n",
        "    save_json(os.path.join(run_dir, \"config.json\"), cfg.as_dict())\n",
        "\n",
        "    # 6) Load data\n",
        "    if cfg.dataset in [\"moons\", \"blobs\", \"circles\"]:\n",
        "        X, y, class_names = make_synthetic(cfg.dataset, n_samples=cfg.n_samples, noise=cfg.noise, random_state=cfg.seed)\n",
        "        bundle = build_tabular_bundle(\n",
        "            X, y, class_names,\n",
        "            test_size=cfg.test_size,\n",
        "            val_size=cfg.val_frac,\n",
        "            seed=cfg.seed,\n",
        "            standardize=cfg.standardize,\n",
        "        )\n",
        "        is_image = False\n",
        "    elif cfg.dataset == \"iris\":\n",
        "        X, y, class_names = load_iris_dataset()\n",
        "        bundle = build_tabular_bundle(\n",
        "            X, y, class_names,\n",
        "            test_size=cfg.test_size,\n",
        "            val_size=cfg.val_frac,\n",
        "            seed=cfg.seed,\n",
        "            standardize=cfg.standardize,\n",
        "        )\n",
        "        is_image = False\n",
        "    elif cfg.dataset == \"mnist\":\n",
        "        train_ds, val_ds, test_ds, class_names = load_mnist_dataset(cfg.data_dir, cfg.download, cfg.seed)\n",
        "        is_image = True\n",
        "    else:\n",
        "        raise ValueError(\"Unsupported dataset.\")\n",
        "\n",
        "    # 7) Dataloaders\n",
        "    if not is_image:\n",
        "        train_loader, val_loader, test_loader = make_dataloaders(bundle, False, cfg.batch_size)\n",
        "        input_shape = bundle.input_shape\n",
        "        num_classes = bundle.num_classes\n",
        "    else:\n",
        "        train_loader, val_loader, test_loader = make_mnist_loaders(train_ds, val_ds, test_ds, cfg.batch_size)\n",
        "        input_shape = (1, 28, 28)\n",
        "        num_classes = 10\n",
        "\n",
        "    # 8) Build model, optimizer, loss\n",
        "    model = build_model(cfg.model, input_shape, num_classes, list(cfg.hidden_sizes), cfg.dropout, cfg.batchnorm).to(device)\n",
        "    loss_fn = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=cfg.lr, weight_decay=cfg.weight_decay)\n",
        "\n",
        "    # 9) Training loop with early stopping on val_acc\n",
        "    best_val_acc = -1.0\n",
        "    best_state = None\n",
        "    best_epoch = -1\n",
        "    epochs_no_improve = 0\n",
        "\n",
        "    history_rows = []\n",
        "    live = LivePlotter(enable=bool(cfg.live_plot))\n",
        "    t0 = time.time()\n",
        "\n",
        "    for epoch in range(1, cfg.epochs + 1):\n",
        "        train_loss, train_acc = train_one_epoch(model, train_loader, optimizer, device, loss_fn)\n",
        "        val_loss, val_acc = eval_epoch(model, val_loader, device, loss_fn)\n",
        "\n",
        "        history_rows.append({\n",
        "            \"epoch\": epoch,\n",
        "            \"train_loss\": train_loss,\n",
        "            \"val_loss\": val_loss,\n",
        "            \"train_acc\": train_acc,\n",
        "            \"val_acc\": val_acc,\n",
        "        })\n",
        "        history = pd.DataFrame(history_rows)\n",
        "        history.to_csv(os.path.join(run_dir, \"metrics.csv\"), index=False)\n",
        "        live.update(history)\n",
        "\n",
        "        if val_acc > best_val_acc + cfg.min_delta:\n",
        "            best_val_acc = val_acc\n",
        "            best_state = {k: v.cpu() for k, v in model.state_dict().items()}\n",
        "            best_epoch = epoch\n",
        "            epochs_no_improve = 0\n",
        "            torch.save(best_state, os.path.join(run_dir, \"best_model.pt\"))\n",
        "        else:\n",
        "            epochs_no_improve += 1\n",
        "\n",
        "        print(f\"[{epoch:03d}/{cfg.epochs}] train_loss={train_loss:.4f} val_loss={val_loss:.4f} \"\n",
        "              f\"train_acc={train_acc:.4f} val_acc={val_acc:.4f}\")\n",
        "\n",
        "        if cfg.early_stop and epochs_no_improve >= cfg.patience:\n",
        "            print(f\"Early stopping at epoch {epoch}. Best val_acc={best_val_acc:.4f} at epoch {best_epoch}.\")\n",
        "            break\n",
        "\n",
        "    live.close()\n",
        "    elapsed = time.time() - t0\n",
        "    print(f\"Training finished in {pretty_time(elapsed)}. Best val_acc={best_val_acc:.4f} (epoch {best_epoch}).\")\n",
        "\n",
        "    # 10) Persist meta\n",
        "    meta = {\n",
        "        \"class_names\": class_names,\n",
        "        \"is_image\": is_image,\n",
        "        \"input_shape\": input_shape,\n",
        "        \"num_classes\": num_classes,\n",
        "        \"scaler_mean\": getattr(bundle, \"scaler_mean\", None) if not is_image else None,\n",
        "        \"scaler_std\": getattr(bundle, \"scaler_std\", None) if not is_image else None,\n",
        "        \"best_epoch\": best_epoch,\n",
        "        \"elapsed_seconds\": elapsed,\n",
        "    }\n",
        "    save_json(os.path.join(run_dir, \"meta.json\"), meta)\n",
        "\n",
        "    # 11) Static plots\n",
        "    try:\n",
        "        plot_training_curves(pd.read_csv(os.path.join(run_dir, \"metrics.csv\")), os.path.join(run_dir, \"training_curves.png\"))\n",
        "    except Exception as e:\n",
        "        print(f\"Training curves plot failed: {e}\")\n",
        "\n",
        "    if not is_image:\n",
        "        try:\n",
        "            if bundle.input_shape[0] == 2:\n",
        "                if best_state is not None:\n",
        "                    model.load_state_dict(best_state)\n",
        "                plot_decision_boundary(model.to(device), bundle, device, os.path.join(run_dir, \"decision_boundary.png\"))\n",
        "        except Exception as e:\n",
        "            print(f\"Decision boundary plot failed: {e}\")\n",
        "\n",
        "    print(f\"Artifacts saved to: {run_dir}\")\n",
        "\n",
        "# ---------------------------\n",
        "# Eval command\n",
        "# ---------------------------\n",
        "\n",
        "def cmd_eval(args: argparse.Namespace):\n",
        "    run_dir = args.run_dir\n",
        "    assert os.path.isdir(run_dir), f\"Run dir not found: {run_dir}\"\n",
        "\n",
        "    cfg = load_json(os.path.join(run_dir, \"config.json\"))\n",
        "    meta = load_json(os.path.join(run_dir, \"meta.json\"))\n",
        "\n",
        "    device = make_device(args.device or cfg.get(\"device\", \"auto\"))\n",
        "    class_names = meta[\"class_names\"]\n",
        "    is_image = meta[\"is_image\"]\n",
        "    num_classes = meta[\"num_classes\"]\n",
        "    input_shape = tuple(meta[\"input_shape\"])\n",
        "\n",
        "    # Reconstruct dataset\n",
        "    if cfg[\"dataset\"] == \"mnist\":\n",
        "        if not HAS_TORCHVISION:\n",
        "            raise RuntimeError(\"torchvision not available for MNIST eval.\")\n",
        "        train_ds, val_ds, test_ds, _ = load_mnist_dataset(cfg[\"data_dir\"], cfg.get(\"download\", True), cfg.get(\"seed\", 42))\n",
        "        _, _, test_loader = make_mnist_loaders(train_ds, val_ds, test_ds, cfg[\"batch_size\"])\n",
        "    else:\n",
        "        if cfg[\"dataset\"] in [\"moons\", \"blobs\", \"circles\"]:\n",
        "            X, y, class_names = make_synthetic(cfg[\"dataset\"], n_samples=cfg[\"n_samples\"], noise=cfg[\"noise\"], random_state=cfg[\"seed\"])\n",
        "        elif cfg[\"dataset\"] == \"iris\":\n",
        "            X, y, class_names = load_iris_dataset()\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported dataset: {cfg['dataset']}\")\n",
        "        bundle = build_tabular_bundle(\n",
        "            X, y, class_names,\n",
        "            test_size=cfg[\"test_size\"],\n",
        "            val_size=cfg[\"val_frac\"],\n",
        "            seed=cfg[\"seed\"],\n",
        "            standardize=cfg[\"standardize\"],\n",
        "        )\n",
        "        _, _, test_loader = make_dataloaders(bundle, is_train=False, batch_size=cfg[\"batch_size\"])\n",
        "\n",
        "    # Build model\n",
        "    model = build_model(\n",
        "        cfg[\"model\"],\n",
        "        input_shape,\n",
        "        num_classes,\n",
        "        cfg.get(\"hidden_sizes\", [64, 64]),\n",
        "        cfg.get(\"dropout\", 0.0),\n",
        "        cfg.get(\"batchnorm\", False)\n",
        "    ).to(device)\n",
        "\n",
        "    # Load weights and prepare loss\n",
        "    state = torch.load(os.path.join(run_dir, \"best_model.pt\"), map_location=\"cpu\")\n",
        "    model.load_state_dict(state)\n",
        "    loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "    # Evaluate\n",
        "    test_loss, test_acc = eval_epoch(model, test_loader, device, loss_fn)\n",
        "    preds, targets = collect_preds(model, test_loader, device)\n",
        "\n",
        "    # Metrics\n",
        "    pr, rc, f1 = precision_recall_fscore_support(targets, preds, average=\"macro\", zero_division=0)\n",
        "    cm = confusion_matrix(targets, preds)\n",
        "\n",
        "    results = {\n",
        "        \"test_loss\": float(test_loss),\n",
        "        \"test_acc\": float(test_acc),\n",
        "        \"precision_macro\": float(pr),\n",
        "        \"recall_macro\": float(rc),\n",
        "        \"f1_macro\": float(f1),\n",
        "    }\n",
        "    save_json(os.path.join(run_dir, \"eval.json\"), results)\n",
        "    print(\"Evaluation:\", results)\n",
        "\n",
        "    # Plots and report\n",
        "    plot_confusion(cm, class_names, os.path.join(run_dir, \"confusion_matrix.png\"))\n",
        "    report = classification_report(targets, preds, target_names=class_names, digits=4)\n",
        "    with open(os.path.join(run_dir, \"classification_report.txt\"), \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(report)\n",
        "    print(\"Saved confusion matrix and classification report.\")\n",
        "\n",
        "# ---------------------------\n",
        "# Predict command\n",
        "# ---------------------------\n",
        "\n",
        "def cmd_predict(args: argparse.Namespace):\n",
        "    run_dir = args.run_dir\n",
        "    input_csv = args.input_csv\n",
        "    output_csv = args.output_csv\n",
        "\n",
        "    assert os.path.isdir(run_dir), f\"Run dir not found: {run_dir}\"\n",
        "    assert os.path.isfile(input_csv), f\"Input CSV not found: {input_csv}\"\n",
        "\n",
        "    cfg = load_json(os.path.join(run_dir, \"config.json\"))\n",
        "    meta = load_json(os.path.join(run_dir, \"meta.json\"))\n",
        "    class_names = meta[\"class_names\"]\n",
        "    is_image = meta[\"is_image\"]\n",
        "    input_shape = tuple(meta[\"input_shape\"])\n",
        "    num_classes = meta[\"num_classes\"]\n",
        "\n",
        "    if is_image:\n",
        "        raise RuntimeError(\"Predict from CSV is for tabular models only.\")\n",
        "\n",
        "    device = make_device(args.device or cfg.get(\"device\", \"auto\"))\n",
        "\n",
        "    # Load model\n",
        "    model = build_model(\n",
        "        cfg[\"model\"],\n",
        "        input_shape,\n",
        "        num_classes,\n",
        "        cfg.get(\"hidden_sizes\", [64, 64]),\n",
        "        cfg.get(\"dropout\", 0.0),\n",
        "        cfg.get(\"batchnorm\", False),\n",
        "    ).to(device)\n",
        "    state = torch.load(os.path.join(run_dir, \"best_model.pt\"), map_location=\"cpu\")\n",
        "    model.load_state_dict(state)\n",
        "    model.eval()\n",
        "\n",
        "    # Load features\n",
        "    df = pd.read_csv(input_csv)\n",
        "    X = df.values.astype(np.float32)\n",
        "\n",
        "    # Standardize if training used scaler\n",
        "    mean = meta.get(\"scaler_mean\", None)\n",
        "    std = meta.get(\"scaler_std\", None)\n",
        "    if mean is not None and std is not None:\n",
        "        X = (X - np.array(mean, dtype=np.float32)) / np.array(std, dtype=np.float32)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        logits = model(torch.from_numpy(X).to(device)).cpu().numpy()\n",
        "        pred_idx = logits.argmax(axis=1)\n",
        "        pred_label = [class_names[i] for i in pred_idx]\n",
        "        proba = torch.softmax(torch.from_numpy(logits), dim=1).numpy()\n",
        "\n",
        "    out = df.copy()\n",
        "    out[\"pred_index\"] = pred_idx\n",
        "    out[\"pred_label\"] = pred_label\n",
        "    out[\"pred_prob\"] = proba.max(axis=1)  # top-1 probability\n",
        "    out.to_csv(output_csv, index=False)\n",
        "\n",
        "    print(f\"Predictions saved to {output_csv}\")\n",
        "\n",
        "# ---------------------------\n",
        "# Export ONNX command\n",
        "# ---------------------------\n",
        "\n",
        "def cmd_export_onnx(args: argparse.Namespace):\n",
        "    run_dir = args.run_dir\n",
        "    assert os.path.isdir(run_dir), f\"Run dir not found: {run_dir}\"\n",
        "    cfg = load_json(os.path.join(run_dir, \"config.json\"))\n",
        "    meta = load_json(os.path.join(run_dir, \"meta.json\"))\n",
        "    input_shape = tuple(meta[\"input_shape\"])\n",
        "    num_classes = meta[\"num_classes\"]\n",
        "    is_image = bool(meta[\"is_image\"])\n",
        "\n",
        "    # Build and load model on CPU for portability\n",
        "    model = build_model(\n",
        "        cfg[\"model\"],\n",
        "        input_shape,\n",
        "        num_classes,\n",
        "        cfg.get(\"hidden_sizes\", [64, 64]),\n",
        "        cfg.get(\"dropout\", 0.0),\n",
        "        cfg.get(\"batchnorm\", False),\n",
        "    )\n",
        "    state = torch.load(os.path.join(run_dir, \"best_model.pt\"), map_location=\"cpu\")\n",
        "    model.load_state_dict(state)\n",
        "    model.eval()\n",
        "\n",
        "    if is_image:\n",
        "        dummy = torch.randn(1, *input_shape, dtype=torch.float32)\n",
        "    else:\n",
        "        dummy = torch.randn(1, input_shape[0], dtype=torch.float32)\n",
        "\n",
        "    onnx_path = os.path.join(run_dir, \"model.onnx\")\n",
        "    torch.onnx.export(\n",
        "        model,\n",
        "        dummy,\n",
        "        onnx_path,\n",
        "        input_names=[\"input\"],\n",
        "        output_names=[\"logits\"],\n",
        "        dynamic_axes={\"input\": {0: \"batch\"}, \"logits\": {0: \"batch\"}},\n",
        "        opset_version=13,\n",
        "    )\n",
        "    print(f\"Exported ONNX to {onnx_path}\")\n",
        "\n",
        "    # Optional: validate with ONNX Runtime\n",
        "    if HAS_ONNX:\n",
        "        sess = ort.InferenceSession(onnx_path, providers=[\"CPUExecutionProvider\"])\n",
        "        outs = sess.run([\"logits\"], {\"input\": dummy.numpy()})\n",
        "        print(\"ONNX runtime check OK. Output shape:\", np.array(outs[0]).shape)\n",
        "        meta2 = meta.copy()\n",
        "        meta2[\"onnx_exported\"] = True\n",
        "        save_json(os.path.join(run_dir, \"meta.json\"), meta2)\n",
        "\n",
        "# ---------------------------\n",
        "# Benchmark command\n",
        "# ---------------------------\n",
        "\n",
        "def cmd_benchmark(args: argparse.Namespace):\n",
        "    # Run multiple seeds quickly and aggregate performance\n",
        "    results = []\n",
        "    t0 = time.time()\n",
        "    for i in range(args.seeds):\n",
        "        seed = args.base_seed + i\n",
        "\n",
        "        tmp_cfg = TrainConfig(\n",
        "            dataset=args.dataset,\n",
        "            model=args.model,\n",
        "            epochs=args.epochs,\n",
        "            batch_size=args.batch_size,\n",
        "            lr=args.lr,\n",
        "            weight_decay=0.0,\n",
        "            hidden_sizes=tuple(args.hidden_sizes or [64, 64]),\n",
        "            dropout=args.dropout,\n",
        "            batchnorm=bool(args.batchnorm),\n",
        "            seed=seed,\n",
        "            device=args.device,\n",
        "            deterministic=True,\n",
        "            n_samples=args.n_samples,\n",
        "            noise=args.noise,\n",
        "            standardize=True,\n",
        "            early_stop=True,\n",
        "            patience=args.patience,\n",
        "            min_delta=args.min_delta,\n",
        "        )\n",
        "\n",
        "        run_tag = f\"{now_tag()}_{tmp_cfg.dataset}_{tmp_cfg.model}_seed{seed}\"\n",
        "        tmp_cfg.run_dir = os.path.join(\"runs\", run_tag)\n",
        "\n",
        "        # Train\n",
        "        train_ns = argparse.Namespace(config=None, **tmp_cfg.as_dict())\n",
        "        cmd_train(train_ns)\n",
        "\n",
        "        # Eval\n",
        "        eval_ns = argparse.Namespace(run_dir=tmp_cfg.run_dir, device=tmp_cfg.device)\n",
        "        cmd_eval(eval_ns)\n",
        "\n",
        "        # Collect metrics\n",
        "        eval_res = load_json(os.path.join(tmp_cfg.run_dir, \"eval.json\"))\n",
        "        row = {\n",
        "            \"run_dir\": tmp_cfg.run_dir,\n",
        "            \"seed\": seed,\n",
        "            **eval_res,\n",
        "        }\n",
        "        results.append(row)\n",
        "\n",
        "    # Save results\n",
        "    df = pd.DataFrame(results)\n",
        "    bench_dir = os.path.join(\"runs\", f\"benchmark_{args.dataset}_{args.model}_{now_tag()}\")\n",
        "    ensure_dir(bench_dir)\n",
        "    df.to_csv(os.path.join(bench_dir, \"benchmark.csv\"), index=False)\n",
        "    print(\"Benchmark summary:\")\n",
        "    print(df.describe(include=\"all\"))\n",
        "\n",
        "    # Plot distribution of test_acc\n",
        "    plt.figure(figsize=(6, 4))\n",
        "    sns.boxplot(y=df[\"test_acc\"], color=\"#5B8FF9\")\n",
        "    plt.title(f\"Test accuracy distribution ({args.dataset}, {args.model}, n={args.seeds})\")\n",
        "    plt.ylabel(\"test_acc\")\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(bench_dir, \"acc_distribution.png\"), dpi=150)\n",
        "    plt.close()\n",
        "\n",
        "    elapsed = time.time() - t0\n",
        "    print(f\"Benchmark finished in {pretty_time(elapsed)}. Artifacts: {bench_dir}\")\n",
        "\n",
        "# ---------------------------\n",
        "# CLI parser\n",
        "# ---------------------------\n",
        "\n",
        "def build_parser():\n",
        "    p = argparse.ArgumentParser(description=\"MetaIntelligence CLI: train/eval/predict/benchmark/export-onnx\")\n",
        "    sub = p.add_subparsers(dest=\"command\", required=True)\n",
        "\n",
        "    # Common options helpers\n",
        "    def add_common_train(sp):\n",
        "        sp.add_argument(\"--config\", type=str, default=None, help=\"JSON or YAML config path\")\n",
        "        sp.add_argument(\"--dataset\", type=str, choices=[\"moons\", \"blobs\", \"circles\", \"iris\", \"mnist\"])\n",
        "        sp.add_argument(\"--model\", type=str, choices=[\"mlp\", \"cnn\"])\n",
        "        sp.add_argument(\"--data_dir\", type=str, help=\"Data dir (MNIST)\")\n",
        "        sp.add_argument(\"--download\", type=lambda x: str(x).lower() == \"true\", help=\"Download MNIST (true/false)\")\n",
        "        sp.add_argument(\"--test_size\", type=float, help=\"Test split size (tabular)\")\n",
        "        sp.add_argument(\"--val_frac\", type=float, help=\"Validation fraction from hold-out (tabular)\")\n",
        "        sp.add_argument(\"--batch_size\", type=int)\n",
        "        sp.add_argument(\"--epochs\", type=int)\n",
        "        sp.add_argument(\"--lr\", type=float)\n",
        "        sp.add_argument(\"--weight_decay\", type=float)\n",
        "        sp.add_argument(\"--hidden_sizes\", nargs=\"*\", type=int, help=\"Hidden sizes for MLP\")\n",
        "        sp.add_argument(\"--dropout\", type=float)\n",
        "        sp.add_argument(\"--batchnorm\", action=\"store_true\")\n",
        "        sp.add_argument(\"--seed\", type=int)\n",
        "        sp.add_argument(\"--deterministic\", action=\"store_true\")\n",
        "        sp.add_argument(\"--device\", type=str, choices=[\"auto\", \"cpu\", \"cuda\"])\n",
        "        sp.add_argument(\"--live_plot\", action=\"store_true\")\n",
        "        sp.add_argument(\"--standardize\", action=\"store_true\")\n",
        "        sp.add_argument(\"--noise\", type=float, help=\"Synthetic dataset noise\")\n",
        "        sp.add_argument(\"--n_samples\", type=int, help=\"Synthetic dataset samples\")\n",
        "        sp.add_argument(\"--early_stop\", action=\"store_true\")\n",
        "        sp.add_argument(\"--patience\", type=int)\n",
        "        sp.add_argument(\"--min_delta\", type=float)\n",
        "\n",
        "    sp_train = sub.add_parser(\"train\", help=\"Train a model\")\n",
        "    add_common_train(sp_train)\n",
        "\n",
        "    sp_eval = sub.add_parser(\"eval\", help=\"Evaluate best checkpoint on test set\")\n",
        "    sp_eval.add_argument(\"--run-dir\", type=str, required=True)\n",
        "    sp_eval.add_argument(\"--device\", type=str, choices=[\"auto\", \"cpu\", \"cuda\"], default=None)\n",
        "\n",
        "    sp_predict = sub.add_parser(\"predict\", help=\"Predict on CSV (tabular only)\")\n",
        "    sp_predict.add_argument(\"--run-dir\", type=str, required=True)\n",
        "    sp_predict.add_argument(\"--input-csv\", type=str, required=True)\n",
        "    sp_predict.add_argument(\"--output-csv\", type=str, required=True)\n",
        "    sp_predict.add_argument(\"--device\", type=str, choices=[\"auto\", \"cpu\", \"cuda\"], default=None)\n",
        "\n",
        "    sp_export = sub.add_parser(\"export-onnx\", help=\"Export best checkpoint to ONNX\")\n",
        "    sp_export.add_argument(\"--run-dir\", type=str, required=True)\n",
        "\n",
        "    sp_bench = sub.add_parser(\"benchmark\", help=\"Benchmark multiple seeds\")\n",
        "    sp_bench.add_argument(\"--dataset\", type=str, choices=[\"moons\", \"blobs\", \"circles\", \"iris\", \"mnist\"], required=True)\n",
        "    sp_bench.add_argument(\"--model\", type=str, choices=[\"mlp\", \"cnn\"], required=True)\n",
        "    sp_bench.add_argument(\"--epochs\", type=int, default=30)\n",
        "    sp_bench.add_argument(\"--batch_size\", type=int, default=128)\n",
        "    sp_bench.add_argument(\"--lr\", type=float, default=1e-3)\n",
        "    sp_bench.add_argument(\"--hidden_sizes\", nargs=\"*\", type=int, default=[128, 64])\n",
        "    sp_bench.add_argument(\"--dropout\", type=float, default=0.0)\n",
        "    sp_bench.add_argument(\"--batchnorm\", action=\"store_true\")\n",
        "    sp_bench.add_argument(\"--seeds\", type=int, default=3)\n",
        "    sp_bench.add_argument(\"--base_seed\", type=int, default=42)\n",
        "    sp_bench.add_argument(\"--device\", type=str, choices=[\"auto\", \"cpu\", \"cuda\"], default=\"auto\")\n",
        "    sp_bench.add_argument(\"--n_samples\", type=int, default=4000)\n",
        "    sp_bench.add_argument(\"--noise\", type=float, default=0.2)\n",
        "    sp_bench.add_argument(\"--patience\", type=int, default=8)\n",
        "    sp_bench.add_argument(\"--min_delta\", type=float, default=1e-4)\n",
        "\n",
        "    return p\n",
        "\n",
        "# ---------------------------\n",
        "# Main\n",
        "# ---------------------------\n",
        "\n",
        "def main(argv=None):\n",
        "    import sys\n",
        "\n",
        "    # Fix for Colab: ignore injected kernel path like .../kernel-XXXX.json\n",
        "    raw_args = argv if argv is not None else sys.argv[1:]\n",
        "    filtered_args = [a for a in raw_args if not a.endswith(\".json\")]\n",
        "\n",
        "    parser = build_parser()\n",
        "\n",
        "    #  Avoid parser crash: early exit if no valid args remain\n",
        "    if not filtered_args:\n",
        "        parser.print_help()\n",
        "        print(\"\\n Example usage (in notebooks):\")\n",
        "        print('main([\"train\", \"--dataset\", \"moons\", \"--epochs\", \"30\", \"--run_dir\", \"runs/moons_test\"])')\n",
        "        print('main([\"eval\", \"--run_dir\", \"runs/moons_test\"])')\n",
        "        print('main([\"export-onnx\", \"--run_dir\", \"runs/moons_test\"])')\n",
        "        return\n",
        "\n",
        "    try:\n",
        "        args = parser.parse_args(filtered_args)\n",
        "    except SystemExit:\n",
        "        #  Notebook safety: don't crash cell\n",
        "        parser.print_help()\n",
        "        print(\"\\n Example usage (in notebooks):\")\n",
        "        print('main([\"train\", \"--dataset\", \"moons\", \"--epochs\", \"30\", \"--run_dir\", \"runs/moons_test\"])')\n",
        "        print('main([\"eval\", \"--run_dir\", \"runs/moons_test\"])')\n",
        "        print('main([\"export-onnx\", \"--run_dir\", \"runs/moons_test\"])')\n",
        "        return\n",
        "\n",
        "    cmd = getattr(args, \"command\", None)\n",
        "    if cmd == \"train\":\n",
        "        cmd_train(args)\n",
        "    elif cmd == \"eval\":\n",
        "        cmd_eval(args)\n",
        "    elif cmd == \"predict\":\n",
        "        cmd_predict(args)\n",
        "    elif cmd == \"export-onnx\":\n",
        "        cmd_export_onnx(args)\n",
        "    elif cmd == \"benchmark\":\n",
        "        cmd_benchmark(args)\n",
        "    else:\n",
        "        parser.print_help()\n",
        "        print(\"\\n Example usage (in notebooks):\")\n",
        "        print('main([\"train\", \"--dataset\", \"moons\", \"--epochs\", \"30\", \"--run_dir\", \"runs/moons_test\"])')\n",
        "        print('main([\"eval\", \"--run_dir\", \"runs/moons_test\"])')\n",
        "        print('main([\"export-onnx\", \"--run_dir\", \"runs/moons_test\"])')"
      ]
    }
  ]
}