{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyP/yuEJLt95H48ycObEEOu/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OneFineStarstuff/Cosmic-Brilliance/blob/main/train_hyperdim_ai_py.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "at487xwV1g80"
      },
      "outputs": [],
      "source": [
        "pip install torch numpy matplotlib"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "train_hyperdim_ai.py\n",
        "\n",
        "Physics‐informed AI pipeline for mapping 5D coordinates to 3D navigation metrics:\n",
        "\n",
        " 1. Synthetic dataset of 5D points → 3D analytic navigation factors\n",
        " 2. PINN loss: MSE + hyperdistance‐preserving residual\n",
        " 3. MLP with LayerNorm & Dropout for uncertainty estimation\n",
        " 4. MC‐Dropout inference to quantify predictive variance\n",
        " 5. Training loop with AdamW, ReduceLROnPlateau, early stopping\n",
        " 6. Visualizations: loss curves, true vs predicted, uncertainty heatmap\n",
        "\"\"\"\n",
        "\n",
        "import math\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 1. Synthetic Dataset\n",
        "# ------------------------------------------------------------------------------\n",
        "class HyperDimDataset(Dataset):\n",
        "    def __init__(self, n_samples=6000, seed=0):\n",
        "        np.random.seed(seed)\n",
        "        # Sample 5D coordinates in [-π, π]\n",
        "        X_raw = np.random.uniform(-math.pi, math.pi, size=(n_samples, 5)).astype(np.float32)\n",
        "        # Analytic 3D navigation factors\n",
        "        #   y0 = sin(x0) + cos(x1)\n",
        "        #   y1 = x2 * x3\n",
        "        #   y2 = exp(−x4²)\n",
        "        y0 = np.sin(X_raw[:, 0]) + np.cos(X_raw[:, 1])\n",
        "        y1 = X_raw[:, 2] * X_raw[:, 3]\n",
        "        y2 = np.exp(- X_raw[:, 4]**2)\n",
        "        Y_raw = np.stack([y0, y1, y2], axis=1).astype(np.float32)\n",
        "        # add noise\n",
        "        Y_raw += 0.02 * np.random.randn(*Y_raw.shape).astype(np.float32)\n",
        "\n",
        "        # compute normalization stats\n",
        "        self.X_mean, self.X_std = X_raw.mean(0), X_raw.std(0)\n",
        "        self.Y_mean, self.Y_std = Y_raw.mean(0), Y_raw.std(0)\n",
        "\n",
        "        # normalize\n",
        "        self.X = (X_raw - self.X_mean) / self.X_std\n",
        "        self.Y = (Y_raw - self.Y_mean) / self.Y_std\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x = torch.from_numpy(self.X[idx])\n",
        "        y = torch.from_numpy(self.Y[idx])\n",
        "        return x, y\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 2. Model Architecture with Dropout\n",
        "# ------------------------------------------------------------------------------\n",
        "class HyperDimAI(nn.Module):\n",
        "    def __init__(self, input_dim=5, hidden_dims=(64,64), output_dim=3, p_drop=0.1):\n",
        "        super().__init__()\n",
        "        layers, dim = [], input_dim\n",
        "        for h in hidden_dims:\n",
        "            layers += [\n",
        "                nn.Linear(dim, h),\n",
        "                nn.LayerNorm(h),\n",
        "                nn.ReLU(),\n",
        "                nn.Dropout(p_drop)\n",
        "            ]\n",
        "            dim = h\n",
        "        layers.append(nn.Linear(dim, output_dim))\n",
        "        self.net = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 3. Physics-Informed Loss: Hyperdistance Preservation\n",
        "# ------------------------------------------------------------------------------\n",
        "def physics_residual(pred, inp, stats_torch):\n",
        "    # denormalize\n",
        "    X_den = inp * stats_torch['X_std'] + stats_torch['X_mean']\n",
        "    Y_den = pred * stats_torch['Y_std'] + stats_torch['Y_mean']\n",
        "    # compute squared norms\n",
        "    norm2_X = torch.sum(X_den**2, dim=1)\n",
        "    norm2_Y = torch.sum(Y_den**2, dim=1)\n",
        "    # residual: ||X||² ≈ ||Y||²\n",
        "    return torch.mean((norm2_X - norm2_Y)**2)\n",
        "\n",
        "def total_loss(pred, true, inp, stats_torch, lambda_phys=0.5):\n",
        "    mse  = nn.MSELoss()(pred, true)\n",
        "    phys = physics_residual(pred, inp, stats_torch)\n",
        "    return mse + lambda_phys * phys, mse, phys\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 4. MC-Dropout Inference for Uncertainty\n",
        "# ------------------------------------------------------------------------------\n",
        "def mc_dropout_predict(model, x, n_samples=100):\n",
        "    model.train()\n",
        "    preds = []\n",
        "    with torch.no_grad():\n",
        "        for _ in range(n_samples):\n",
        "            preds.append(model(x).cpu().numpy())\n",
        "    arr = np.stack(preds, axis=0)\n",
        "    return arr.mean(axis=0), arr.std(axis=0)\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 5. Training Loop\n",
        "# ------------------------------------------------------------------------------\n",
        "def train(model, train_loader, val_loader, stats_torch, device,\n",
        "          lr=1e-3, wd=1e-5, lambda_phys=0.5,\n",
        "          max_epochs=100, patience=10):\n",
        "    model.to(device)\n",
        "    opt = optim.AdamW(model.parameters(), lr=lr, weight_decay=wd)\n",
        "    sched = optim.lr_scheduler.ReduceLROnPlateau(opt, mode='min',\n",
        "                                                factor=0.5, patience=5)\n",
        "\n",
        "    best_val, wait = float('inf'), 0\n",
        "    history = {'train_loss': [], 'val_loss': []}\n",
        "\n",
        "    for epoch in range(1, max_epochs+1):\n",
        "        # training\n",
        "        model.train()\n",
        "        tr_loss = 0.0\n",
        "        for xb, yb in train_loader:\n",
        "            xb, yb = xb.to(device), yb.to(device)\n",
        "            pred = model(xb)\n",
        "            loss, _, _ = total_loss(pred, yb, xb, stats_torch, lambda_phys)\n",
        "            opt.zero_grad()\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "            tr_loss += loss.item() * xb.size(0)\n",
        "        tr_loss /= len(train_loader.dataset)\n",
        "\n",
        "        # validation\n",
        "        model.eval()\n",
        "        vl_loss = 0.0\n",
        "        with torch.no_grad():\n",
        "            for xb, yb in val_loader:\n",
        "                xb, yb = xb.to(device), yb.to(device)\n",
        "                pred = model(xb)\n",
        "                loss, _, _ = total_loss(pred, yb, xb, stats_torch, lambda_phys)\n",
        "                vl_loss += loss.item() * xb.size(0)\n",
        "        vl_loss /= len(val_loader.dataset)\n",
        "\n",
        "        sched.step(vl_loss)\n",
        "        history['train_loss'].append(tr_loss)\n",
        "        history['val_loss'].append(vl_loss)\n",
        "        print(f\"Epoch {epoch:03d} | Train {tr_loss:.4e} | Val {vl_loss:.4e}\")\n",
        "\n",
        "        if vl_loss < best_val - 1e-6:\n",
        "            best_val, wait = vl_loss, 0\n",
        "            torch.save(model.state_dict(), \"best_hyperdim_ai.pth\")\n",
        "        else:\n",
        "            wait += 1\n",
        "            if wait >= patience:\n",
        "                print(f\"Early stopping at epoch {epoch}\")\n",
        "                break\n",
        "\n",
        "    model.load_state_dict(torch.load(\"best_hyperdim_ai.pth\"))\n",
        "    return history\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 6. Visualization Helpers\n",
        "# ------------------------------------------------------------------------------\n",
        "def plot_losses(history):\n",
        "    plt.figure()\n",
        "    plt.plot(history['train_loss'], label='Train')\n",
        "    plt.plot(history['val_loss'],   label='Val')\n",
        "    plt.xlabel(\"Epoch\"); plt.ylabel(\"Loss\")\n",
        "    plt.legend(); plt.title(\"Training Curve\")\n",
        "    plt.show()\n",
        "\n",
        "def plot_scatter(true_vals, pred_vals, name):\n",
        "    plt.figure()\n",
        "    plt.scatter(true_vals, pred_vals, s=5, alpha=0.5)\n",
        "    m, M = true_vals.min(), true_vals.max()\n",
        "    plt.plot([m, M], [m, M], 'r--')\n",
        "    plt.xlabel(f\"True {name}\"); plt.ylabel(f\"Pred {name}\")\n",
        "    plt.title(f\"{name}: True vs Pred\")\n",
        "    plt.show()\n",
        "\n",
        "def plot_uncertainty_heatmap(model, stats_np, device):\n",
        "    # vary x0, x1 in [-π,π], fix others at 0\n",
        "    vals = np.linspace(-math.pi, math.pi, 100)\n",
        "    G0, G1 = np.meshgrid(vals, vals)\n",
        "    grid = np.zeros((G0.size, 5), dtype=np.float32)\n",
        "    grid[:,0] = G0.ravel()\n",
        "    grid[:,1] = G1.ravel()\n",
        "\n",
        "    # normalize\n",
        "    Xn = (grid - stats_np['X_mean']) / stats_np['X_std']\n",
        "    Xt = torch.from_numpy(Xn).float().to(device)\n",
        "    _, std = mc_dropout_predict(model, Xt, n_samples=100)\n",
        "    # take std of first output\n",
        "    std_map = std[:,0].reshape(G0.shape)\n",
        "\n",
        "    plt.figure(figsize=(6,5))\n",
        "    plt.pcolormesh(G0, G1, std_map, shading='auto', cmap='viridis')\n",
        "    plt.colorbar(label=\"Std of y0\")\n",
        "    plt.xlabel(\"x0\"); plt.ylabel(\"x1\")\n",
        "    plt.title(\"Uncertainty Heatmap for y0\")\n",
        "    plt.show()\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 7. Main Execution\n",
        "# ------------------------------------------------------------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # prepare data\n",
        "    ds = HyperDimDataset(n_samples=8000)\n",
        "    stats_torch = {\n",
        "        'X_mean': torch.tensor(ds.X_mean, device=device),\n",
        "        'X_std':  torch.tensor(ds.X_std,  device=device),\n",
        "        'Y_mean': torch.tensor(ds.Y_mean, device=device),\n",
        "        'Y_std':  torch.tensor(ds.Y_std,  device=device),\n",
        "    }\n",
        "    stats_np = {\n",
        "        'X_mean': ds.X_mean,\n",
        "        'X_std':  ds.X_std,\n",
        "        'Y_mean': ds.Y_mean,\n",
        "        'Y_std':  ds.Y_std,\n",
        "    }\n",
        "\n",
        "    # split\n",
        "    n_val = int(0.2 * len(ds))\n",
        "    trn_ds, val_ds = random_split(ds, [len(ds)-n_val, n_val])\n",
        "    trn_ld = DataLoader(trn_ds, batch_size=64, shuffle=True)\n",
        "    val_ld = DataLoader(val_ds,   batch_size=128)\n",
        "\n",
        "    # build & train\n",
        "    model   = HyperDimAI().to(device)\n",
        "    history = train(model, trn_ld, val_ld, stats_torch, device)\n",
        "\n",
        "    # plots\n",
        "    plot_losses(history)\n",
        "\n",
        "    # scatter for each output\n",
        "    X_all = ds.X\n",
        "    with torch.no_grad():\n",
        "        Y_pred_norm = model(torch.from_numpy(X_all).float().to(device)).cpu().numpy()\n",
        "    Y_true_norm = ds.Y\n",
        "    Y_pred = Y_pred_norm * ds.Y_std + ds.Y_mean\n",
        "    Y_true = Y_true_norm * ds.Y_std + ds.Y_mean\n",
        "\n",
        "    for i, name in enumerate([\"y0\",\"y1\",\"y2\"]):\n",
        "        plot_scatter(Y_true[:,i], Y_pred[:,i], name)\n",
        "\n",
        "    # uncertainty heatmap\n",
        "    plot_uncertainty_heatmap(model, stats_np, device)"
      ],
      "metadata": {
        "id": "eIXV-m0T2W2J"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}