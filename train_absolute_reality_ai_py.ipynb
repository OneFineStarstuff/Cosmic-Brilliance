{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyMG/MSHDkU/i7ex/xg8DbiC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OneFineStarstuff/Cosmic-Brilliance/blob/main/train_absolute_reality_ai_py.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r70DXdtCuFLO"
      },
      "outputs": [],
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "train_absolute_reality_ai.py\n",
        "\n",
        "Pipeline:\n",
        "1. Synthetic “reality substrate” dataset (6 inputs → 3 targets)\n",
        "2. Float64 data generation → Float32 normalization\n",
        "3. Enhanced MLP with LayerNorm, Dropout & ReLU\n",
        "4. Physics-informed residual enforcing toy “fabrication laws”\n",
        "5. MC-Dropout for epistemic uncertainty\n",
        "6. Training loop: AdamW, ReduceLROnPlateau, grad clipping, NaN checks, early stopping\n",
        "7. Checkpointing & safe model reload\n",
        "8. Visualizations: loss curves, scatter plots, uncertainty map\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 1. Synthetic Reality Substrate Dataset\n",
        "# ------------------------------------------------------------------------------\n",
        "class RealityDataset(Dataset):\n",
        "    def __init__(self, n_samples=5000, seed=123):\n",
        "        np.random.seed(seed)\n",
        "        # Inputs: pre-entropy (PE), causality seeds (CS1–CS4), coherence metric (CM)\n",
        "        PE  = np.random.uniform(0.0, 1.0, (n_samples,1))\n",
        "        CS1 = np.random.uniform(0.0, 5.0, (n_samples,1))\n",
        "        CS2 = np.random.uniform(0.0, 5.0, (n_samples,1))\n",
        "        CS3 = np.random.uniform(0.0, 5.0, (n_samples,1))\n",
        "        CS4 = np.random.uniform(0.0, 5.0, (n_samples,1))\n",
        "        CM  = np.random.uniform(0.1, 2.0, (n_samples,1))\n",
        "        X_raw = np.hstack([PE, CS1, CS2, CS3, CS4, CM]).astype(np.float64)\n",
        "\n",
        "        # Toy fabrication laws → targets\n",
        "        eps = 1e-6\n",
        "        # 1. Reality Emergence (RE)\n",
        "        RE = PE * np.log1p(CS1 + CS2 + eps) * CM\n",
        "        # 2. Substrate Coherence (SC)\n",
        "        SC = (CS1 * CS2 * CS3 * CS4)**0.25 / (1 + PE)\n",
        "        # 3. Trans-dimensional Balance (TB)\n",
        "        TB = np.sin(PE * np.pi) * CM\n",
        "\n",
        "        Y_raw = np.hstack([RE, SC, TB]).astype(np.float64)\n",
        "        Y_raw += 0.02 * Y_raw.std(axis=0) * np.random.randn(*Y_raw.shape)\n",
        "\n",
        "        # Compute stats\n",
        "        self.X_mean, self.X_std = X_raw.mean(0), X_raw.std(0) + 1e-8\n",
        "        self.Y_mean, self.Y_std = Y_raw.mean(0), Y_raw.std(0) + 1e-8\n",
        "\n",
        "        # Normalize to float32\n",
        "        self.X = ((X_raw - self.X_mean) / self.X_std).astype(np.float32)\n",
        "        self.Y = ((Y_raw - self.Y_mean) / self.Y_std).astype(np.float32)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return torch.from_numpy(self.X[idx]), torch.from_numpy(self.Y[idx])\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 2. Enhanced AbsoluteRealityAI Definition\n",
        "# ------------------------------------------------------------------------------\n",
        "class AbsoluteRealityAI(nn.Module):\n",
        "    def __init__(self, input_dim=6, hidden_dims=(64, 64), output_dim=3, p_drop=0.1):\n",
        "        super().__init__()\n",
        "        layers, d = [], input_dim\n",
        "        for h in hidden_dims:\n",
        "            layers += [\n",
        "                nn.Linear(d, h),\n",
        "                nn.LayerNorm(h),\n",
        "                nn.ReLU(),\n",
        "                nn.Dropout(p_drop)\n",
        "            ]\n",
        "            d = h\n",
        "        layers.append(nn.Linear(d, output_dim))\n",
        "        self.net = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 3. Physics-informed Residual Loss\n",
        "# ------------------------------------------------------------------------------\n",
        "def physics_residual(pred, X, stats):\n",
        "    X_denorm = X * stats['X_std'] + stats['X_mean']\n",
        "    PE, CS1, CS2, CS3, CS4, CM = X_denorm.t()\n",
        "    eps = 1e-6\n",
        "\n",
        "    RE_t = PE * torch.log1p(CS1 + CS2 + eps) * CM\n",
        "    SC_t = (CS1 * CS2 * CS3 * CS4)**0.25 / (1 + PE)\n",
        "    TB_t = torch.sin(PE * np.pi) * CM\n",
        "\n",
        "    Y_true = torch.stack([RE_t, SC_t, TB_t], dim=1)\n",
        "    Y_norm = (Y_true - stats['Y_mean']) / stats['Y_std']\n",
        "    return nn.MSELoss()(pred, Y_norm)\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 4. Combined Loss\n",
        "# ------------------------------------------------------------------------------\n",
        "def total_loss(pred, true, X, stats, lam=1.0):\n",
        "    mse_loss = nn.MSELoss()(pred, true)\n",
        "    phys_loss = physics_residual(pred, X, stats)\n",
        "    return mse_loss + lam * phys_loss, mse_loss, phys_loss\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 5. MC-Dropout Uncertainty\n",
        "# ------------------------------------------------------------------------------\n",
        "def mc_dropout_predict(model, X, T=50):\n",
        "    model.train()\n",
        "    preds = []\n",
        "    with torch.no_grad():\n",
        "        for _ in range(T):\n",
        "            preds.append(model(X))\n",
        "    all_preds = torch.stack(preds)\n",
        "    return all_preds.mean(0), all_preds.std(0)\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 6. Training Loop & Checkpointing\n",
        "# ------------------------------------------------------------------------------\n",
        "def train(model, train_loader, val_loader, stats, device,\n",
        "          lr=1e-4, wd=1e-5, lam=1.0, epochs=100, patience=10):\n",
        "    model.to(device)\n",
        "    opt = optim.AdamW(model.parameters(), lr=lr, weight_decay=wd)\n",
        "    sched = optim.lr_scheduler.ReduceLROnPlateau(opt, factor=0.5, patience=5)\n",
        "    best_val, wait = float('inf'), 0\n",
        "    history = {'train': [], 'val': []}\n",
        "\n",
        "    for epoch in range(1, epochs+1):\n",
        "        # -- Train --\n",
        "        model.train()\n",
        "        train_loss = 0\n",
        "        for Xb, Yb in train_loader:\n",
        "            Xb, Yb = Xb.to(device), Yb.to(device)\n",
        "            pred = model(Xb)\n",
        "            loss, _, _ = total_loss(pred, Yb, Xb, stats, lam)\n",
        "            if torch.isnan(loss):\n",
        "                print(f\"NaN at epoch {epoch}, abort.\")\n",
        "                return history\n",
        "            opt.zero_grad(); loss.backward()\n",
        "            nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            opt.step()\n",
        "            train_loss += loss.item() * Xb.size(0)\n",
        "        train_loss /= len(train_loader.dataset)\n",
        "\n",
        "        # -- Validate --\n",
        "        model.eval()\n",
        "        val_loss = 0\n",
        "        with torch.no_grad():\n",
        "            for Xv, Yv in val_loader:\n",
        "                Xv, Yv = Xv.to(device), Yv.to(device)\n",
        "                pred = model(Xv)\n",
        "                l, _, _ = total_loss(pred, Yv, Xv, stats, lam)\n",
        "                val_loss += l.item() * Xv.size(0)\n",
        "        val_loss /= len(val_loader.dataset)\n",
        "\n",
        "        sched.step(val_loss)\n",
        "        history['train'].append(train_loss)\n",
        "        history['val'].append(val_loss)\n",
        "        print(f\"Epoch {epoch:03d} | Train {train_loss:.4e} | Val {val_loss:.4e}\")\n",
        "\n",
        "        # Checkpoint\n",
        "        if val_loss < best_val - 1e-6:\n",
        "            best_val, wait = val_loss, 0\n",
        "            torch.save(model.state_dict(), \"best_absolute_ai.pth\")\n",
        "        else:\n",
        "            wait += 1\n",
        "            if wait >= patience:\n",
        "                print(\"Early stopping.\")\n",
        "                break\n",
        "\n",
        "    # Reload best\n",
        "    if os.path.exists(\"best_absolute_ai.pth\"):\n",
        "        model.load_state_dict(torch.load(\"best_absolute_ai.pth\", map_location=device))\n",
        "    return history\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 7. Visualization Helpers\n",
        "# ------------------------------------------------------------------------------\n",
        "def plot_history(hist):\n",
        "    plt.figure()\n",
        "    plt.plot(hist['train'], label='Train')\n",
        "    plt.plot(hist['val'],   label='Val')\n",
        "    plt.xlabel(\"Epoch\"); plt.ylabel(\"Loss\"); plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "def plot_scatter(y_true, y_pred, name):\n",
        "    plt.figure()\n",
        "    plt.scatter(y_true, y_pred, s=5, alpha=0.6)\n",
        "    mn, mx = min(y_true.min(), y_pred.min()), max(y_true.max(), y_pred.max())\n",
        "    plt.plot([mn, mx],[mn, mx],'r--')\n",
        "    plt.title(name); plt.show()\n",
        "\n",
        "def plot_uncertainty(model, stats, device):\n",
        "    G = 100\n",
        "    PE = np.linspace(0,1,G, dtype=np.float32)\n",
        "    CM = np.linspace(0.1,2.0,G, dtype=np.float32)\n",
        "    grid = np.array(np.meshgrid(PE, CM)).T.reshape(-1,2)\n",
        "    Xg = np.zeros((G*G,6), dtype=np.float32)\n",
        "    # fill PE, CM; keep CS1–4 at mean\n",
        "    Xg[:,0] = grid[:,0]; Xg[:,5] = grid[:,1]\n",
        "    Xg[:,1:5] = stats['X_mean'][1:5].cpu().numpy()\n",
        "\n",
        "    Xn = (torch.from_numpy(Xg).to(device) - stats['X_mean']) / stats['X_std']\n",
        "    _, std = mc_dropout_predict(model, Xn, T=40)\n",
        "    U = std[:,0].cpu().numpy().reshape(G,G)\n",
        "\n",
        "    plt.figure(figsize=(5,4))\n",
        "    plt.pcolormesh(PE, CM, U, shading='auto', cmap='viridis')\n",
        "    plt.colorbar(label=\"Std(RE)\")\n",
        "    plt.xlabel(\"Pre-entropy\"); plt.ylabel(\"Coherence\")\n",
        "    plt.title(\"Uncertainty: Reality Emergence\")\n",
        "    plt.show()\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 8. Main\n",
        "# ------------------------------------------------------------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # Dataset & stats\n",
        "    ds = RealityDataset(n_samples=6000)\n",
        "    stats = {\n",
        "        'X_mean': torch.tensor(ds.X_mean, device=device, dtype=torch.float32),\n",
        "        'X_std' : torch.tensor(ds.X_std,  device=device, dtype=torch.float32),\n",
        "        'Y_mean': torch.tensor(ds.Y_mean, device=device, dtype=torch.float32),\n",
        "        'Y_std' : torch.tensor(ds.Y_std,  device=device, dtype=torch.float32),\n",
        "    }\n",
        "\n",
        "    # Split & loaders\n",
        "    n_val = int(0.2 * len(ds))\n",
        "    train_ds, val_ds = random_split(ds, [len(ds)-n_val, n_val])\n",
        "    tr_ld = DataLoader(train_ds, batch_size=128, shuffle=True)\n",
        "    va_ld = DataLoader(val_ds,   batch_size=256, shuffle=False)\n",
        "\n",
        "    # Model, Train, Visualize\n",
        "    model = AbsoluteRealityAI().to(device)\n",
        "    history = train(model, tr_ld, va_ld, stats, device)\n",
        "\n",
        "    plot_history(history)\n",
        "\n",
        "    # True vs Predicted\n",
        "    X_all = torch.from_numpy(ds.X).to(device)\n",
        "    with torch.no_grad():\n",
        "        Yp_n = model(X_all).cpu().numpy()\n",
        "    Y_true = ds.Y * ds.Y_std + ds.Y_mean\n",
        "    Y_pred  = Yp_n * ds.Y_std + ds.Y_mean\n",
        "    names = [\"Reality Emergence\",\"Substrate Coherence\",\"Trans-dimensional Balance\"]\n",
        "    for i, nm in enumerate(names):\n",
        "        plot_scatter(Y_true[:,i], Y_pred[:,i], nm)\n",
        "\n",
        "    plot_uncertainty(model, stats, device)"
      ]
    }
  ]
}