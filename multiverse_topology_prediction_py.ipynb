{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyNPubbKW5ydbmNbtJl0mOYS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OneFineStarstuff/Cosmic-Brilliance/blob/main/multiverse_topology_prediction_py.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CtvtISKH78sS"
      },
      "outputs": [],
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "multiverse_topology_prediction.py\n",
        "\n",
        "Train a neural network to predict multiverse topology classes\n",
        "from string-theory compactification parameters.\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "# 1. Model Definition\n",
        "class MultiverseAI(nn.Module):\n",
        "    def __init__(self, input_dim=5, hidden_dim=32, output_dim=10):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(input_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, output_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        logits = self.net(x)\n",
        "        return F.softmax(logits, dim=-1)\n",
        "\n",
        "# 2. Synthetic Dataset Generation\n",
        "def sample_compactifications(n_samples=10000, input_dim=5):\n",
        "    \"\"\"\n",
        "    Generate synthetic compactification parameters and discrete topology labels.\n",
        "    \"\"\"\n",
        "    params = np.random.uniform(0, 2*np.pi, size=(n_samples, input_dim)).astype(np.float32)\n",
        "    scores = np.sum(np.sin(params), axis=1)\n",
        "    # Use np.ptp() instead of ndarray.ptp(), added 1e-6 to avoid division by zero\n",
        "    labels = np.floor((scores - scores.min()) / (np.ptp(scores) + 1e-6) * 10).astype(int)\n",
        "    labels = np.clip(labels, 0, 9)\n",
        "    return params, labels\n",
        "\n",
        "# 3. Training and Validation Function\n",
        "def train_and_validate(\n",
        "    model, train_loader, val_loader, epochs=50, lr=1e-3, device=None\n",
        "):\n",
        "    if device is None:\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    for epoch in range(1, epochs+1):\n",
        "        # Training\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        correct_train = 0\n",
        "        total_train = 0\n",
        "\n",
        "        for xb, yb in train_loader:\n",
        "            xb, yb = xb.to(device), yb.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            preds = model(xb)\n",
        "            loss = criterion(preds, yb)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item() * xb.size(0)\n",
        "            correct_train += (preds.argmax(1) == yb).sum().item()\n",
        "            total_train += xb.size(0)\n",
        "\n",
        "        train_loss = running_loss / total_train\n",
        "        train_acc  = correct_train / total_train\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        correct_val = 0\n",
        "        total_val = 0\n",
        "        with torch.no_grad():\n",
        "            for xb, yb in val_loader:\n",
        "                xb, yb = xb.to(device), yb.to(device)\n",
        "                preds = model(xb)\n",
        "                correct_val += (preds.argmax(1) == yb).sum().item()\n",
        "                total_val += xb.size(0)\n",
        "        val_acc = correct_val / total_val\n",
        "\n",
        "        if epoch == 1 or epoch % 10 == 0:\n",
        "            print(\n",
        "                f\"Epoch {epoch:02d} ─ \"\n",
        "                f\"Train Loss: {train_loss:.4f} ─ \"\n",
        "                f\"Train Acc: {train_acc:.3f} ─ \"\n",
        "                f\"Val Acc: {val_acc:.3f}\"\n",
        "            )\n",
        "\n",
        "# 4. Main Execution\n",
        "if __name__ == \"__main__\":\n",
        "    # Hyperparameters\n",
        "    INPUT_DIM   = 5\n",
        "    HIDDEN_DIM  = 32\n",
        "    OUTPUT_DIM  = 10\n",
        "    N_SAMPLES   = 10000\n",
        "    TEST_SIZE   = 0.2\n",
        "    BATCH_SIZE  = 128\n",
        "    EPOCHS      = 50\n",
        "    LEARNING_RT = 1e-3\n",
        "\n",
        "    # Generate data\n",
        "    X, y = sample_compactifications(n_samples=N_SAMPLES, input_dim=INPUT_DIM)\n",
        "\n",
        "    # Split into training and validation\n",
        "    X_train, X_val, y_train, y_val = train_test_split(\n",
        "        X, y, test_size=TEST_SIZE, random_state=42\n",
        "    )\n",
        "\n",
        "    # Create DataLoaders\n",
        "    train_ds = TensorDataset(torch.from_numpy(X_train), torch.from_numpy(y_train))\n",
        "    val_ds   = TensorDataset(torch.from_numpy(X_val),   torch.from_numpy(y_val))\n",
        "    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
        "    val_loader   = DataLoader(val_ds,   batch_size=BATCH_SIZE)\n",
        "\n",
        "    # Initialize and train model\n",
        "    model = MultiverseAI(input_dim=INPUT_DIM,\n",
        "                         hidden_dim=HIDDEN_DIM,\n",
        "                         output_dim=OUTPUT_DIM)\n",
        "    train_and_validate(\n",
        "        model, train_loader, val_loader,\n",
        "        epochs=EPOCHS, lr=LEARNING_RT\n",
        "    )\n",
        "\n",
        "    # Sample evaluation\n",
        "    sample_params = np.random.uniform(0, 2*np.pi, size=(3, INPUT_DIM)).astype(np.float32)\n",
        "    with torch.no_grad():\n",
        "        probs = model(torch.from_numpy(sample_params))\n",
        "    print(\"\\nSample Predictions:\")\n",
        "    for i, p in enumerate(probs):\n",
        "        print(f\"  Params {i+1}: {p.numpy()}\")"
      ]
    }
  ]
}