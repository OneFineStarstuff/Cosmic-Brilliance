{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyN4U+jdXZAZ7YK5tYlh2SMr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OneFineStarstuff/Cosmic-Brilliance/blob/main/Complete_Unified_AI_System_(Single_File).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch, torch.nn as nn, torch.nn.functional as F\n",
        "import timm\n",
        "from sentencepiece import SentencePieceProcessor\n",
        "\n",
        "# ---------- 1. Perception ----------------------------------------------------\n",
        "class VisionEncoder(nn.Module):\n",
        "    def __init__(self, embed_dim=1024):\n",
        "        super().__init__()\n",
        "        self.backbone = timm.create_model(\n",
        "            'vit_large_patch14_clip_224.laion2b_ft_in12k_in1k',\n",
        "            pretrained=True, num_classes=0\n",
        "        )\n",
        "        self.proj = nn.Linear(self.backbone.embed_dim, embed_dim)\n",
        "\n",
        "    def forward(self, img):\n",
        "        return self.proj(self.backbone(img))\n",
        "\n",
        "class TextEncoder(nn.Module):\n",
        "    def __init__(self, vocab_path, embed_dim=1024):\n",
        "        super().__init__()\n",
        "        self.tok = SentencePieceProcessor(model_file=vocab_path)\n",
        "        self.embed = nn.Embedding(self.tok.get_piece_size(), embed_dim)\n",
        "\n",
        "    def forward(self, txt):\n",
        "        ids = torch.tensor(self.tok.encode(txt, out_type=int), device=self.embed.weight.device)\n",
        "        return self.embed(ids)\n",
        "\n",
        "# ---------- 2. Memory --------------------------------------------------------\n",
        "class EpisodicMemory(nn.Module):\n",
        "    def __init__(self, dim=1024, slots=4096):\n",
        "        super().__init__()\n",
        "        self.keys   = nn.Parameter(torch.randn(slots, dim))\n",
        "        self.values = nn.Parameter(torch.randn(slots, dim))\n",
        "\n",
        "    def write(self, k, v):\n",
        "        idx = torch.argmax((self.keys @ k.T).diag())\n",
        "        self.keys.data[idx]   = k.detach()\n",
        "        self.values.data[idx] = v.detach()\n",
        "\n",
        "    def read(self, q):\n",
        "        scores = q @ self.keys.T\n",
        "        w = F.softmax(scores, dim=-1)\n",
        "        return w @ self.values\n",
        "\n",
        "# ---------- 3. World Model ---------------------------------------------------\n",
        "class LatentRSSM(nn.Module):\n",
        "    def __init__(self, latent=256, action_dim=32, obs_dim=1024):\n",
        "        super().__init__()\n",
        "        self.rnn   = nn.GRU(latent + action_dim, latent, batch_first=True)\n",
        "        self.post  = nn.Linear(latent + obs_dim, 2 * latent)\n",
        "        self.prior = nn.Linear(latent, 2 * latent)\n",
        "\n",
        "    def forward(self, prev_state, action, obs_embed):\n",
        "        rnn_input = torch.cat([prev_state, action], -1)\n",
        "        _, h = self.rnn(rnn_input.unsqueeze(0))\n",
        "        h = h.squeeze(0)\n",
        "        post_stats  = self.post(torch.cat([h, obs_embed], -1))\n",
        "        prior_stats = self.prior(h)\n",
        "        return post_stats, prior_stats, h\n",
        "\n",
        "# ---------- 4. Planner -------------------------------------------------------\n",
        "class LLMStub:\n",
        "    def chain_of_thought(self, goal_desc):\n",
        "        return [\"move_left\", \"jump\", \"move_right\", \"duck\", \"jump\"]\n",
        "\n",
        "    def fix(self, plan):\n",
        "        return [\"move_left\", \"move_right\", \"duck\", \"jump\", \"duck\"]\n",
        "\n",
        "def reparam(stats):\n",
        "    mu, log_std = stats.chunk(2, dim=-1)\n",
        "    std = torch.exp(log_std)\n",
        "    eps = torch.randn_like(std)\n",
        "    return mu + eps * std\n",
        "\n",
        "def reward_head(latent):\n",
        "    return torch.sigmoid(torch.mean(latent))\n",
        "\n",
        "def textual_policy_to_action(text_cmd):\n",
        "    vocab = {\"move_left\": 0, \"move_right\": 1, \"jump\": 2, \"duck\": 3}\n",
        "    vec = torch.zeros(len(vocab))\n",
        "    if text_cmd in vocab:\n",
        "        vec[vocab[text_cmd]] = 1.0\n",
        "    return vec\n",
        "\n",
        "def pddl_validator(plan):\n",
        "    return True\n",
        "\n",
        "class HybridPlanner(nn.Module):\n",
        "    def __init__(self, llm, rssm, max_depth=5):\n",
        "        super().__init__()\n",
        "        self.llm = llm\n",
        "        self.rssm = rssm\n",
        "        self.max_depth = max_depth\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def plan(self, goal_desc, init_state):\n",
        "        prelim_plan = self.llm.chain_of_thought(goal_desc)\n",
        "        if not pddl_validator(prelim_plan):\n",
        "            prelim_plan = self.llm.fix(prelim_plan)\n",
        "        return self.imagine(prelim_plan, init_state)\n",
        "\n",
        "    def imagine(self, plan, state):\n",
        "        traj, total_reward = [], 0.0\n",
        "        for t in range(self.max_depth):\n",
        "            act = textual_policy_to_action(plan[t])\n",
        "            _, prior_stats, state = self.rssm(state, act, obs_embed=torch.zeros(1024))\n",
        "            state = reparam(prior_stats)\n",
        "            r_t = reward_head(state)\n",
        "            traj.append((state, act, r_t))\n",
        "            total_reward += r_t\n",
        "        return traj, total_reward\n",
        "\n",
        "# ---------- 5. Runner --------------------------------------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    vision   = VisionEncoder()\n",
        "    text     = TextEncoder(\"spiece.model\")  # Provide actual vocab file path\n",
        "    memory   = EpisodicMemory()\n",
        "    rssm     = LatentRSSM()\n",
        "    llm      = LLMStub()\n",
        "    planner  = HybridPlanner(llm, rssm)\n",
        "\n",
        "    init_state = torch.randn(256)\n",
        "    goal = \"navigate obstacle course\"\n",
        "    traj, reward = planner.plan(goal, init_state)\n",
        "\n",
        "    print(f\"Planned {len(traj)} steps | Total Reward: {reward:.4f}\")"
      ],
      "metadata": {
        "id": "88n78cvyRk8H"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}