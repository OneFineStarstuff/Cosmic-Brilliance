{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyPnb6imn+kJH/Wk+pmF3dvD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OneFineStarstuff/Cosmic-Brilliance/blob/main/meta_intelligence_e2e_py.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9E4Dp20QbUf3"
      },
      "outputs": [],
      "source": [
        "#!/usr/bin/env python3\n",
        "# -*- coding: utf-8 -*-\n",
        "\n",
        "\"\"\"\n",
        "meta_intelligence_e2e.py\n",
        "\n",
        "Notebook/Colab-safe end-to-end toolkit for training, evaluating, predicting,\n",
        "and benchmarking a compact MLP on synthetic datasets.\n",
        "\n",
        "Subcommands:\n",
        "- train:     train a model (self/supervised/hybrid)\n",
        "- eval:      evaluate a checkpoint on a fresh synthetic test set\n",
        "- predict:   predict for inline points or CSV rows\n",
        "- benchmark: run multiple configurations over several seeds, summarize results\n",
        "- export-onnx: export a trained checkpoint to ONNX (if onnx available)\n",
        "\n",
        "Artifacts in <run_dir>:\n",
        "- config.json\n",
        "- metrics.csv\n",
        "- model.pt\n",
        "- metrics.png        (if matplotlib available)\n",
        "- boundary.png       (if dim == 2 and matplotlib available)\n",
        "\"\"\"\n",
        "\n",
        "import argparse\n",
        "import csv\n",
        "import json\n",
        "import math\n",
        "import os\n",
        "import random\n",
        "import sys\n",
        "import time\n",
        "from dataclasses import dataclass, asdict\n",
        "from typing import Dict, List, Optional, Tuple\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# Optional plotting (script runs without it)\n",
        "try:\n",
        "    import matplotlib.pyplot as plt\n",
        "    _HAS_MPL = True\n",
        "except Exception:\n",
        "    _HAS_MPL = False\n",
        "\n",
        "# Optional ONNX export\n",
        "try:\n",
        "    import onnx  # noqa: F401\n",
        "    import torch.onnx\n",
        "    _HAS_ONNX = True\n",
        "except Exception:\n",
        "    _HAS_ONNX = False\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "# --- Notebook-safe CLI utilities ------------------------------------------------\n",
        "\n",
        "def sanitize_argv(argv: Optional[List[str]] = None) -> List[str]:\n",
        "    \"\"\"Strip Jupyter/Colab's '-f <kernel.json>' and stray kernel json args.\"\"\"\n",
        "    if argv is None:\n",
        "        argv = sys.argv[1:]\n",
        "    cleaned, skip = [], False\n",
        "    for a in argv:\n",
        "        if skip:\n",
        "            skip = False\n",
        "            continue\n",
        "        if a == \"-f\":\n",
        "            skip = True\n",
        "            continue\n",
        "        if a.endswith(\".json\") and (\"jupyter\" in a or \"kernel\" in a):\n",
        "            continue\n",
        "        cleaned.append(a)\n",
        "    return cleaned\n",
        "\n",
        "\n",
        "# --- Reproducibility and misc ---------------------------------------------------\n",
        "\n",
        "def set_seed(seed: int) -> None:\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "\n",
        "def ts() -> str:\n",
        "    return time.strftime(\"%Y%m%d-%H%M%S\")\n",
        "\n",
        "\n",
        "def ensure_dir(path: str) -> None:\n",
        "    os.makedirs(path, exist_ok=True)\n",
        "\n",
        "\n",
        "# --- Data generation ------------------------------------------------------------\n",
        "\n",
        "@dataclass\n",
        "class DataConfig:\n",
        "    dataset: str = \"blobs\"     # blobs | moons | circles\n",
        "    dim: int = 2\n",
        "    n_classes: int = 2\n",
        "    radius: float = 3.0        # blobs base radius\n",
        "    spread: float = 1.0        # blobs spread\n",
        "    extra_noise: float = 0.5   # for dims > 2\n",
        "    moons_noise: float = 0.15  # moons noise\n",
        "    circles_noise: float = 0.1 # circles noise\n",
        "    circles_factor: float = 0.5  # inner/outer radius ratio\n",
        "\n",
        "\n",
        "def make_dataset(n_samples: int, cfg: DataConfig, seed: int) -> Tuple[np.ndarray, np.ndarray]:\n",
        "    if cfg.dataset == \"blobs\":\n",
        "        return make_blobs(n_samples, cfg, seed)\n",
        "    elif cfg.dataset == \"moons\":\n",
        "        return make_moons(n_samples, cfg, seed)\n",
        "    elif cfg.dataset == \"circles\":\n",
        "        return make_circles(n_samples, cfg, seed)\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown dataset: {cfg.dataset}\")\n",
        "\n",
        "\n",
        "def pad_dims(X: np.ndarray, target_dim: int, noise: float, rng: np.random.Generator) -> np.ndarray:\n",
        "    if X.shape[1] >= target_dim:\n",
        "        return X[:, :target_dim].astype(np.float32)\n",
        "    extra = rng.normal(0, noise, size=(X.shape[0], target_dim - X.shape[1])).astype(np.float32)\n",
        "    return np.hstack([X.astype(np.float32), extra])\n",
        "\n",
        "\n",
        "def make_blobs(n_samples: int, cfg: DataConfig, seed: int) -> Tuple[np.ndarray, np.ndarray]:\n",
        "    rng = np.random.default_rng(seed)\n",
        "    n0 = n_samples // 2\n",
        "    n1 = n_samples - n0\n",
        "    ang0, ang1 = 0.0, math.pi\n",
        "    c0 = np.array([cfg.radius * math.cos(ang0), cfg.radius * math.sin(ang0)], dtype=np.float32)\n",
        "    c1 = np.array([cfg.radius * math.cos(ang1), cfg.radius * math.sin(ang1)], dtype=np.float32)\n",
        "    x0 = rng.normal(0, cfg.spread, size=(n0, 2)).astype(np.float32) + c0\n",
        "    x1 = rng.normal(0, cfg.spread, size=(n1, 2)).astype(np.float32) + c1\n",
        "    X = np.vstack([x0, x1])\n",
        "    y = np.concatenate([np.zeros(n0, dtype=np.int64), np.ones(n1, dtype=np.int64)], axis=0)\n",
        "    X = pad_dims(X, cfg.dim, cfg.extra_noise, rng)\n",
        "    idx = rng.permutation(n_samples)\n",
        "    return X[idx], y[idx]\n",
        "\n",
        "\n",
        "def make_moons(n_samples: int, cfg: DataConfig, seed: int) -> Tuple[np.ndarray, np.ndarray]:\n",
        "    rng = np.random.default_rng(seed)\n",
        "    n0 = n_samples // 2\n",
        "    n1 = n_samples - n0\n",
        "    t0 = rng.uniform(0, math.pi, size=n0)\n",
        "    t1 = rng.uniform(0, math.pi, size=n1)\n",
        "    r = 1.0\n",
        "    x0 = np.stack([r * np.cos(t0), r * np.sin(t0)], axis=1)\n",
        "    x1 = np.stack([r * np.cos(t1) + 1.0, -r * np.sin(t1) - 0.5], axis=1)\n",
        "    x0 += rng.normal(0, cfg.moons_noise, size=x0.shape)\n",
        "    x1 += rng.normal(0, cfg.moons_noise, size=x1.shape)\n",
        "    X = np.vstack([x0, x1]).astype(np.float32)\n",
        "    y = np.concatenate([np.zeros(n0, dtype=np.int64), np.ones(n1, dtype=np.int64)], axis=0)\n",
        "    X *= cfg.radius\n",
        "    X = pad_dims(X, cfg.dim, cfg.extra_noise, rng)\n",
        "    idx = rng.permutation(n_samples)\n",
        "    return X[idx], y[idx]\n",
        "\n",
        "\n",
        "def make_circles(n_samples: int, cfg: DataConfig, seed: int) -> Tuple[np.ndarray, np.ndarray]:\n",
        "    rng = np.random.default_rng(seed)\n",
        "    n0 = n_samples // 2\n",
        "    n1 = n_samples - n0\n",
        "    t0 = rng.uniform(0, 2 * math.pi, size=n0)\n",
        "    t1 = rng.uniform(0, 2 * math.pi, size=n1)\n",
        "    r_out = cfg.radius\n",
        "    r_in = cfg.radius * cfg.circles_factor\n",
        "    x0 = np.stack([r_out * np.cos(t0), r_out * np.sin(t0)], axis=1)\n",
        "    x1 = np.stack([r_in * np.cos(t1), r_in * np.sin(t1)], axis=1)\n",
        "    x0 += rng.normal(0, cfg.circles_noise, size=x0.shape)\n",
        "    x1 += rng.normal(0, cfg.circles_noise, size=x1.shape)\n",
        "    X = np.vstack([x0, x1]).astype(np.float32)\n",
        "    y = np.concatenate([np.zeros(n0, dtype=np.int64), np.ones(n1, dtype=np.int64)], axis=0)\n",
        "    X = pad_dims(X, cfg.dim, cfg.extra_noise, rng)\n",
        "    idx = rng.permutation(n_samples)\n",
        "    return X[idx], y[idx]\n",
        "\n",
        "\n",
        "def split_dataset(X: np.ndarray, y: np.ndarray, val_ratio: float, test_ratio: float, seed: int):\n",
        "    rng = np.random.default_rng(seed)\n",
        "    n = X.shape[0]\n",
        "    idx = rng.permutation(n)\n",
        "    n_test = int(test_ratio * n)\n",
        "    n_val = int(val_ratio * n)\n",
        "    test_idx = idx[:n_test]\n",
        "    val_idx = idx[n_test:n_test + n_val]\n",
        "    train_idx = idx[n_test + n_val:]\n",
        "    return (X[train_idx], y[train_idx],\n",
        "            X[val_idx], y[val_idx],\n",
        "            X[test_idx], y[test_idx])\n",
        "\n",
        "\n",
        "def to_tensor(x: np.ndarray, y: Optional[np.ndarray], device: torch.device):\n",
        "    xt = torch.tensor(x, dtype=torch.float32, device=device)\n",
        "    yt = None if y is None else torch.tensor(y, dtype=torch.long, device=device)\n",
        "    return xt, yt\n",
        "\n",
        "\n",
        "def augment_noise(x: torch.Tensor, sigma: float = 0.25) -> torch.Tensor:\n",
        "    return x + sigma * torch.randn_like(x)\n",
        "\n",
        "\n",
        "# --- Model ---------------------------------------------------------------------\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, in_dim: int, n_classes: int, hidden: int = 64, depth: int = 2, dropout: float = 0.0):\n",
        "        super().__init__()\n",
        "        layers: List[nn.Module] = []\n",
        "        last = in_dim\n",
        "        for _ in range(depth):\n",
        "            layers += [nn.Linear(last, hidden), nn.ReLU(inplace=True)]\n",
        "            if dropout > 0:\n",
        "                layers.append(nn.Dropout(dropout))\n",
        "            last = hidden\n",
        "        layers.append(nn.Linear(last, n_classes))\n",
        "        self.net = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        return self.net(x)\n",
        "\n",
        "\n",
        "# --- Loss helpers and metrics ---------------------------------------------------\n",
        "\n",
        "def entropy_from_logits(logits: torch.Tensor) -> torch.Tensor:\n",
        "    p = logits.softmax(dim=-1).clamp_min(1e-12)\n",
        "    return (-(p * p.log()).sum(dim=-1)).mean()\n",
        "\n",
        "\n",
        "def cross_entropy_soft_targets(logits: torch.Tensor, target_probs: torch.Tensor) -> torch.Tensor:\n",
        "    logp = logits.log_softmax(dim=-1)\n",
        "    return (-(target_probs * logp).sum(dim=-1)).mean()\n",
        "\n",
        "\n",
        "def one_hot(n_classes: int, labels: torch.Tensor) -> torch.Tensor:\n",
        "    return F.one_hot(labels, num_classes=n_classes).float()\n",
        "\n",
        "\n",
        "def label_smooth(target_one_hot: torch.Tensor, smoothing: float) -> torch.Tensor:\n",
        "    if smoothing <= 0.0:\n",
        "        return target_one_hot\n",
        "    K = target_one_hot.shape[-1]\n",
        "    u = torch.full_like(target_one_hot, 1.0 / K)\n",
        "    return (1.0 - smoothing) * target_one_hot + smoothing * u\n",
        "\n",
        "\n",
        "def consistency_kl(logits_a: torch.Tensor, logits_b: torch.Tensor) -> torch.Tensor:\n",
        "    pa = logits_a.softmax(dim=-1).clamp_min(1e-8)\n",
        "    pb = logits_b.softmax(dim=-1).clamp_min(1e-8)\n",
        "    kl_ab = (pa * (pa.log() - pb.log())).sum(dim=-1)\n",
        "    kl_ba = (pb * (pb.log() - pa.log())).sum(dim=-1)\n",
        "    return 0.5 * (kl_ab + kl_ba).mean()\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def accuracy(logits: torch.Tensor, y: torch.Tensor) -> float:\n",
        "    pred = logits.argmax(dim=-1)\n",
        "    return (pred == y).float().mean().item()\n",
        "\n",
        "\n",
        "# --- Training + evaluation ------------------------------------------------------\n",
        "\n",
        "@dataclass\n",
        "class TrainConfig:\n",
        "    mode: str\n",
        "    steps: int\n",
        "    batch_size: int\n",
        "    lr: float\n",
        "    wd: float\n",
        "    entropy_bonus: float\n",
        "    label_sharpen: float\n",
        "    seed: int\n",
        "    outdir: str\n",
        "    log_every: int\n",
        "    dim: int\n",
        "    n_classes: int\n",
        "    train_size: int\n",
        "    val_ratio: float\n",
        "    test_ratio: float\n",
        "    dataset: str\n",
        "    hidden: int\n",
        "    depth: int\n",
        "    dropout: float\n",
        "    device: str = \"auto\"\n",
        "    early_patience: int = 0  # 0 disables early stopping\n",
        "    scheduler_gamma: float = 0.0  # 0 disables StepLR\n",
        "    sigma_aug: float = 0.25\n",
        "\n",
        "    def device_obj(self) -> torch.device:\n",
        "        if self.device == \"cpu\":\n",
        "            return torch.device(\"cpu\")\n",
        "        if self.device == \"cuda\":\n",
        "            return torch.device(\"cuda\")\n",
        "        return torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "def make_run_dir(cfg: TrainConfig) -> str:\n",
        "    run_dir = os.path.join(cfg.outdir, f\"{ts()}_{cfg.dataset}_{cfg.mode}_seed{cfg.seed}_dim{cfg.dim}\")\n",
        "    ensure_dir(run_dir)\n",
        "    with open(os.path.join(run_dir, \"config.json\"), \"w\") as f:\n",
        "        json.dump(asdict(cfg), f, indent=2)\n",
        "    return run_dir\n",
        "\n",
        "\n",
        "def train_run(cfg: TrainConfig) -> str:\n",
        "    set_seed(cfg.seed)\n",
        "    device = cfg.device_obj()\n",
        "    run_dir = make_run_dir(cfg)\n",
        "\n",
        "    # Data\n",
        "    dc = DataConfig(dataset=cfg.dataset, dim=cfg.dim, n_classes=cfg.n_classes)\n",
        "    X, y = make_dataset(cfg.train_size + 4096, dc, seed=cfg.seed)\n",
        "    Xtr, ytr, Xval, yval, Xte, yte = split_dataset(X, y, cfg.val_ratio, cfg.test_ratio, seed=cfg.seed + 1)\n",
        "    Xtr_t, ytr_t = to_tensor(Xtr, ytr, device)\n",
        "    Xval_t, yval_t = to_tensor(Xval, yval, device)\n",
        "\n",
        "    model = MLP(cfg.dim, cfg.n_classes, hidden=cfg.hidden, depth=cfg.depth, dropout=cfg.dropout).to(device)\n",
        "    opt = torch.optim.AdamW(model.parameters(), lr=cfg.lr, weight_decay=cfg.wd)\n",
        "    sched = torch.optim.lr_scheduler.StepLR(opt, step_size=max(1, cfg.steps // 3), gamma=cfg.scheduler_gamma) \\\n",
        "        if cfg.scheduler_gamma and cfg.scheduler_gamma > 0 else None\n",
        "\n",
        "    csv_path = os.path.join(run_dir, \"metrics.csv\")\n",
        "    with open(csv_path, \"w\", newline=\"\") as f:\n",
        "        csv.writer(f).writerow([\"step\", \"loss\", \"sup\", \"ssl\", \"entropy\", \"val_acc\", \"lr\"])\n",
        "\n",
        "    best_val = -1.0\n",
        "    best_path = os.path.join(run_dir, \"model.pt\")\n",
        "    n = Xtr_t.shape[0]\n",
        "    patience_left = cfg.early_patience\n",
        "\n",
        "    for step in range(1, cfg.steps + 1):\n",
        "        model.train()\n",
        "        idx = np.random.randint(0, n, size=(cfg.batch_size,))\n",
        "        xb = Xtr_t[idx]\n",
        "        yb = ytr_t[idx]\n",
        "\n",
        "        logits = model(xb)\n",
        "\n",
        "        loss_sup = torch.tensor(0.0, device=device)\n",
        "        loss_ssl = torch.tensor(0.0, device=device)\n",
        "\n",
        "        if cfg.mode in (\"supervised\", \"hybrid\"):\n",
        "            y1h = one_hot(cfg.n_classes, yb)\n",
        "            ysoft = label_smooth(y1h, cfg.label_sharpen)\n",
        "            loss_sup = cross_entropy_soft_targets(logits, ysoft)\n",
        "\n",
        "        if cfg.mode in (\"self\", \"hybrid\"):\n",
        "            xa = augment_noise(xb, cfg.sigma_aug)\n",
        "            xb2 = augment_noise(xb, cfg.sigma_aug)\n",
        "            la = model(xa)\n",
        "            lb = model(xb2)\n",
        "            loss_ssl = consistency_kl(la, lb)\n",
        "\n",
        "        ent = entropy_from_logits(logits)\n",
        "        total = loss_sup + loss_ssl - cfg.entropy_bonus * ent\n",
        "\n",
        "        opt.zero_grad(set_to_none=True)\n",
        "        total.backward()\n",
        "        opt.step()\n",
        "        if sched:\n",
        "            sched.step()\n",
        "\n",
        "        if step % cfg.log_every == 0 or step in (1, cfg.steps):\n",
        "            model.eval()\n",
        "            with torch.no_grad():\n",
        "                val_acc = accuracy(model(Xval_t), yval_t)\n",
        "            lr_current = next(iter(opt.param_groups))[\"lr\"]\n",
        "            with open(csv_path, \"a\", newline=\"\") as f:\n",
        "                csv.writer(f).writerow([\n",
        "                    step,\n",
        "                    f\"{float(total):.6f}\",\n",
        "                    f\"{float(loss_sup):.6f}\",\n",
        "                    f\"{float(loss_ssl):.6f}\",\n",
        "                    f\"{float(ent):.6f}\",\n",
        "                    f\"{val_acc:.4f}\",\n",
        "                    f\"{lr_current:.6e}\",\n",
        "                ])\n",
        "            print(f\"[{step:5d}/{cfg.steps}] {cfg.dataset}/{cfg.mode} \"\n",
        "                  f\"loss={float(total):.4f} sup={float(loss_sup):.4f} ssl={float(loss_ssl):.4f} \"\n",
        "                  f\"ent={float(ent):.4f} val_acc={val_acc:.3f} lr={lr_current:.2e}\")\n",
        "\n",
        "            # Early stopping on best val_acc\n",
        "            if val_acc > best_val + 1e-6:\n",
        "                best_val = val_acc\n",
        "                torch.save({\n",
        "                    \"model_state\": model.state_dict(),\n",
        "                    \"in_dim\": cfg.dim,\n",
        "                    \"n_classes\": cfg.n_classes,\n",
        "                    \"config\": asdict(cfg),\n",
        "                    \"dataset\": cfg.dataset,\n",
        "                }, best_path)\n",
        "                patience_left = cfg.early_patience\n",
        "            else:\n",
        "                if cfg.early_patience > 0:\n",
        "                    patience_left -= 1\n",
        "                    if patience_left <= 0:\n",
        "                        print(f\"Early stopping at step {step} (best val_acc={best_val:.4f}).\")\n",
        "                        break\n",
        "\n",
        "    # Optional plots\n",
        "    if _HAS_MPL:\n",
        "        try_plot_metrics(csv_path, os.path.join(run_dir, \"metrics.png\"))\n",
        "        if cfg.dim == 2:\n",
        "            try:\n",
        "                payload = torch.load(best_path, map_location=device)\n",
        "                model.load_state_dict(payload[\"model_state\"])\n",
        "            except Exception:\n",
        "                pass\n",
        "            X_comb = np.vstack([Xtr[:1000], Xval[:1000]])\n",
        "            y_comb = np.concatenate([ytr[:1000], yval[:1000]], axis=0)\n",
        "            try_plot_boundary(model, X_comb, y_comb, os.path.join(run_dir, \"boundary.png\"), device)\n",
        "\n",
        "    print(f\"Done. Outputs saved to: {run_dir}\")\n",
        "    return run_dir\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def eval_run(ckpt_path: str, dataset: Optional[str] = None, dim: Optional[int] = None,\n",
        "             n_samples: int = 4096, seed: Optional[int] = None) -> float:\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    payload = torch.load(ckpt_path, map_location=device)\n",
        "    in_dim = payload.get(\"in_dim\")\n",
        "    n_classes = payload.get(\"n_classes\")\n",
        "    cfg = payload.get(\"config\", {})\n",
        "    dataset_name = dataset or payload.get(\"dataset\", cfg.get(\"dataset\", \"blobs\"))\n",
        "    in_dim = dim or in_dim\n",
        "    if in_dim is None or n_classes is None:\n",
        "        raise ValueError(\"Checkpoint missing in_dim/n_classes metadata.\")\n",
        "\n",
        "    model = MLP(in_dim, n_classes, hidden=cfg.get(\"hidden\", 64),\n",
        "                depth=cfg.get(\"depth\", 2), dropout=cfg.get(\"dropout\", 0.0)).to(device)\n",
        "    model.load_state_dict(payload[\"model_state\"])\n",
        "    model.eval()\n",
        "\n",
        "    dc = DataConfig(dataset=dataset_name, dim=in_dim, n_classes=n_classes)\n",
        "    test_seed = (cfg.get(\"seed\", 42) + 999) if seed is None else seed\n",
        "    X, y = make_dataset(n_samples, dc, seed=test_seed)\n",
        "    Xt, yt = to_tensor(X, y, device)\n",
        "    logits = model(Xt)\n",
        "    acc = accuracy(logits, yt)\n",
        "    print(f\"Eval accuracy ({dataset_name}, dim={in_dim}, n={n_samples}): {acc:.4f}\")\n",
        "\n",
        "    # Plot boundary if 2D\n",
        "    if _HAS_MPL and in_dim == 2:\n",
        "        try_plot_boundary(model, X, y, os.path.join(os.path.dirname(ckpt_path), \"boundary_eval.png\"), device)\n",
        "\n",
        "    return acc\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def predict_run(ckpt_path: str, points_csv: Optional[str], points_inline: Optional[str]) -> None:\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    payload = torch.load(ckpt_path, map_location=device)\n",
        "    in_dim = payload[\"in_dim\"]\n",
        "    n_classes = payload[\"n_classes\"]\n",
        "    cfg = payload.get(\"config\", {})\n",
        "\n",
        "    model = MLP(in_dim, n_classes, hidden=cfg.get(\"hidden\", 64),\n",
        "                depth=cfg.get(\"depth\", 2), dropout=cfg.get(\"dropout\", 0.0)).to(device)\n",
        "    model.load_state_dict(payload[\"model_state\"])\n",
        "    model.eval()\n",
        "\n",
        "    pts: List[List[float]] = []\n",
        "    if points_csv:\n",
        "        with open(points_csv, \"r\", newline=\"\") as f:\n",
        "            r = csv.reader(f)\n",
        "            for row in r:\n",
        "                if not row:\n",
        "                    continue\n",
        "                vals = [float(t) for t in row]\n",
        "                if len(vals) != in_dim:\n",
        "                    raise ValueError(f\"Each CSV row must have {in_dim} values, got {len(vals)}.\")\n",
        "                pts.append(vals)\n",
        "    elif points_inline:\n",
        "        # format: \"x1,x2; y1,y2; ...\"\n",
        "        for chunk in points_inline.split(\";\"):\n",
        "            chunk = chunk.strip()\n",
        "            if not chunk:\n",
        "                continue\n",
        "            row = [float(t) for t in chunk.split(\",\") if t.strip() != \"\"]\n",
        "            if len(row) != in_dim:\n",
        "                raise ValueError(f\"Each point must have {in_dim} values.\")\n",
        "            pts.append(row)\n",
        "    else:\n",
        "        raise ValueError(\"Provide --points-csv or --points-inline.\")\n",
        "\n",
        "    Xt = torch.tensor(np.array(pts, dtype=np.float32), device=device)\n",
        "    logits = model(Xt)\n",
        "    probs = logits.softmax(dim=-1).cpu().numpy()\n",
        "    preds = probs.argmax(axis=-1).tolist()\n",
        "\n",
        "    out = [{\"x\": p, \"pred\": int(c), \"probs\": [float(q) for q in pr]} for p, c, pr in zip(pts, preds, probs)]\n",
        "    print(json.dumps(out, indent=2))\n",
        "\n",
        "\n",
        "def export_onnx(ckpt_path: str, out_path: Optional[str] = None) -> None:\n",
        "    if not _HAS_ONNX:\n",
        "        print(\"ONNX not available. Install onnx to enable export.\")\n",
        "        return\n",
        "    device = torch.device(\"cpu\")\n",
        "    payload = torch.load(ckpt_path, map_location=device)\n",
        "    in_dim = payload[\"in_dim\"]\n",
        "    n_classes = payload[\"n_classes\"]\n",
        "    cfg = payload.get(\"config\", {})\n",
        "\n",
        "    model = MLP(in_dim, n_classes, hidden=cfg.get(\"hidden\", 64),\n",
        "                depth=cfg.get(\"depth\", 2), dropout=0.0).to(device)\n",
        "    model.load_state_dict(payload[\"model_state\"])\n",
        "    model.eval()\n",
        "\n",
        "    dummy = torch.zeros(1, in_dim, dtype=torch.float32, device=device)\n",
        "    out_path = out_path or os.path.join(os.path.dirname(ckpt_path), \"model.onnx\")\n",
        "    torch.onnx.export(\n",
        "        model, dummy, out_path,\n",
        "        input_names=[\"input\"], output_names=[\"logits\"],\n",
        "        dynamic_axes={\"input\": {0: \"batch\"}, \"logits\": {0: \"batch\"}},\n",
        "        opset_version=13,\n",
        "    )\n",
        "    print(f\"Exported ONNX to: {out_path}\")\n",
        "\n",
        "\n",
        "# --- Plotting helpers -----------------------------------------------------------\n",
        "\n",
        "def try_plot_metrics(csv_path: str, out_png: str) -> None:\n",
        "    if not os.path.exists(csv_path):\n",
        "        return\n",
        "    steps, loss, sup, ssl, ent, acc, lr = [], [], [], [], [], [], []\n",
        "    with open(csv_path, \"r\") as f:\n",
        "        reader = csv.DictReader(f)\n",
        "        for r in reader:\n",
        "            steps.append(int(r[\"step\"]))\n",
        "            loss.append(float(r[\"loss\"]))\n",
        "            sup.append(float(r[\"sup\"]))\n",
        "            ssl.append(float(r[\"ssl\"]))\n",
        "            ent.append(float(r[\"entropy\"]))\n",
        "            acc.append(float(r[\"val_acc\"]))\n",
        "            lr.append(float(r.get(\"lr\", 0.0)))\n",
        "    if not steps:\n",
        "        return\n",
        "    fig, ax1 = plt.subplots(1, 1, figsize=(8, 4.5))\n",
        "    ax1.plot(steps, loss, label=\"total\", color=\"#1f77b4\")\n",
        "    ax1.plot(steps, sup, label=\"supervised\", color=\"#ff7f0e\")\n",
        "    ax1.plot(steps, ssl, label=\"self\", color=\"#2ca02c\")\n",
        "    ax1.plot(steps, ent, label=\"entropy\", color=\"#9467bd\")\n",
        "    ax1.plot(steps, acc, label=\"val_acc\", color=\"#d62728\")\n",
        "    ax1.set_title(\"Training metrics\")\n",
        "    ax1.set_xlabel(\"step\")\n",
        "    ax1.grid(True, alpha=0.3)\n",
        "    ax1.legend(loc=\"best\")\n",
        "    if any(lr):\n",
        "        ax2 = ax1.twinx()\n",
        "        ax2.plot(steps, lr, label=\"lr\", color=\"#8c564b\", linestyle=\"--\", alpha=0.6)\n",
        "        ax2.set_ylabel(\"learning rate\")\n",
        "    fig.tight_layout()\n",
        "    fig.savefig(out_png, dpi=140)\n",
        "    plt.close(fig)\n",
        "\n",
        "\n",
        "def tryplotboundary(model: nn.Module, X: np.ndarray, y: np.ndarray, out_png: str, device: torch.device) -> None:\n",
        "    \"\"\"2D decision boundary + points.\"\"\"\n",
        "    if X.shape[1] != 2:\n",
        "        return\n",
        "    xmin, xmax = X[:, 0].min() - 1.5, X[:, 0].max() + 1.5\n",
        "    ymin, ymax = X[:, 1].min() - 1.5, X[:, 1].max() + 1.5\n",
        "    xx, yy = np.meshgrid(np.linspace(xmin, xmax, 400), np.linspace(ymin, ymax, 400))\n",
        "    grid = np.c_[xx.ravel(), yy.ravel()].astype(np.float32)\n",
        "    with torch.no_grad():\n",
        "        logits = model(torch.tensor(grid, device=device))\n",
        "        Z = logits.softmax(dim=-1)[:, 1].cpu().numpy().reshape(xx.shape)\n",
        "    fig, ax = plt.subplots(1, 1, figsize=(6.5, 5.5))\n",
        "    cs = ax.contourf(xx, yy, Z, levels=30, cmap=\"coolwarm\", alpha=0.85)\n",
        "    fig.colorbar(cs, ax=ax, label=\"P(class=1)\")\n",
        "    ax.scatter(X[:, 0], X[:, 1], c=y, cmap=\"bwr\", edgecolor=\"k\", s=12, alpha=0.8)\n",
        "    ax.set_title(\"Decision boundary\")\n",
        "    ax.set_xlabel(\"x1\")\n",
        "    ax.set_ylabel(\"y1\")\n",
        "    ax.grid(True, alpha=0.2)\n",
        "    fig.tight_layout()\n",
        "    fig.savefig(out_png, dpi=140)\n",
        "    plt.close(fig)\n",
        "\n",
        "\n",
        "# --- Benchmark ------------------------------------------------------------------\n",
        "\n",
        "@dataclass\n",
        "class BenchmarkCase:\n",
        "    mode: str\n",
        "    dataset: str\n",
        "    dim: int\n",
        "    seed: int\n",
        "\n",
        "\n",
        "def benchmarkrun(outcsv: str,\n",
        "                  modes: List[str],\n",
        "                  datasets: List[str],\n",
        "                  dims: List[int],\n",
        "                  seeds: List[int],\n",
        "                  base_cfg: Dict) -> None:\n",
        "    rows = []\n",
        "    for ds in datasets:\n",
        "        for mode in modes:\n",
        "            for d in dims:\n",
        "                for s in seeds:\n",
        "                    cfg = TrainConfig(\n",
        "                        mode=mode,\n",
        "                        steps=int(base_cfg.get(\"steps\", 300)),\n",
        "                        batchsize=int(basecfg.get(\"batch_size\", 128)),\n",
        "                        lr=float(base_cfg.get(\"lr\", 1e-3)),\n",
        "                        wd=float(base_cfg.get(\"wd\", 0.0)),\n",
        "                        entropybonus=float(basecfg.get(\"entropy_bonus\", 0.0)),\n",
        "                        labelsharpen=float(basecfg.get(\"label_sharpen\", 0.0)),\n",
        "                        seed=s,\n",
        "                        outdir=str(base_cfg.get(\"outdir\", \"runs\")),\n",
        "                        logevery=int(basecfg.get(\"log_every\", 100)),\n",
        "                        dim=d,\n",
        "                        nclasses=int(basecfg.get(\"n_classes\", 2)),\n",
        "                        trainsize=int(basecfg.get(\"train_size\", 8192)),\n",
        "                        valratio=float(basecfg.get(\"val_ratio\", 0.15)),\n",
        "                        testratio=float(basecfg.get(\"test_ratio\", 0.15)),\n",
        "                        dataset=ds,\n",
        "                        hidden=int(base_cfg.get(\"hidden\", 64)),\n",
        "                        depth=int(base_cfg.get(\"depth\", 2)),\n",
        "                        dropout=float(base_cfg.get(\"dropout\", 0.0)),\n",
        "                        device=str(base_cfg.get(\"device\", \"auto\")),\n",
        "                        earlypatience=int(basecfg.get(\"early_patience\", 0)),\n",
        "                        schedulergamma=float(basecfg.get(\"scheduler_gamma\", 0.0)),\n",
        "                        sigmaaug=float(basecfg.get(\"sigma_aug\", 0.25)),\n",
        "                    )\n",
        "                    print(f\"=== Benchmark: {ds} / {mode} / dim={d} / seed={s} ===\")\n",
        "                    rundir = trainrun(cfg)\n",
        "                    ckpt = os.path.join(run_dir, \"model.pt\")\n",
        "                    acc = evalrun(ckpt, dataset=ds, dim=d, nsamples=int(basecfg.get(\"benchn\", 4096)),\n",
        "                                   seed=s + 12345)\n",
        "                    rows.append({\n",
        "                        \"dataset\": ds, \"mode\": mode, \"dim\": d, \"seed\": s,\n",
        "                        \"rundir\": rundir, \"valbestacc\": acc\n",
        "                    })\n",
        "\n",
        "    ensuredir(os.path.dirname(outcsv) or \".\")\n",
        "    with open(out_csv, \"w\", newline=\"\") as f:\n",
        "        w = csv.DictWriter(f, fieldnames=[\"dataset\", \"mode\", \"dim\", \"seed\", \"rundir\", \"valbest_acc\"])\n",
        "        w.writeheader()\n",
        "        for r in rows:\n",
        "            w.writerow(r)\n",
        "    print(f\"Benchmark summary saved to: {out_csv}\")\n",
        "\n",
        "\n",
        "# --- CLI -----------------------------------------------------------------------\n",
        "\n",
        "def build_parser() -> argparse.ArgumentParser:\n",
        "    p = argparse.ArgumentParser(description=\"MetaIntelligence end-to-end toolkit (notebook/Colab-safe)\")\n",
        "    sub = p.add_subparsers(dest=\"cmd\")\n",
        "\n",
        "    # Train\n",
        "    t = sub.add_parser(\"train\", help=\"Train a model (self/supervised/hybrid)\")\n",
        "    t.add_argument(\"--mode\", choices=[\"self\", \"supervised\", \"hybrid\"], default=\"supervised\")\n",
        "    t.add_argument(\"--dataset\", choices=[\"blobs\", \"moons\", \"circles\"], default=\"blobs\")\n",
        "    t.add_argument(\"--steps\", type=int, default=500)\n",
        "    t.add_argument(\"--batch-size\", type=int, default=128)\n",
        "    t.add_argument(\"--lr\", type=float, default=1e-3)\n",
        "    t.add_argument(\"--wd\", type=float, default=0.0)\n",
        "    t.add_argument(\"--entropy-bonus\", type=float, default=0.0)\n",
        "    t.add_argument(\"--label-sharpen\", type=float, default=0.0)\n",
        "    t.add_argument(\"--seed\", type=int, default=42)\n",
        "    t.add_argument(\"--outdir\", type=str, default=\"runs\")\n",
        "    t.add_argument(\"--log-every\", type=int, default=50)\n",
        "    t.add_argument(\"--dim\", type=int, default=2)\n",
        "    t.add_argument(\"--n-classes\", type=int, default=2)\n",
        "    t.add_argument(\"--train-size\", type=int, default=8192)\n",
        "    t.add_argument(\"--val-ratio\", type=float, default=0.15)\n",
        "    t.add_argument(\"--test-ratio\", type=float, default=0.15)\n",
        "    t.add_argument(\"--hidden\", type=int, default=64)\n",
        "    t.add_argument(\"--depth\", type=int, default=2)\n",
        "    t.add_argument(\"--dropout\", type=float, default=0.0)\n",
        "    t.add_argument(\"--device\", type=str, default=\"auto\", choices=[\"auto\", \"cpu\", \"cuda\"])\n",
        "    t.add_argument(\"--early-patience\", type=int, default=0, help=\"Early stopping patience (0 disables)\")\n",
        "    t.add_argument(\"--scheduler-gamma\", type=float, default=0.0, help=\"StepLR gamma (0 disables)\")\n",
        "    t.add_argument(\"--sigma-aug\", type=float, default=0.25, help=\"Noise sigma for self/hybrid consistency\")\n",
        "\n",
        "    # Eval\n",
        "    e = sub.add_parser(\"eval\", help=\"Evaluate a saved checkpoint on a fresh synthetic test set\")\n",
        "    e.add_argument(\"--ckpt\", type=str, required=True, help=\"Path to model.pt\")\n",
        "    e.add_argument(\"--dataset\", type=str, default=None, help=\"Override dataset used for eval\")\n",
        "    e.add_argument(\"--dim\", type=int, default=None, help=\"Override input dim used for eval\")\n",
        "    e.add_argument(\"--n\", type=int, default=4096, help=\"Number of samples for eval\")\n",
        "    e.add_argument(\"--seed\", type=int, default=None, help=\"Random seed for eval set\")\n",
        "\n",
        "    # Predict\n",
        "    p2 = sub.add_parser(\"predict\", help=\"Predict for given points\")\n",
        "    p2.add_argument(\"--ckpt\", type=str, required=True, help=\"Path to model.pt\")\n",
        "    p2.add_argument(\"--points-csv\", type=str, default=None, help=\"CSV file without header, each row is a point\")\n",
        "    p2.add_argument(\"--points-inline\", type=str, default=None,\n",
        "                    help='Inline points, e.g., \"x1,x2; y1,y2\" (must match input dim)')\n",
        "\n",
        "    # Benchmark\n",
        "    b = sub.add_parser(\"benchmark\", help=\"Run grid of modes/datasets/dims across seeds and summarize\")\n",
        "    b.add_argument(\"--modes\", type=str, default=\"supervised,hybrid\", help=\"Comma list of modes\")\n",
        "    b.add_argument(\"--datasets\", type=str, default=\"blobs,moons,circles\", help=\"Comma list of datasets\")\n",
        "    b.add_argument(\"--dims\", type=str, default=\"2\", help=\"Comma list of dims (e.g., '2,4')\")\n",
        "    b.add_argument(\"--seeds\", type=str, default=\"1,2,3\", help=\"Comma list of seeds\")\n",
        "    b.add_argument(\"--summary\", type=str, default=\"runs/benchmarksummary.csv\", help=\"Output CSV\")\n",
        "    b.add_argument(\"--steps\", type=int, default=300)\n",
        "    b.add_argument(\"--batch-size\", type=int, default=128)\n",
        "    b.add_argument(\"--lr\", type=float, default=1e-3)\n",
        "    b.add_argument(\"--wd\", type=float, default=0.0)\n",
        "    b.add_argument(\"--entropy-bonus\", type=float, default=0.0)\n",
        "    b.add_argument(\"--label-sharpen\", type=float, default=0.0)\n",
        "    b.add_argument(\"--outdir\", type=str, default=\"runs\")\n",
        "    b.add_argument(\"--log-every\", type=int, default=100)\n",
        "    b.add_argument(\"--n-classes\", type=int, default=2)\n",
        "    b.add_argument(\"--train-size\", type=int, default=8192)\n",
        "    b.add_argument(\"--val-ratio\", type=float, default=0.15)\n",
        "    b.add_argument(\"--test-ratio\", type=float, default=0.15)\n",
        "    b.add_argument(\"--hidden\", type=int, default=64)\n",
        "    b.add_argument(\"--depth\", type=int, default=2)\n",
        "    b.add_argument(\"--dropout\", type=float, default=0.0)\n",
        "    b.add_argument(\"--device\", type=str, default=\"auto\", choices=[\"auto\", \"cpu\", \"cuda\"])\n",
        "    b.add_argument(\"--early-patience\", type=int, default=0)\n",
        "    b.add_argument(\"--scheduler-gamma\", type=float, default=0.0)\n",
        "    b.add_argument(\"--sigma-aug\", type=float, default=0.25)\n",
        "    b.add_argument(\"--bench-n\", type=int, default=4096, help=\"Samples per eval during benchmark\")\n",
        "\n",
        "    # ONNX export\n",
        "    o = sub.add_parser(\"export-onnx\", help=\"Export a trained checkpoint to ONNX\")\n",
        "    o.add_argument(\"--ckpt\", type=str, required=True, help=\"Path to model.pt\")\n",
        "    o.add_argument(\"--out\", type=str, default=None, help=\"Output .onnx path\")\n",
        "\n",
        "    return p\n",
        "\n",
        "\n",
        "def main(argv: Optional[List[str]] = None) -> None:\n",
        "    argv = sanitize_argv(argv)\n",
        "    parser = build_parser()\n",
        "    if not argv:\n",
        "        parser.print_help()\n",
        "        return\n",
        "    args,  = parser.parseknown_args(argv)\n",
        "\n",
        "    if args.cmd == \"train\":\n",
        "        cfg = TrainConfig(\n",
        "            mode=args.mode,\n",
        "            steps=args.steps,\n",
        "            batchsize=args.batchsize,\n",
        "            lr=args.lr,\n",
        "            wd=args.wd,\n",
        "            entropybonus=args.entropybonus,\n",
        "            labelsharpen=args.labelsharpen,\n",
        "            seed=args.seed,\n",
        "            outdir=args.outdir,\n",
        "            logevery=args.logevery,\n",
        "            dim=args.dim,\n",
        "            nclasses=args.nclasses,\n",
        "            trainsize=args.trainsize,\n",
        "            valratio=args.valratio,\n",
        "            testratio=args.testratio,\n",
        "            dataset=args.dataset,\n",
        "            hidden=args.hidden,\n",
        "            depth=args.depth,\n",
        "            dropout=args.dropout,\n",
        "            device=args.device,\n",
        "            earlypatience=args.earlypatience,\n",
        "            schedulergamma=args.schedulergamma,\n",
        "            sigmaaug=args.sigmaaug,\n",
        "        )\n",
        "        train_run(cfg)\n",
        "\n",
        "    elif args.cmd == \"eval\":\n",
        "        evalrun(args.ckpt, dataset=args.dataset, dim=args.dim, nsamples=args.n, seed=args.seed)\n",
        "\n",
        "    elif args.cmd == \"predict\":\n",
        "        predictrun(args.ckpt, args.pointscsv, args.points_inline)\n",
        "\n",
        "    elif args.cmd == \"benchmark\":\n",
        "        modes = [m.strip() for m in args.modes.split(\",\") if m.strip()]\n",
        "        datasets = [d.strip() for d in args.datasets.split(\",\") if d.strip()]\n",
        "        dims = [int(x) for x in args.dims.split(\",\") if x.strip()]\n",
        "        seeds = [int(x) for x in args.seeds.split(\",\") if x.strip()]\n",
        "        base_cfg = dict(\n",
        "            steps=args.steps, batchsize=args.batchsize, lr=args.lr, wd=args.wd,\n",
        "            entropybonus=args.entropybonus, labelsharpen=args.labelsharpen,\n",
        "            outdir=args.outdir, logevery=args.logevery, nclasses=args.nclasses,\n",
        "            trainsize=args.trainsize, valratio=args.valratio, testratio=args.testratio,\n",
        "            hidden=args.hidden, depth=args.depth, dropout=args.dropout, device=args.device,\n",
        "            earlypatience=args.earlypatience, schedulergamma=args.schedulergamma,\n",
        "            sigmaaug=args.sigmaaug, benchn=args.benchn\n",
        "        )\n",
        "        benchmarkrun(args.summary, modes, datasets, dims, seeds, basecfg)\n",
        "\n",
        "    elif args.cmd == \"export-onnx\":\n",
        "        export_onnx(args.ckpt, args.out)\n",
        "\n",
        "    else:\n",
        "        parser.print_help()\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    try:\n",
        "        main()\n",
        "    except SystemExit:\n",
        "        print(\"Use 'exit', 'quit', or Ctrl-D to exit.\", file=sys.stderr)\n",
        "        raise"
      ]
    }
  ]
}