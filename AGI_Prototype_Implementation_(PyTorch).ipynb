{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyMgGLDYYFvotpka04FDjaND",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OneFineStarstuff/Cosmic-Brilliance/blob/main/AGI_Prototype_Implementation_(PyTorch).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BVV3GwfLj6HQ"
      },
      "outputs": [],
      "source": [
        "pip install torch numpy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class DifferentiableMemory(nn.Module):\n",
        "    \"\"\"\n",
        "    A simple Neural Turing Machine–style memory module with\n",
        "    content-based read/write heads.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 memory_size: int,\n",
        "                 vector_dim: int,\n",
        "                 controller_dim: int,\n",
        "                 num_read_heads: int = 1,\n",
        "                 num_write_heads: int = 1):\n",
        "        super().__init__()\n",
        "        self.memory_size = memory_size\n",
        "        self.vector_dim = vector_dim\n",
        "        self.controller_dim = controller_dim\n",
        "        self.num_read_heads = num_read_heads\n",
        "        self.num_write_heads = num_write_heads\n",
        "\n",
        "        # Initialize memory to zeros (stateful, updated in forward)\n",
        "        self.register_buffer('memory',\n",
        "                             torch.zeros(memory_size, vector_dim))\n",
        "\n",
        "        # Read‐key projection: controller_state → (num_read_heads × vector_dim)\n",
        "        self.read_key_proj = nn.Linear(controller_dim,\n",
        "                                       num_read_heads * vector_dim)\n",
        "\n",
        "        # Write‐key projection: controller_state → (num_write_heads × vector_dim)\n",
        "        self.write_key_proj = nn.Linear(controller_dim,\n",
        "                                        num_write_heads * vector_dim)\n",
        "\n",
        "        # Erase/Add/Gate projections for writes\n",
        "        self.erase_proj = nn.Linear(controller_dim,\n",
        "                                    num_write_heads * vector_dim)\n",
        "        self.add_proj = nn.Linear(controller_dim,\n",
        "                                  num_write_heads * vector_dim)\n",
        "        self.write_gate_proj = nn.Linear(controller_dim,\n",
        "                                         num_write_heads)\n",
        "\n",
        "    def reset_memory(self):\n",
        "        \"\"\"Zero out memory before each new sequence (if desired).\"\"\"\n",
        "        self.memory.zero_()\n",
        "\n",
        "    def content_addressing(self, keys: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Given keys of shape (heads, D), returns a weight matrix\n",
        "        (heads × memory_size) via cosine‐similarity + softmax.\n",
        "        \"\"\"\n",
        "        # Normalize along dim\n",
        "        mem_norm = F.normalize(self.memory, dim=1)          # [M × D]\n",
        "        key_norm = F.normalize(keys, dim=1)                 # [H × D]\n",
        "\n",
        "        # similarity[h, i] = key_norm[h] ⋅ mem_norm[i]\n",
        "        similarity = torch.matmul(key_norm, mem_norm.t())   # [H × M]\n",
        "        weights = F.softmax(similarity, dim=1)              # [H × M]\n",
        "        return weights\n",
        "\n",
        "    def read(self, controller_state: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        controller_state: [hidden_dim]\n",
        "        returns:\n",
        "          read_vector: [heads × D] flattened → [heads*D]\n",
        "        \"\"\"\n",
        "        # 1) compute keys\n",
        "        rk = self.read_key_proj(controller_state)                      # [H*D]\n",
        "        rk = rk.view(self.num_read_heads, self.vector_dim)            # [H × D]\n",
        "\n",
        "        # 2) content addressing\n",
        "        w = self.content_addressing(rk)                               # [H × M]\n",
        "\n",
        "        # 3) weighted sum\n",
        "        read_vecs = torch.matmul(w, self.memory)                      # [H × D]\n",
        "        return read_vecs.view(-1), w                                  # ([H*D], [H × M])\n",
        "\n",
        "    def write(self, controller_state: torch.Tensor):\n",
        "        \"\"\"\n",
        "        controller_state: [hidden_dim]\n",
        "        Updates self.memory in place via:\n",
        "          m ← m * (1 – g * wᵀ erase) + g * wᵀ add\n",
        "        where w ∈ ℝ^{H×M}, erase/add ∈ ℝ^{H×D}, g ∈ ℝ^{H}\n",
        "        \"\"\"\n",
        "        # 1) compute write keys & weights\n",
        "        wk = self.write_key_proj(controller_state)                    # [H*D]\n",
        "        wk = wk.view(self.num_write_heads, self.vector_dim)          # [H × D]\n",
        "        w = self.content_addressing(wk)                               # [H × M]\n",
        "\n",
        "        # 2) gates & erase/add\n",
        "        erase = torch.sigmoid(self.erase_proj(controller_state))      # [H*D]\n",
        "        erase = erase.view(self.num_write_heads, self.vector_dim)    # [H × D]\n",
        "\n",
        "        add = torch.tanh(self.add_proj(controller_state))             # [H*D]\n",
        "        add = add.view(self.num_write_heads, self.vector_dim)        # [H × D]\n",
        "\n",
        "        g = torch.sigmoid(self.write_gate_proj(controller_state))     # [H]\n",
        "\n",
        "        # 3) update memory\n",
        "        # For each head h: m ← m * (1 - g_h * w_h.unsqueeze(1) * erase_h.unsqueeze(0))\n",
        "        #                  + g_h * w_h.unsqueeze(1) * add_h.unsqueeze(0)\n",
        "        M = self.memory.clone()\n",
        "        for h in range(self.num_write_heads):\n",
        "            w_h = w[h].unsqueeze(1)       # [M × 1]\n",
        "            e_h = erase[h].unsqueeze(0)   # [1 × D]\n",
        "            a_h = add[h].unsqueeze(0)     # [1 × D]\n",
        "            gate = g[h]\n",
        "\n",
        "            M = M * (1 - gate * (w_h * e_h)) + gate * (w_h * a_h)\n",
        "\n",
        "        # overwrite memory\n",
        "        self.memory.copy_(M)\n",
        "\n",
        "\n",
        "class AGI_Core(nn.Module):\n",
        "    \"\"\"\n",
        "    An LSTM‐based controller that reads from and writes to\n",
        "    a DifferentiableMemory at each time step.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 input_dim: int,\n",
        "                 hidden_dim: int,\n",
        "                 output_dim: int,\n",
        "                 memory_size: int,\n",
        "                 vector_dim: int,\n",
        "                 num_read_heads: int = 1,\n",
        "                 num_write_heads: int = 1):\n",
        "        super().__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.num_read_heads = num_read_heads\n",
        "        self.vector_dim = vector_dim\n",
        "\n",
        "        # Memory module\n",
        "        self.memory = DifferentiableMemory(\n",
        "            memory_size,\n",
        "            vector_dim,\n",
        "            controller_dim=hidden_dim,\n",
        "            num_read_heads=num_read_heads,\n",
        "            num_write_heads=num_write_heads,\n",
        "        )\n",
        "\n",
        "        # Controller: LSTM expects [input + read_vec] per time step\n",
        "        self.lstm = nn.LSTM(input_dim + num_read_heads * vector_dim,\n",
        "                            hidden_dim,\n",
        "                            batch_first=True)\n",
        "\n",
        "        # Final output head\n",
        "        self.fc = nn.Linear(hidden_dim + num_read_heads * vector_dim,\n",
        "                            output_dim)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        x: [batch=1, seq_len, input_dim]\n",
        "        returns:\n",
        "          outputs: [1, seq_len, output_dim]\n",
        "        \"\"\"\n",
        "        batch_size, seq_len, _ = x.size()\n",
        "        assert batch_size == 1, \"Current impl assumes batch size = 1\"\n",
        "\n",
        "        # reset memory & controller states\n",
        "        self.memory.reset_memory()\n",
        "        h = torch.zeros(1, 1, self.hidden_dim, device=x.device)\n",
        "        c = torch.zeros(1, 1, self.hidden_dim, device=x.device)\n",
        "\n",
        "        # initial read vector = zeros\n",
        "        read_vec = torch.zeros(1, self.num_read_heads * self.vector_dim,\n",
        "                               device=x.device)\n",
        "\n",
        "        outputs = []\n",
        "        for t in range(seq_len):\n",
        "            xi = x[:, t, :]                                      # [1 × input_dim]\n",
        "            inp = torch.cat([xi, read_vec], dim=-1).unsqueeze(1) # [1,1,input+read]\n",
        "\n",
        "            lstm_out, (h, c) = self.lstm(inp, (h, c))            # lstm_out: [1,1,hidden]\n",
        "            controller_state = h.squeeze(0).squeeze(0)           # [hidden_dim]\n",
        "\n",
        "            # write then read\n",
        "            self.memory.write(controller_state)\n",
        "            read_vec, _ = self.memory.read(controller_state)     # ([1×read_dim], [H×M])\n",
        "            read_vec = read_vec.unsqueeze(0)                     # [1 × read_dim]\n",
        "\n",
        "            # final output uses both controller_state and read_vec\n",
        "            fc_in = torch.cat([controller_state.unsqueeze(0), read_vec],\n",
        "                              dim=-1)                            # [1, hidden+read]\n",
        "            out = self.fc(fc_in).unsqueeze(1)                    # [1,1,output_dim]\n",
        "            outputs.append(out)\n",
        "\n",
        "        return torch.cat(outputs, dim=1)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Example usage\n",
        "    input_dim = 10\n",
        "    hidden_dim = 20\n",
        "    output_dim = 5\n",
        "\n",
        "    memory_size = 100\n",
        "    vector_dim = 16\n",
        "    num_read_heads = 1\n",
        "    num_write_heads = 1\n",
        "\n",
        "    agi = AGI_Core(input_dim,\n",
        "                   hidden_dim,\n",
        "                   output_dim,\n",
        "                   memory_size,\n",
        "                   vector_dim,\n",
        "                   num_read_heads,\n",
        "                   num_write_heads)\n",
        "\n",
        "    # Dummy input: batch=1, seq_len=3\n",
        "    sample = torch.randn(1, 3, input_dim)\n",
        "    out = agi(sample)\n",
        "\n",
        "    print(\"AGI Output Shape:\", out.shape)  # → [1, 3, 5]\n",
        "    print(out)"
      ],
      "metadata": {
        "id": "j2MRb7GElyF_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}