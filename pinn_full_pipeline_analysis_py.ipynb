{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyNJqAOd/CVzwm58OeL1vABp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OneFineStarstuff/Cosmic-Brilliance/blob/main/pinn_full_pipeline_analysis_py.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ZPsK-vAtcyH"
      },
      "outputs": [],
      "source": [
        "pip install torch numpy matplotlib scikit-learn scipy umap-learn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# pinn_full_pipeline_analysis.py\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from scipy.stats import pearsonr\n",
        "import umap\n",
        "import random\n",
        "\n",
        "# Reproducibility\n",
        "SEED = 42\n",
        "torch.manual_seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "random.seed(SEED)\n",
        "\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# 1. Synthetic Data Generators\n",
        "# -----------------------------------------------------------------------------\n",
        "def generate_dissolution_data(n_samples=500):\n",
        "    k = 0.5\n",
        "    C0 = 1.0\n",
        "    t = np.linspace(0, 10, n_samples)[:, None]\n",
        "    C = C0 * np.exp(-k * t) + 0.01 * np.random.randn(*t.shape)\n",
        "    return torch.tensor(t, dtype=torch.float32), torch.tensor(C, dtype=torch.float32)\n",
        "\n",
        "\n",
        "def generate_accumulation_data(n_samples=500):\n",
        "    k = 0.3\n",
        "    M0 = 0.5\n",
        "    t = np.linspace(0, 10, n_samples)[:, None]\n",
        "    M = M0 * np.exp(k * t) + 0.01 * np.random.randn(*t.shape)\n",
        "    return torch.tensor(t, dtype=torch.float32), torch.tensor(M, dtype=torch.float32)\n",
        "\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# 2. PINN Model Definition\n",
        "# -----------------------------------------------------------------------------\n",
        "class PINN(nn.Module):\n",
        "    def __init__(self, in_dim=1, hidden=64, out_dim=1):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(in_dim, hidden),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(hidden, hidden),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(hidden, out_dim),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# 3. Physics Residual Functions\n",
        "# -----------------------------------------------------------------------------\n",
        "def residual_dissolution(model, t):\n",
        "    t.requires_grad_(True)\n",
        "    C_pred = model(t)\n",
        "    dC_dt = torch.autograd.grad(C_pred.sum(), t, create_graph=True)[0]\n",
        "    return dC_dt + 0.5 * C_pred  # dC/dt + k C = 0\n",
        "\n",
        "\n",
        "def residual_accumulation(model, t):\n",
        "    t.requires_grad_(True)\n",
        "    M_pred = model(t)\n",
        "    dM_dt = torch.autograd.grad(M_pred.sum(), t, create_graph=True)[0]\n",
        "    return dM_dt - 0.3 * M_pred  # dM/dt - k M = 0\n",
        "\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# 4. Training Loop\n",
        "# -----------------------------------------------------------------------------\n",
        "def train_pinn(model, loader, res_fn, epochs=100, phy_lambda=1.0, lr=1e-3, tag=\"PINN\"):\n",
        "    opt = optim.Adam(model.parameters(), lr=lr)\n",
        "    mse = nn.MSELoss()\n",
        "    for ep in range(1, epochs + 1):\n",
        "        data_loss = 0.0\n",
        "        phy_loss = 0.0\n",
        "        for t_b, y_b in loader:\n",
        "            opt.zero_grad()\n",
        "            y_pred = model(t_b)\n",
        "            ld = mse(y_pred, y_b)\n",
        "            res = res_fn(model, t_b)\n",
        "            lp = mse(res, torch.zeros_like(res))\n",
        "            loss = ld + phy_lambda * lp\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "            data_loss += ld.item()\n",
        "            phy_loss += lp.item()\n",
        "\n",
        "        print(f\"[{tag}] Epoch {ep:3d}/{epochs} â€” Data: {data_loss/len(loader):.4f}, Phy: {phy_loss/len(loader):.4f}\")\n",
        "    return model\n",
        "\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# 5. MC-Dropout Inference\n",
        "# -----------------------------------------------------------------------------\n",
        "def mc_dropout_predict(model, t, samples=50):\n",
        "    model.train()  # keep dropout on (if any)\n",
        "    preds = [model(t).detach().cpu().numpy() for _ in range(samples)]\n",
        "    arr = np.stack(preds, axis=0)\n",
        "    return arr.mean(axis=0).flatten(), arr.std(axis=0).flatten()\n",
        "\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# 6. Main Pipeline\n",
        "# -----------------------------------------------------------------------------\n",
        "def main():\n",
        "    # Generate synthetic datasets\n",
        "    t_diss, C = generate_dissolution_data()\n",
        "    t_acc, M = generate_accumulation_data()\n",
        "\n",
        "    # Create DataLoaders\n",
        "    bs = 128\n",
        "    loader_d = DataLoader(TensorDataset(t_diss, C), batch_size=bs, shuffle=True)\n",
        "    loader_a = DataLoader(TensorDataset(t_acc, M), batch_size=bs, shuffle=True)\n",
        "\n",
        "    # Instantiate and train models\n",
        "    model_d = train_pinn(PINN(), loader_d, residual_dissolution, tag=\"DissolutionAI\")\n",
        "    model_a = train_pinn(PINN(), loader_a, residual_accumulation, tag=\"PreComputationalAI\")\n",
        "\n",
        "    # Inference grid\n",
        "    t_full = torch.linspace(0, 10, 500)[:, None]\n",
        "    mean_d, std_d = mc_dropout_predict(model_d, t_full)\n",
        "    mean_a, std_a = mc_dropout_predict(model_a, t_full)\n",
        "\n",
        "    print(\"\\nDissolutionAI Mean shape:\", mean_d.shape, \"Std shape:\", std_d.shape)\n",
        "    print(\"PreComputationalAI Mean shape:\", mean_a.shape, \"Std shape:\", std_a.shape)\n",
        "\n",
        "    # Extract latent features (last hidden layer) for UMAP\n",
        "    def extract_features(model, t):\n",
        "        x = model.net[0](t)\n",
        "        x = torch.tanh(x)\n",
        "        x = model.net[2](x)\n",
        "        x = torch.tanh(x)\n",
        "        return x.detach().cpu().numpy()\n",
        "\n",
        "    feats_d = extract_features(model_d, t_full)\n",
        "    feats_a = extract_features(model_a, t_full)\n",
        "    feats = np.vstack([feats_d, feats_a])\n",
        "\n",
        "    # UMAP projection\n",
        "    reducer = umap.UMAP(n_components=2, random_state=SEED)\n",
        "    emb = reducer.fit_transform(feats)\n",
        "    print(\"UMAP embedding computed:\", emb.shape)\n",
        "\n",
        "    # -------------------------\n",
        "    # Analysis: Visualization\n",
        "    # -------------------------\n",
        "    std_all = np.concatenate([std_d, std_a])\n",
        "    labels = np.array([0]*len(std_d) + [1]*len(std_a))\n",
        "\n",
        "    plt.figure(figsize=(7, 6))\n",
        "    plt.scatter(emb[:500, 0], emb[:500, 1], c='C0', alpha=0.6, label='Dissolution')\n",
        "    plt.scatter(emb[500:, 0], emb[500:, 1], c='C1', alpha=0.6, label='Accumulation')\n",
        "    plt.title('UMAP of PINN Latent Features')\n",
        "    plt.xlabel('UMAP1')\n",
        "    plt.ylabel('UMAP2')\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # -------------------------\n",
        "    # Analysis: Clustering\n",
        "    # -------------------------\n",
        "    kmeans = KMeans(n_clusters=2, random_state=SEED)\n",
        "    cluster_ids = kmeans.fit_predict(emb)\n",
        "    sil = silhouette_score(emb, cluster_ids)\n",
        "    print(f\"KMeans Silhouette Score: {sil:.4f}\")\n",
        "\n",
        "    plt.figure(figsize=(7, 6))\n",
        "    plt.scatter(emb[:, 0], emb[:, 1], c=cluster_ids, cmap='tab10', alpha=0.6)\n",
        "    centers = kmeans.cluster_centers_\n",
        "    plt.scatter(centers[:, 0], centers[:, 1], c='k', s=80, marker='X', label='Centers')\n",
        "    plt.title('KMeans on UMAP Embedding')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # -------------------------\n",
        "    # Analysis: Classification\n",
        "    # -------------------------\n",
        "    clf = LogisticRegression()\n",
        "    clf.fit(emb, labels)\n",
        "    acc = clf.score(emb, labels)\n",
        "    print(f\"Logistic Regression Accuracy on embeddings: {acc:.4f}\")\n",
        "\n",
        "    # -------------------------\n",
        "    # Analysis: Uncertainty vs Distance\n",
        "    # -------------------------\n",
        "    dists = np.linalg.norm(emb - centers[cluster_ids], axis=1)\n",
        "    corr, pval = pearsonr(dists, std_all)\n",
        "    print(f\"Pearson r (distance vs uncertainty): {corr:.4f}, p={pval:.2e}\")\n",
        "\n",
        "    plt.figure(figsize=(6, 4))\n",
        "    plt.scatter(dists, std_all, alpha=0.6)\n",
        "    plt.xlabel('Distance to Cluster Center')\n",
        "    plt.ylabel('MC-Dropout Std')\n",
        "    plt.title('Uncertainty vs Embedding Distance')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "vnvf-OiJtkRs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}