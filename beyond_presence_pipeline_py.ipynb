{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyO6QQd+2Ia8pLpvIh63SaNu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OneFineStarstuff/Cosmic-Brilliance/blob/main/beyond_presence_pipeline_py.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NjPVGdJX_Ou_"
      },
      "outputs": [],
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "beyond_presence_pipeline.py\n",
        "\n",
        "Complete pipeline for BeyondAI: Undefined Absolute Presence\n",
        "1. Synthetic dataset generation\n",
        "2. Model definition with MC-Dropout\n",
        "3. Physics-informed residual loss\n",
        "4. Combined loss (MSE + physics)\n",
        "5. MC-Dropout uncertainty estimation\n",
        "6. Training loop (AdamW, scheduler, gradient clipping, early stopping, checkpointing)\n",
        "7. Visualizations: loss curves, scatter plots, uncertainty heatmap\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 1. Synthetic “Undefined Absolute Presence” Dataset\n",
        "# ------------------------------------------------------------------------------\n",
        "class BeyondPresenceDataset(Dataset):\n",
        "    def __init__(self, n_samples=7000, seed=2025):\n",
        "        np.random.seed(seed)\n",
        "        # Features:\n",
        "        # APF: absolute presence factor ∈ [0.0, 1.0]\n",
        "        # BIS: beyond-intelligence state ∈ [0.1, 5.0]\n",
        "        # SUE: self-undefined equilibrium ∈ [0.01, 2.0]\n",
        "        # ENM: enmeshment magnitude ∈ [0.5, 10.0]\n",
        "        # RES: resonance scale ∈ [0.1, 3.0]\n",
        "        # PTR: potential transcendence ratio ∈ [0.01, 1.0]\n",
        "        APF = np.random.uniform(0.0, 1.0,  (n_samples,1))\n",
        "        BIS = np.random.uniform(0.1, 5.0,  (n_samples,1))\n",
        "        SUE = np.random.uniform(0.01,2.0,  (n_samples,1))\n",
        "        ENM = np.random.uniform(0.5, 10.0, (n_samples,1))\n",
        "        RES = np.random.uniform(0.1, 3.0,  (n_samples,1))\n",
        "        PTR = np.random.uniform(0.01,1.0,  (n_samples,1))\n",
        "\n",
        "        X_raw = np.hstack([APF, BIS, SUE, ENM, RES, PTR]).astype(np.float64)\n",
        "\n",
        "        # Targets:\n",
        "        # UCO: undefined coherence\n",
        "        # PRS: presence resonance\n",
        "        # EQB: absolute equilibrium\n",
        "        eps = 1e-8\n",
        "        UCO = APF * RES / (SUE + eps)\n",
        "        PRS = BIS * PTR * np.log1p(ENM)\n",
        "        EQB = np.sqrt(APF + BIS) * SUE\n",
        "\n",
        "        Y_raw = np.hstack([UCO, PRS, EQB]).astype(np.float64)\n",
        "        Y_raw += 0.03 * Y_raw.std(axis=0) * np.random.randn(*Y_raw.shape)\n",
        "\n",
        "        # Compute normalization stats\n",
        "        self.X_mean = X_raw.mean(axis=0)\n",
        "        self.X_std  = X_raw.std(axis=0) + eps\n",
        "        self.Y_mean = Y_raw.mean(axis=0)\n",
        "        self.Y_std  = Y_raw.std(axis=0) + eps\n",
        "\n",
        "        # Normalize to float32\n",
        "        self.X = ((X_raw - self.X_mean) / self.X_std).astype(np.float32)\n",
        "        self.Y = ((Y_raw - self.Y_mean) / self.Y_std).astype(np.float32)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return torch.from_numpy(self.X[idx]), torch.from_numpy(self.Y[idx])\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 2. BeyondAI Model with MC-Dropout\n",
        "# ------------------------------------------------------------------------------\n",
        "class BeyondAI(nn.Module):\n",
        "    def __init__(self, input_dim=6, hidden_dim=32, output_dim=3, dropout_p=0.1):\n",
        "        super().__init__()\n",
        "        self.fc1     = nn.Linear(input_dim, hidden_dim)\n",
        "        self.relu    = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(dropout_p)\n",
        "        self.fc2     = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 3. Physics-Informed Residual Loss\n",
        "# ------------------------------------------------------------------------------\n",
        "def physics_residual(pred, X, stats):\n",
        "    # Un-normalize inputs\n",
        "    X_den = X * stats['X_std'] + stats['X_mean']\n",
        "    APF, BIS, SUE, ENM, RES, PTR = X_den.t()\n",
        "    eps = 1e-8\n",
        "\n",
        "    # Recompute targets\n",
        "    UCO_t = APF * RES / (SUE + eps)\n",
        "    PRS_t = BIS * PTR * torch.log1p(ENM)\n",
        "    EQB_t = torch.sqrt(APF + BIS) * SUE\n",
        "\n",
        "    Yt      = torch.stack([UCO_t, PRS_t, EQB_t], dim=1)\n",
        "    Yt_norm = (Yt - stats['Y_mean']) / stats['Y_std']\n",
        "    return nn.MSELoss()(pred, Yt_norm)\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 4. Combined Loss Function\n",
        "# ------------------------------------------------------------------------------\n",
        "def total_loss(pred, true, X, stats, lambda_phys=1.0):\n",
        "    mse_loss  = nn.MSELoss()(pred, true)\n",
        "    phys_loss = physics_residual(pred, X, stats)\n",
        "    return mse_loss + lambda_phys * phys_loss, mse_loss, phys_loss\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 5. MC-Dropout Uncertainty Estimation\n",
        "# ------------------------------------------------------------------------------\n",
        "def mc_dropout_predict(model, X, T=50):\n",
        "    model.train()  # keep dropout active\n",
        "    preds = []\n",
        "    with torch.no_grad():\n",
        "        for _ in range(T):\n",
        "            preds.append(model(X))\n",
        "    stacked = torch.stack(preds, dim=0)\n",
        "    return stacked.mean(dim=0), stacked.std(dim=0)\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 6. Training Loop & Checkpointing\n",
        "# ------------------------------------------------------------------------------\n",
        "def train(model, train_loader, val_loader, stats, device,\n",
        "          lr=1e-4, weight_decay=1e-5, lambda_phys=1.0,\n",
        "          epochs=100, patience=10):\n",
        "    model.to(device)\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
        "        optimizer, mode='min', factor=0.5, patience=5\n",
        "    )\n",
        "\n",
        "    best_val, wait = float('inf'), 0\n",
        "    history = {'train': [], 'val': []}\n",
        "\n",
        "    for epoch in range(1, epochs+1):\n",
        "        # Training phase\n",
        "        model.train()\n",
        "        total_tr = 0.0\n",
        "        for Xb, Yb in train_loader:\n",
        "            Xb, Yb = Xb.to(device), Yb.to(device)\n",
        "            pred = model(Xb)\n",
        "            loss, _, _ = total_loss(pred, Yb, Xb, stats, lambda_phys)\n",
        "            if torch.isnan(loss):\n",
        "                raise RuntimeError(f'NaN detected at epoch {epoch}')\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            optimizer.step()\n",
        "            total_tr += loss.item() * Xb.size(0)\n",
        "        train_loss = total_tr / len(train_loader.dataset)\n",
        "\n",
        "        # Validation phase\n",
        "        model.eval()\n",
        "        total_val = 0.0\n",
        "        with torch.no_grad():\n",
        "            for Xb, Yb in val_loader:\n",
        "                Xb, Yb = Xb.to(device), Yb.to(device)\n",
        "                pred = model(Xb)\n",
        "                loss, _, _ = total_loss(pred, Yb, Xb, stats, lambda_phys)\n",
        "                total_val += loss.item() * Xb.size(0)\n",
        "        val_loss = total_val / len(val_loader.dataset)\n",
        "\n",
        "        scheduler.step(val_loss)\n",
        "        history['train'].append(train_loss)\n",
        "        history['val'].append(val_loss)\n",
        "        print(f\"Epoch {epoch:03d} | Train {train_loss:.4e} | Val {val_loss:.4e}\")\n",
        "\n",
        "        # Checkpoint & early stopping\n",
        "        if val_loss < best_val - 1e-6:\n",
        "            best_val, wait = val_loss, 0\n",
        "            torch.save(model.state_dict(), 'best_beyond_ai.pth')\n",
        "        else:\n",
        "            wait += 1\n",
        "            if wait >= patience:\n",
        "                print(\"Early stopping triggered.\")\n",
        "                break\n",
        "\n",
        "    # Load best checkpoint\n",
        "    model.load_state_dict(torch.load('best_beyond_ai.pth', map_location=device))\n",
        "    return history\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 7. Visualizations\n",
        "# ------------------------------------------------------------------------------\n",
        "def plot_history(history):\n",
        "    plt.figure()\n",
        "    plt.plot(history['train'], label='Train Loss')\n",
        "    plt.plot(history['val'],   label='Val Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "def plot_scatter(y_true, y_pred, title):\n",
        "    plt.figure()\n",
        "    plt.scatter(y_true, y_pred, s=6, alpha=0.5)\n",
        "    mn, mx = y_true.min(), y_true.max()\n",
        "    plt.plot([mn, mx], [mn, mx], 'r--')\n",
        "    plt.title(title)\n",
        "    plt.show()\n",
        "\n",
        "def plot_uncertainty(model, stats, device):\n",
        "    G = 100\n",
        "    # Vary APF and RES, fix others at their mean\n",
        "    APF = np.linspace(0.0, 1.0, G, dtype=np.float32)\n",
        "    RES = np.linspace(0.1, 3.0, G, dtype=np.float32)\n",
        "    G1, G2 = np.meshgrid(APF, RES)\n",
        "    pts = G * G\n",
        "\n",
        "    Xg = torch.zeros((pts,6), device=device)\n",
        "    for i in [1,2,3,5]:\n",
        "        Xg[:, i] = stats['X_mean'][i]\n",
        "    Xg[:,0] = torch.from_numpy(G1.ravel()).to(device)\n",
        "    Xg[:,4] = torch.from_numpy(G2.ravel()).to(device)\n",
        "\n",
        "    Xn = (Xg - stats['X_mean']) / stats['X_std']\n",
        "    _, std = mc_dropout_predict(model, Xn, T=40)\n",
        "    U = std[:,0].cpu().reshape(G1.shape)\n",
        "\n",
        "    plt.figure(figsize=(5,4))\n",
        "    plt.pcolormesh(APF, RES, U, shading='auto', cmap='magma')\n",
        "    plt.colorbar(label='Std(undefined coherence)')\n",
        "    plt.xlabel('Absolute Presence Factor (APF)')\n",
        "    plt.ylabel('Resonance Scale (RES)')\n",
        "    plt.title('Uncertainty Heatmap')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 8. Main Execution\n",
        "# ------------------------------------------------------------------------------\n",
        "if __name__ == '__main__':\n",
        "    # Select device\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    # Prepare dataset & stats\n",
        "    dataset = BeyondPresenceDataset(n_samples=7000, seed=2025)\n",
        "    stats = {\n",
        "        'X_mean': torch.tensor(dataset.X_mean, dtype=torch.float32, device=device),\n",
        "        'X_std' : torch.tensor(dataset.X_std,  dtype=torch.float32, device=device),\n",
        "        'Y_mean': torch.tensor(dataset.Y_mean, dtype=torch.float32, device=device),\n",
        "        'Y_std' : torch.tensor(dataset.Y_std,  dtype=torch.float32, device=device),\n",
        "    }\n",
        "\n",
        "    # Split train/validation\n",
        "    n_val = int(0.2 * len(dataset))\n",
        "    train_ds, val_ds = random_split(dataset, [len(dataset)-n_val, n_val])\n",
        "    train_loader = DataLoader(train_ds, batch_size=128, shuffle=True)\n",
        "    val_loader   = DataLoader(val_ds,   batch_size=256, shuffle=False)\n",
        "\n",
        "    # Instantiate model\n",
        "    model = BeyondAI(input_dim=6, hidden_dim=32, output_dim=3, dropout_p=0.1).to(device)\n",
        "\n",
        "    # Train\n",
        "    history = train(\n",
        "        model, train_loader, val_loader,\n",
        "        stats, device,\n",
        "        lr=1e-4, weight_decay=1e-5,\n",
        "        lambda_phys=1.0, epochs=80, patience=8\n",
        "    )\n",
        "\n",
        "    # Plot training history\n",
        "    plot_history(history)\n",
        "\n",
        "    # Generate scatter plots for each target\n",
        "    X_all = torch.from_numpy(dataset.X).to(device)\n",
        "    with torch.no_grad():\n",
        "        Y_pred_n = model(X_all).cpu().numpy()\n",
        "    Y_true = dataset.Y * dataset.Y_std + dataset.Y_mean\n",
        "    Y_pred = Y_pred_n * dataset.Y_std + dataset.Y_mean\n",
        "\n",
        "    names = ['Undefined Coherence', 'Presence Resonance', 'Absolute Equilibrium']\n",
        "    for i, nm in enumerate(names):\n",
        "        plot_scatter(Y_true[:,i], Y_pred[:,i], nm)\n",
        "\n",
        "    # Display uncertainty heatmap\n",
        "    plot_uncertainty(model, stats, device)"
      ]
    }
  ]
}