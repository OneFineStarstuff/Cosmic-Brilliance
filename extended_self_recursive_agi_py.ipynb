{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyNmMgrmIkgi6cVwezN70SMR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OneFineStarstuff/Cosmic-Brilliance/blob/main/extended_self_recursive_agi_py.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eu6lDnAi6y3A"
      },
      "outputs": [],
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "extended_self_recursive_agi.py\n",
        "\n",
        "- Synthetic train + val data\n",
        "- Feed-forward net with inner/outer loops + Dropout for MC-Dropout\n",
        "- Training loop + checkpointing\n",
        "- Evaluation on validation set\n",
        "- Visualization: scatter, residuals, reliability diagram\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.calibration import calibration_curve\n",
        "\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# 1. Synthetic Regression Dataset (Train + Val)\n",
        "# -----------------------------------------------------------------------------\n",
        "class SyntheticRegressionDataset(Dataset):\n",
        "    def __init__(self, num_samples=2500, input_size=10, val_split=500):\n",
        "        super().__init__()\n",
        "        X = torch.randn(num_samples, input_size)\n",
        "        true_w = torch.randn(input_size, 1)\n",
        "        y = X @ true_w + 0.1 * torch.randn(num_samples, 1)\n",
        "        # split into train / val\n",
        "        self.train_set, self.val_set = random_split(\n",
        "            list(zip(X, y)),\n",
        "            [num_samples - val_split, val_split]\n",
        "        )\n",
        "\n",
        "    def get_loaders(self, batch_size=32):\n",
        "        train_loader = DataLoader(self.train_set, batch_size=batch_size,\n",
        "                                  shuffle=True)\n",
        "        val_loader   = DataLoader(self.val_set,   batch_size=batch_size,\n",
        "                                  shuffle=False)\n",
        "        return train_loader, val_loader\n",
        "\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# 2. Model with Dropout & MC-Dropout Inference\n",
        "# -----------------------------------------------------------------------------\n",
        "class SelfRecursiveAGI(nn.Module):\n",
        "    def __init__(self,\n",
        "                 input_size: int,\n",
        "                 hidden_size: int,\n",
        "                 output_size: int,\n",
        "                 lr_main: float = 1e-3,\n",
        "                 lr_self: float = 1e-3,\n",
        "                 dropout_p: float = 0.1):\n",
        "        super().__init__()\n",
        "        self.hidden = nn.Sequential(\n",
        "            nn.Linear(input_size, hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout_p)\n",
        "        )\n",
        "        self.output = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "        # Outer and inner optimizers\n",
        "        self.main_optimizer = optim.Adam(self.parameters(), lr=lr_main)\n",
        "        self.self_optimizer = optim.Adam(self.parameters(), lr=lr_self)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        h = self.hidden(x)\n",
        "        return self.output(h)\n",
        "\n",
        "    def self_improve(self,\n",
        "                     loss_fn,\n",
        "                     x: torch.Tensor,\n",
        "                     y: torch.Tensor,\n",
        "                     steps: int = 5,\n",
        "                     clip_grad_norm: float = 1.0) -> float:\n",
        "        last_loss = 0.0\n",
        "        for _ in range(steps):\n",
        "            self.self_optimizer.zero_grad()\n",
        "            preds = self.forward(x)\n",
        "            loss = loss_fn(preds, y)\n",
        "            loss.backward()\n",
        "            nn.utils.clip_grad_norm_(self.parameters(), clip_grad_norm)\n",
        "            self.self_optimizer.step()\n",
        "            last_loss = loss.item()\n",
        "        return last_loss\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def mc_dropout_predict(self,\n",
        "                           x: torch.Tensor,\n",
        "                           mc_samples: int = 50) -> (torch.Tensor, torch.Tensor):\n",
        "        \"\"\"\n",
        "        Run T stochastic forward passes under Dropout,\n",
        "        returning predictive mean and std for each input.\n",
        "        \"\"\"\n",
        "        self.train()  # keep dropout on\n",
        "        preds = []\n",
        "        for _ in range(mc_samples):\n",
        "            preds.append(self.forward(x).cpu().numpy())\n",
        "        preds = np.stack(preds, axis=0)  # shape: [T, batch, 1]\n",
        "        mean = preds.mean(axis=0).squeeze()\n",
        "        std  = preds.std(axis=0).squeeze()\n",
        "        return mean, std\n",
        "\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# 3. Training with Self-Improvement & Checkpoints\n",
        "# -----------------------------------------------------------------------------\n",
        "def train_self_recursive(\n",
        "    model: SelfRecursiveAGI,\n",
        "    train_loader: DataLoader,\n",
        "    loss_fn,\n",
        "    device: torch.device,\n",
        "    epochs: int = 20,\n",
        "    self_steps: int = 3,\n",
        "    clip_grad_norm: float = 1.0,\n",
        "    scheduler_step: int = 5,\n",
        "    scheduler_gamma: float = 0.5,\n",
        "    checkpoint_dir: str = 'checkpoints'):\n",
        "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "    model.to(device)\n",
        "    scheduler = StepLR(model.main_optimizer,\n",
        "                       step_size=scheduler_step,\n",
        "                       gamma=scheduler_gamma)\n",
        "\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        pbar = tqdm(train_loader, desc=f\"Epoch {epoch}/{epochs}\")\n",
        "        for Xb, yb in pbar:\n",
        "            Xb, yb = Xb.to(device), yb.to(device)\n",
        "\n",
        "            # Outer loop\n",
        "            model.main_optimizer.zero_grad()\n",
        "            out = model(Xb)\n",
        "            loss_main = loss_fn(out, yb)\n",
        "            loss_main.backward()\n",
        "            nn.utils.clip_grad_norm_(model.parameters(), clip_grad_norm)\n",
        "            model.main_optimizer.step()\n",
        "\n",
        "            # Inner loop self-improve\n",
        "            loss_self = model.self_improve(loss_fn, Xb, yb,\n",
        "                                           steps=self_steps,\n",
        "                                           clip_grad_norm=clip_grad_norm)\n",
        "\n",
        "            running_loss += loss_main.item()\n",
        "            pbar.set_postfix({\n",
        "                'main': f\"{loss_main.item():.4f}\",\n",
        "                'self': f\"{loss_self:.4f}\"\n",
        "            })\n",
        "\n",
        "        scheduler.step()\n",
        "        avg = running_loss / len(train_loader)\n",
        "        print(f\"Epoch {epoch} done, Avg Loss: {avg:.6f}\")\n",
        "\n",
        "        if epoch % scheduler_step == 0:\n",
        "            path = os.path.join(checkpoint_dir, f\"agi_epoch{epoch}.pt\")\n",
        "            torch.save({\n",
        "                'epoch': epoch,\n",
        "                'model': model.state_dict(),\n",
        "                'optim': model.main_optimizer.state_dict(),\n",
        "                'sched': scheduler.state_dict()\n",
        "            }, path)\n",
        "            print(f\"Checkpoint â†’ {path}\")\n",
        "\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# 4. Evaluation & Plots\n",
        "# -----------------------------------------------------------------------------\n",
        "def evaluate_and_plot(model: SelfRecursiveAGI,\n",
        "                      val_loader: DataLoader,\n",
        "                      device: torch.device):\n",
        "    model.eval()\n",
        "    ys, preds_mean, preds_std = [], [], []\n",
        "\n",
        "    for Xb, yb in val_loader:\n",
        "        Xb = Xb.to(device)\n",
        "        mean, std = model.mc_dropout_predict(Xb, mc_samples=50)\n",
        "        preds_mean.append(mean)\n",
        "        preds_std.append(std)\n",
        "        ys.append(yb.numpy().squeeze())\n",
        "\n",
        "    y_true = np.concatenate(ys)\n",
        "    y_pred = np.concatenate(preds_mean)\n",
        "    y_err  = np.abs(y_pred - y_true)\n",
        "    uncert = np.concatenate(preds_std)\n",
        "\n",
        "    # 1) Scatter True vs Pred\n",
        "    plt.figure(figsize=(5,5))\n",
        "    plt.scatter(y_true, y_pred, alpha=0.6)\n",
        "    lims = [y_true.min(), y_true.max()]\n",
        "    plt.plot(lims, lims, '--', color='gray')\n",
        "    plt.xlabel(\"True y\")\n",
        "    plt.ylabel(\"Predicted y\")\n",
        "    plt.title(\"True vs. Predicted\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # 2) Residual Histogram\n",
        "    plt.figure()\n",
        "    plt.hist(y_err, bins=30, alpha=0.7)\n",
        "    plt.xlabel(\"Absolute Error\")\n",
        "    plt.title(\"Residuals\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # 3) Reliability Diagram\n",
        "    # Bin by predicted uncertainty, compare mean error vs uncertainty\n",
        "    bins = np.linspace(uncert.min(), uncert.max(), 10)\n",
        "    bin_ids = np.digitize(uncert, bins) - 1\n",
        "    mean_err = [y_err[bin_ids == i].mean() if np.any(bin_ids==i) else 0.0\n",
        "                for i in range(len(bins))]\n",
        "    bin_center = (bins[:-1] + bins[1:]) / 2\n",
        "\n",
        "    plt.figure()\n",
        "    plt.plot(bin_center, mean_err[:-1], '-o')\n",
        "    plt.plot([uncert.min(), uncert.max()],\n",
        "             [uncert.min(), uncert.max()],\n",
        "             '--', color='gray')\n",
        "    plt.xlabel(\"Predicted Ïƒ\")\n",
        "    plt.ylabel(\"Empirical |Error|\")\n",
        "    plt.title(\"Reliability Diagram\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# 5. Main Entry Point\n",
        "# -----------------------------------------------------------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    # Hyperparameters\n",
        "    BATCH_SIZE = 32\n",
        "    INPUT_SIZE = 10\n",
        "    HIDDEN_SIZE = 64\n",
        "    OUTPUT_SIZE = 1\n",
        "    LR_MAIN = 1e-3\n",
        "    LR_SELF = 1e-3\n",
        "    EPOCHS = 20\n",
        "    SELF_STEPS = 3\n",
        "    DROP_P = 0.1\n",
        "    CLIP_GRAD = 1.0\n",
        "    SCHED_STEP = 5\n",
        "    SCHED_GAMMA = 0.5\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # Data\n",
        "    ds = SyntheticRegressionDataset(\n",
        "        num_samples=2500,\n",
        "        input_size=INPUT_SIZE,\n",
        "        val_split=500\n",
        "    )\n",
        "    train_loader, val_loader = ds.get_loaders(batch_size=BATCH_SIZE)\n",
        "\n",
        "    # Model, loss\n",
        "    model = SelfRecursiveAGI(\n",
        "        input_size=INPUT_SIZE,\n",
        "        hidden_size=HIDDEN_SIZE,\n",
        "        output_size=OUTPUT_SIZE,\n",
        "        lr_main=LR_MAIN,\n",
        "        lr_self=LR_SELF,\n",
        "        dropout_p=DROP_P\n",
        "    )\n",
        "    criterion = nn.MSELoss()\n",
        "\n",
        "    # Train\n",
        "    train_self_recursive(\n",
        "        model, train_loader, criterion, device,\n",
        "        epochs=EPOCHS,\n",
        "        self_steps=SELF_STEPS,\n",
        "        clip_grad_norm=CLIP_GRAD,\n",
        "        scheduler_step=SCHED_STEP,\n",
        "        scheduler_gamma=SCHED_GAMMA,\n",
        "        checkpoint_dir='checkpoints'\n",
        "    )\n",
        "\n",
        "    # Evaluate + visualize\n",
        "    evaluate_and_plot(model, val_loader, device)"
      ]
    }
  ]
}