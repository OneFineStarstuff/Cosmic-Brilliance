{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyPx+ytBu/0yMNpZ5dkR/H6T",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OneFineStarstuff/Cosmic-Brilliance/blob/main/unified_ai_memory_eem_py.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a-s8AtHWZToC"
      },
      "outputs": [],
      "source": [
        "# unified_ai/memory/eem.py\n",
        "# Self-contained: complex episodic memory + reflection scaffold + smoke test.\n",
        "\n",
        "from __future__ import annotations\n",
        "from typing import Optional, Tuple, Callable, Dict, Any, List\n",
        "\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "def _as_complex(r: torch.Tensor, i: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
        "    if i is None:\n",
        "        i = torch.zeros_like(r)\n",
        "    return torch.complex(r, i)\n",
        "\n",
        "\n",
        "def _norm_complex(z: torch.Tensor, eps: float = 1e-6, dim: int = -1) -> torch.Tensor:\n",
        "    # Normalize complex vector along dim: z / ||z||\n",
        "    mag = torch.sqrt((z.real**2 + z.imag**2).sum(dim=dim, keepdim=True) + eps)\n",
        "    return z / mag\n",
        "\n",
        "\n",
        "def _inner_complex(a: torch.Tensor, b: torch.Tensor, dim: int = -1) -> torch.Tensor:\n",
        "    # ⟨a, b⟩ = sum(conj(a) * b) along dim\n",
        "    return torch.sum(torch.conj(a) * b, dim=dim)\n",
        "\n",
        "\n",
        "def _phase_unitary(z: torch.Tensor, theta: torch.Tensor) -> torch.Tensor:\n",
        "    # Element-wise complex phase: e^{iθ} ⊙ z\n",
        "    # z: (..., D) complex; theta: (D,) real\n",
        "    phase = torch.complex(torch.cos(theta), torch.sin(theta))  # (D,)\n",
        "    return z * phase\n",
        "\n",
        "\n",
        "def _stack_householder(U: torch.Tensor, v: torch.Tensor) -> torch.Tensor:\n",
        "    # Apply Householder reflection H(v) = I - 2 vv^T / (v^T v) on real matrix U\n",
        "    # v: (D,) real; U: (..., D) real; reflection along last dim.\n",
        "    v = v / (v.norm() + 1e-6)\n",
        "    # Compute U - 2*(U v) v^T\n",
        "    proj = torch.matmul(U, v)  # (..., )\n",
        "    return U - 2.0 * proj.unsqueeze(-1) * v\n",
        "\n",
        "\n",
        "class EntangledEpisodicMemory(nn.Module):\n",
        "    \"\"\"\n",
        "    Entangled episodic memory in a complex Hilbert space.\n",
        "\n",
        "    - Keys live in C^D (stored as real & imag).\n",
        "    - Retrieval probabilities are \"measurement\" over amplitudes: p_i ∝ |⟨q, k_i⟩|^2 / τ.\n",
        "    - A learnable unitary U(θ) modulates phase before similarity.\n",
        "    - Reads are differentiable; writes are no-grad EMA or ring buffer.\n",
        "\n",
        "    Shapes:\n",
        "      - keys: (S, D), values: (S, V)\n",
        "      - read:\n",
        "          q: (B, D) real or complex (real inputs are promoted to complex)\n",
        "          returns: (B, V)\n",
        "      - write:\n",
        "          k: (B, D) real or complex, v: (B, V)\n",
        "\n",
        "    Args:\n",
        "      slots: memory slots S\n",
        "      key_dim: complex key dimension D\n",
        "      value_dim: value dimension V (real)\n",
        "      temperature: softmax temperature τ\n",
        "      ema: EMA mixing for 'nearest' writes\n",
        "      trainable_memory: if True, keys/values are learnable parameters; else buffers (typical)\n",
        "      use_householder: if >0, apply that many real Householder reflections on [Re; Im] stack\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        slots: int = 4096,\n",
        "        key_dim: int = 512,\n",
        "        value_dim: int = 512,\n",
        "        temperature: float = 0.1,\n",
        "        ema: float = 0.1,\n",
        "        trainable_memory: bool = False,\n",
        "        householder_layers: int = 0,\n",
        "        dtype: torch.dtype = torch.float32,\n",
        "        device: Optional[torch.device] = None,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.slots = slots\n",
        "        self.key_dim = key_dim\n",
        "        self.value_dim = value_dim\n",
        "        self.temperature = temperature\n",
        "        self.ema = ema\n",
        "        self.householder_layers = householder_layers\n",
        "\n",
        "        # Memory storage\n",
        "        init_keys_r = F.normalize(torch.randn(slots, key_dim, dtype=dtype, device=device), dim=-1)\n",
        "        init_keys_i = F.normalize(torch.randn(slots, key_dim, dtype=dtype, device=device), dim=-1)\n",
        "        init_values = torch.zeros(slots, value_dim, dtype=dtype, device=device)\n",
        "\n",
        "        if trainable_memory:\n",
        "            self.keys_r = nn.Parameter(init_keys_r)\n",
        "            self.keys_i = nn.Parameter(init_keys_i)\n",
        "            self.values = nn.Parameter(init_values)\n",
        "        else:\n",
        "            self.register_buffer(\"keys_r\", init_keys_r)\n",
        "            self.register_buffer(\"keys_i\", init_keys_i)\n",
        "            self.register_buffer(\"values\", init_values)\n",
        "\n",
        "        # Learnable unitary: diagonal phase θ in R^D\n",
        "        self.theta = nn.Parameter(torch.zeros(key_dim, dtype=dtype, device=device))\n",
        "\n",
        "        # Optional Householder vectors (real reflections on concatenated [Re(z); Im(z)])\n",
        "        if self.householder_layers > 0:\n",
        "            self.house_v = nn.ParameterList(\n",
        "                [nn.Parameter(F.normalize(torch.randn(2 * key_dim, dtype=dtype, device=device), dim=0))\n",
        "                 for _ in range(self.householder_layers)]\n",
        "            )\n",
        "\n",
        "        # Ring buffer pointer + age tracker (buffers)\n",
        "        self.register_buffer(\"age\", torch.zeros(slots, dtype=torch.long, device=device))\n",
        "        self.register_buffer(\"ptr\", torch.zeros((), dtype=torch.long, device=device))\n",
        "\n",
        "    # ------------- Internals -------------\n",
        "    def _apply_unitary(self, z_c: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Apply U to complex vector(s): phase rotation + optional stacked Householder over [Re; Im].\n",
        "        z_c: (..., D) complex\n",
        "        \"\"\"\n",
        "        z_c = _phase_unitary(z_c, self.theta)  # element-wise unitary in C^D\n",
        "        if self.householder_layers > 0:\n",
        "            # Real-augment: concat [Re; Im] along feature dim\n",
        "            re, im = z_c.real, z_c.imag  # (..., D)\n",
        "            cat = torch.cat([re, im], dim=-1)  # (..., 2D)\n",
        "            for v in self.house_v:\n",
        "                cat = _stack_householder(cat, v)\n",
        "            # Split back to complex\n",
        "            D = z_c.size(-1)\n",
        "            re2, im2 = cat[..., :D], cat[..., D:]\n",
        "            z_c = torch.complex(re2, im2)\n",
        "        return z_c\n",
        "\n",
        "    def _similarity(self, q_c: torch.Tensor, k_c: torch.Tensor, mode: str = \"inner\") -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Compute similarity scores between a batch of queries and all keys.\n",
        "\n",
        "        q_c: (B, D) complex\n",
        "        k_c: (S, D) complex\n",
        "        returns: (B, S) real scores\n",
        "        \"\"\"\n",
        "        # Normalize to lie on complex unit sphere\n",
        "        qn = _norm_complex(q_c, dim=-1)\n",
        "        kn = _norm_complex(k_c, dim=-1)\n",
        "        if mode == \"inner\":\n",
        "            # Use squared magnitude of inner product as \"measurement\" amplitude\n",
        "            # scores[b, s] = |⟨q_b, k_s⟩|^2\n",
        "            scores = torch.abs(qn.unsqueeze(1) @ torch.conj(kn).unsqueeze(0).transpose(-1, -2)) ** 2\n",
        "            # qn.unsqueeze(1): (B,1,D), conj(kn).unsqueeze(0).T: (1,D,S) => (B,1,S) via matmul\n",
        "            scores = scores.squeeze(1)  # (B, S)\n",
        "        elif mode == \"cos\":\n",
        "            # Real cosine on concatenated [Re; Im]\n",
        "            qr = torch.cat([qn.real, qn.imag], dim=-1)  # (B, 2D)\n",
        "            kr = torch.cat([kn.real, kn.imag], dim=-1)  # (S, 2D)\n",
        "            scores = F.cosine_similarity(qr.unsqueeze(1), kr.unsqueeze(0), dim=-1)  # (B, S)\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown similarity mode: {mode}\")\n",
        "        return scores\n",
        "\n",
        "    # ------------- Public API -------------\n",
        "    @torch.no_grad()\n",
        "    def write(self, k: torch.Tensor, v: torch.Tensor, strategy: str = \"nearest\"):\n",
        "        \"\"\"\n",
        "        Write key-value pairs into memory.\n",
        "        k: (B, D) real or complex; v: (B, V) real\n",
        "        strategy: 'nearest' (EMA into nearest slot), or 'ring' (append)\n",
        "        \"\"\"\n",
        "        single = k.dim() == 1\n",
        "        if single:\n",
        "            k = k.unsqueeze(0)\n",
        "            v = v.unsqueeze(0)\n",
        "\n",
        "        # Promote to complex if needed, then apply unitary to canonicalize writer-side keys\n",
        "        if torch.is_complex(k):\n",
        "            k_c = k\n",
        "        else:\n",
        "            k_c = _as_complex(k)\n",
        "        k_c = self._apply_unitary(k_c)\n",
        "        k_c = _norm_complex(k_c)\n",
        "\n",
        "        # Memory views\n",
        "        mem_c = _as_complex(self.keys_r, self.keys_i)  # (S, D)\n",
        "\n",
        "        if strategy == \"ring\":\n",
        "            for i in range(k_c.size(0)):\n",
        "                idx = int(self.ptr.item() % self.slots)\n",
        "                self.keys_r[idx] = k_c[i].real\n",
        "                self.keys_i[idx] = k_c[i].imag\n",
        "                self.values[idx] = v[i]\n",
        "                self.age[idx] = 0\n",
        "                self.ptr += 1\n",
        "        elif strategy == \"nearest\":\n",
        "            # Find nearest slots by measurement similarity\n",
        "            sims = self._similarity(k_c, mem_c)  # (B, S)\n",
        "            idxs = sims.argmax(dim=-1)  # (B,)\n",
        "            for i, idx in enumerate(idxs.tolist()):\n",
        "                # EMA update\n",
        "                self.keys_r[idx] = F.normalize((1 - self.ema) * self.keys_r[idx] + self.ema * k_c[i].real, dim=-1)\n",
        "                self.keys_i[idx] = F.normalize((1 - self.ema) * self.keys_i[idx] + self.ema * k_c[i].imag, dim=-1)\n",
        "                self.values[idx] = (1 - self.ema) * self.values[idx] + self.ema * v[i]\n",
        "                self.age[idx] = 0\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown write strategy: {strategy}\")\n",
        "\n",
        "        self.age += 1\n",
        "\n",
        "    def read(\n",
        "        self,\n",
        "        q: torch.Tensor,\n",
        "        topk: int = 0,\n",
        "        similarity: str = \"inner\",\n",
        "        return_weights: bool = False,\n",
        "    ) -> Tuple[torch.Tensor, Optional[torch.Tensor]]:\n",
        "        \"\"\"\n",
        "        Read from memory with measurement probabilities.\n",
        "\n",
        "        q: (B, D) real or complex input query\n",
        "        topk: if >0, restrict softmax to top-k keys per query\n",
        "        similarity: 'inner' (|⟨q,k⟩|^2) or 'cos' on [Re;Im]\n",
        "        return_weights: also return attention weights (B, S or B, K)\n",
        "\n",
        "        Returns:\n",
        "          values: (B, V)\n",
        "          weights (optional): (B, S) or (B, K)\n",
        "        \"\"\"\n",
        "        single = q.dim() == 1\n",
        "        if single:\n",
        "            q = q.unsqueeze(0)\n",
        "\n",
        "        # Promote real to complex, apply unitary on the query side\n",
        "        q_c = q if torch.is_complex(q) else _as_complex(q)\n",
        "        q_c = self._apply_unitary(q_c)\n",
        "\n",
        "        # Current memory keys\n",
        "        k_c = _as_complex(self.keys_r, self.keys_i)\n",
        "\n",
        "        scores = self._similarity(q_c, k_c, mode=similarity)  # (B, S)\n",
        "        scores = scores / max(self.temperature, 1e-6)\n",
        "\n",
        "        if topk and topk < self.slots:\n",
        "            vals, idxs = scores.topk(topk, dim=-1)  # (B, K)\n",
        "            w = F.softmax(vals, dim=-1)            # (B, K)\n",
        "            gathered = self.values[idxs]           # (B, K, V)\n",
        "            out = (w.unsqueeze(-1) * gathered).sum(dim=1)  # (B, V)\n",
        "            weights = w\n",
        "        else:\n",
        "            w = F.softmax(scores, dim=-1)  # (B, S)\n",
        "            out = w @ self.values          # (B, V)\n",
        "            weights = w\n",
        "\n",
        "        if single:\n",
        "            out = out.squeeze(0)\n",
        "            if return_weights:\n",
        "                weights = weights.squeeze(0)\n",
        "        return (out, weights) if return_weights else (out, None)\n",
        "\n",
        "    # Convenience: functional multi-read with different topk/temperatures without rebuilding the module\n",
        "    def attend(self, q: torch.Tensor, *, topk: int = 0, temperature: Optional[float] = None) -> torch.Tensor:\n",
        "        old_tau = self.temperature\n",
        "        if temperature is not None:\n",
        "            self.temperature = temperature\n",
        "        v, _ = self.read(q, topk=topk)\n",
        "        if temperature is not None:\n",
        "            self.temperature = old_tau\n",
        "        return v\n",
        "\n",
        "\n",
        "# -------------------------\n",
        "# Reflection scaffolding\n",
        "# -------------------------\n",
        "\n",
        "def reflect_on_episode(\n",
        "    agent_log: str,\n",
        "    prompt_template: str,\n",
        "    llm_fn: Optional[Callable[[str], str]] = None,\n",
        "    parse_fn: Optional[Callable[[str], Dict[str, Any]]] = None,\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Uses an LLM (or a provided function) to generate introspective commentary.\n",
        "\n",
        "    Args:\n",
        "      agent_log: raw transcript text (actions, states, outcomes)\n",
        "      prompt_template: f-string-like template with '{log}' placeholder\n",
        "      llm_fn: function(prompt) -> str (defaults to a heuristic stub)\n",
        "      parse_fn: function(text) -> dict (defaults to a robust JSON-ish parser stub)\n",
        "\n",
        "    Returns:\n",
        "      dict with keys: failures (list), hypotheses (list), improvements (list), raw (str)\n",
        "    \"\"\"\n",
        "    if \"{log}\" not in prompt_template:\n",
        "        raise ValueError(\"prompt_template must contain '{log}' placeholder\")\n",
        "\n",
        "    prompt = prompt_template.format(log=agent_log)\n",
        "\n",
        "    # Default LLM: a simple heuristic splitter if none provided\n",
        "    def _default_llm(p: str) -> str:\n",
        "        lines = [ln.strip() for ln in p.splitlines() if ln.strip()]\n",
        "        # naive patterns\n",
        "        failures = [ln for ln in lines if \"fail\" in ln.lower() or \"error\" in ln.lower()]\n",
        "        hyp = [\n",
        "            \"Insufficient situational memory caused plan drift.\",\n",
        "            \"Overconfident prior led to under-exploration.\",\n",
        "        ]\n",
        "        imps = [\n",
        "            \"Increase retrieval top-k and use recency-weighted writes.\",\n",
        "            \"Inject uncertainty-aware exploration and replan when surprise spikes.\",\n",
        "        ]\n",
        "        return (\n",
        "            \"Failures:\\n- \" + \"\\n- \".join(failures[:3] or [\"No explicit failures found; inspect reward shaping.\"]) + \"\\n\\n\"\n",
        "            \"Hypotheses:\\n- \" + \"\\n- \".join(hyp) + \"\\n\\n\"\n",
        "            \"Improvements:\\n- \" + \"\\n- \".join(imps)\n",
        "        )\n",
        "\n",
        "    # Default parser: extract sections\n",
        "    def _default_parse(s: str) -> Dict[str, Any]:\n",
        "        out = {\"failures\": [], \"hypotheses\": [], \"improvements\": [], \"raw\": s}\n",
        "        sec, buf = None, []\n",
        "        def _flush():\n",
        "            if sec and buf:\n",
        "                items = [b.lstrip(\"- \").strip() for b in buf if b]\n",
        "                out[sec] = items\n",
        "        for ln in s.splitlines():\n",
        "            l = ln.strip()\n",
        "            if not l: continue\n",
        "            lower = l.lower()\n",
        "            if lower.startswith(\"failures:\"):\n",
        "                _flush(); sec, buf = \"failures\", []\n",
        "            elif lower.startswith(\"hypotheses:\"):\n",
        "                _flush(); sec, buf = \"hypotheses\", []\n",
        "            elif lower.startswith(\"improvements:\"):\n",
        "                _flush(); sec, buf = \"improvements\", []\n",
        "            else:\n",
        "                buf.append(l)\n",
        "        _flush()\n",
        "        return out\n",
        "\n",
        "    llm = llm_fn or _default_llm\n",
        "    parser = parse_fn or _default_parse\n",
        "    raw = llm(prompt)\n",
        "    return parser(raw)\n",
        "\n",
        "\n",
        "# -------------------------\n",
        "# Smoke test\n",
        "# -------------------------\n",
        "\n",
        "def _smoke():\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    torch.manual_seed(7)\n",
        "\n",
        "    # Memory\n",
        "    mem = EntangledEpisodicMemory(\n",
        "        slots=128, key_dim=32, value_dim=16, temperature=0.2, ema=0.2,\n",
        "        trainable_memory=False, householder_layers=1, device=device\n",
        "    )\n",
        "\n",
        "    # Create two clusters of keys/values and write them\n",
        "    B = 16\n",
        "    k1 = F.normalize(torch.randn(B, 32, device=device) + 2.0, dim=-1)\n",
        "    k2 = F.normalize(torch.randn(B, 32, device=device) - 2.0, dim=-1)\n",
        "    v1 = torch.ones(B, 16, device=device)        # value signature for cluster 1\n",
        "    v2 = torch.zeros(B, 16, device=device) + 3.0 # value signature for cluster 2\n",
        "\n",
        "    mem.write(k1, v1, strategy=\"ring\")\n",
        "    mem.write(k2, v2, strategy=\"ring\")\n",
        "\n",
        "    # Queries near cluster 1\n",
        "    q = k1[:4] + 0.05 * torch.randn(4, 32, device=device)\n",
        "    out, w = mem.read(q, topk=8, return_weights=True)\n",
        "    assert out.shape == (4, 16)\n",
        "    print(\"[PASS] Read shape:\", out.shape, \"| weights:\", None if w is None else w.shape)\n",
        "\n",
        "    # Check that outputs lean towards v1 (~1's) rather than v2 (~3's)\n",
        "    m = out.mean().item()\n",
        "    print(f\"[INFO] Output mean ~ {m:.3f} (closer to 1.0 => matched cluster 1)\")\n",
        "\n",
        "    # Reflection test\n",
        "    tmpl = \"\"\"You are a meta-cognitive agent.\n",
        "\n",
        "Below is a transcript of your actions, states, and outcomes during Task #274.\n",
        "\n",
        "Please:\n",
        "1. Identify at least 2 failure points or suboptimal decisions.\n",
        "2. Hypothesize why these failures occurred.\n",
        "3. Propose improvements to policy, memory use, or planning strategy.\n",
        "\n",
        "Transcript:\n",
        "{log}\"\"\"\n",
        "    log = \"\"\"\n",
        "Step 5: Missed object due to stale plan. Repeated scan without updating map.\n",
        "Step 12: Reached dead-end; planner failed to backtrack.\n",
        "Outcome: Goal not achieved; time limit exceeded.\n",
        "\"\"\"\n",
        "    reflection = reflect_on_episode(log, tmpl)\n",
        "    print(\"[REFLECTION] Failures:\", reflection[\"failures\"][:2])\n",
        "    print(\"[REFLECTION] Improvements (sample):\", reflection[\"improvements\"][:2])\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    _smoke()"
      ]
    }
  ]
}