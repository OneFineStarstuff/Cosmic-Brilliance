{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyNcPIwNI2vE/o1LjN0FRJ5Y",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OneFineStarstuff/Cosmic-Brilliance/blob/main/train_hyperdimensional_ai_py.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_gnSXfJqEq8t"
      },
      "outputs": [],
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "train_hyperdimensional_ai.py\n",
        "\n",
        "Physics‐informed AI pipeline for HyperdimensionalAI with NaN safety and uncertainty.\n",
        "1. Synthetic dataset: quantum energy fluctuations, dimensional stability, entanglement, multiverse interference, hyperflux density\n",
        "2. Float32 normalization and dtype consistency\n",
        "3. MLP with LayerNorm, Dropout, ReLU (accepts int hidden_dims)\n",
        "4. Physics‐informed residual with denominator clamps\n",
        "5. MC‐Dropout for uncertainty quantification\n",
        "6. Training loop: AdamW, ReduceLROnPlateau, gradient clipping, NaN checks, early stopping\n",
        "7. Safe checkpoint load\n",
        "8. Visualizations: loss history, scatter, uncertainty heatmap\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 1. Synthetic Hyperdimensional Energy Dataset\n",
        "# ------------------------------------------------------------------------------\n",
        "class HyperdimensionalDataset(Dataset):\n",
        "    def __init__(self, n_samples=5000, seed=42):\n",
        "        np.random.seed(seed)\n",
        "        # Features:\n",
        "        # QEF ∈ [0.1, 1.0], DS ∈ [0.1, 10.0], EF ∈ [0.0, 1.0],\n",
        "        # MI ∈ [0.0, 5.0], HD ∈ [1e2, 1e4]\n",
        "        QEF = np.random.uniform(0.1,   1.0,    (n_samples,1))\n",
        "        DS  = np.random.uniform(0.1,  10.0,    (n_samples,1))\n",
        "        EF  = np.random.uniform(0.0,   1.0,    (n_samples,1))\n",
        "        MI  = np.random.uniform(0.0,   5.0,    (n_samples,1))\n",
        "        HD  = np.random.uniform(1e2, 1e4,      (n_samples,1))\n",
        "\n",
        "        X_raw = np.hstack([QEF, DS, EF, MI, HD]).astype(np.float64)\n",
        "\n",
        "        # Physics targets:\n",
        "        # EFC (energy field coherence) = QEF * EF / (DS + eps)\n",
        "        # SF  (stability factor)        = DS * MI / (EF + eps)\n",
        "        # UEO (usable energy output)    = QEF * EF * HD\n",
        "        eps = 1e-6\n",
        "        EFC = QEF * EF / (DS + eps)\n",
        "        SF  = DS  * MI / (EF + eps)\n",
        "        UEO = QEF * EF * HD\n",
        "\n",
        "        Y_raw = np.hstack([EFC, SF, UEO]).astype(np.float64)\n",
        "        # add relative noise\n",
        "        Y_raw += 0.02 * Y_raw.std(axis=0) * np.random.randn(*Y_raw.shape)\n",
        "\n",
        "        # compute stats\n",
        "        self.X_mean = X_raw.mean(axis=0)\n",
        "        self.X_std  = X_raw.std(axis=0) + 1e-8\n",
        "        self.Y_mean = Y_raw.mean(axis=0)\n",
        "        self.Y_std  = Y_raw.std(axis=0) + 1e-8\n",
        "\n",
        "        # normalize\n",
        "        self.X = ((X_raw - self.X_mean) / self.X_std).astype(np.float32)\n",
        "        self.Y = ((Y_raw - self.Y_mean) / self.Y_std).astype(np.float32)\n",
        "\n",
        "        print(f\"Dataset X range {self.X.min():.3e}–{self.X.max():.3e}\")\n",
        "        print(f\"Dataset Y range {self.Y.min():.3e}–{self.Y.max():.3e}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        return torch.from_numpy(self.X[i]), torch.from_numpy(self.Y[i])\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 2. Model Definition (accepts int hidden_dims)\n",
        "# ------------------------------------------------------------------------------\n",
        "class HyperdimensionalAI(nn.Module):\n",
        "    def __init__(self, input_dim=5, hidden_dims=(64,64), output_dim=3, p_drop=0.1):\n",
        "        super().__init__()\n",
        "        # allow integer for hidden_dims\n",
        "        if isinstance(hidden_dims, int):\n",
        "            hidden_dims = (hidden_dims,)\n",
        "\n",
        "        layers, dim = [], input_dim\n",
        "        for h in hidden_dims:\n",
        "            layers += [\n",
        "                nn.Linear(dim, h),\n",
        "                nn.LayerNorm(h),\n",
        "                nn.ReLU(),\n",
        "                nn.Dropout(p_drop)\n",
        "            ]\n",
        "            dim = h\n",
        "        layers.append(nn.Linear(dim, output_dim))\n",
        "        self.net = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 3. Physics‐informed Loss with Denominator Clamps\n",
        "# ------------------------------------------------------------------------------\n",
        "def physics_residual(pred, X, stats):\n",
        "    X_den = X * stats['X_std'] + stats['X_mean']\n",
        "    QEF, DS, EF, MI, HD = X_den.t()\n",
        "    eps = 1e-4\n",
        "\n",
        "    EFC_t = QEF * EF / torch.clamp(DS + eps, min=eps)\n",
        "    SF_t  = DS  * MI / torch.clamp(EF + eps, min=eps)\n",
        "    UEO_t = QEF * EF * HD\n",
        "\n",
        "    Yt = torch.stack([EFC_t, SF_t, UEO_t], dim=1)\n",
        "    Yt_norm = (Yt - stats['Y_mean']) / stats['Y_std']\n",
        "    return nn.MSELoss()(pred, Yt_norm)\n",
        "\n",
        "def total_loss(pred, true, X, stats, lam=1.0):\n",
        "    mse  = nn.MSELoss()(pred, true)\n",
        "    phys = physics_residual(pred, X, stats)\n",
        "    return mse + lam * phys, mse, phys\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 4. MC‐Dropout Uncertainty Estimation\n",
        "# ------------------------------------------------------------------------------\n",
        "def mc_dropout_predict(model, X, T=50):\n",
        "    model.train()\n",
        "    preds = []\n",
        "    with torch.no_grad():\n",
        "        for _ in range(T):\n",
        "            preds.append(model(X))\n",
        "    arr = torch.stack(preds, 0)\n",
        "    return arr.mean(0), arr.std(0)\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 5. Training Loop (NaN safety, checkpoints, early stopping)\n",
        "# ------------------------------------------------------------------------------\n",
        "def train(model, train_loader, val_loader, stats, device,\n",
        "          lr=1e-4, wd=1e-5, lam=1.0, epochs=100, patience=10):\n",
        "    model.to(device)\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=wd)\n",
        "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.5, patience=5)\n",
        "\n",
        "    best_val, wait = float('inf'), 0\n",
        "    history = {'train': [], 'val': []}\n",
        "\n",
        "    for epoch in range(1, epochs+1):\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "        for xb, yb in train_loader:\n",
        "            xb, yb = xb.to(device), yb.to(device)\n",
        "            pred = model(xb)\n",
        "            loss, _, _ = total_loss(pred, yb, xb, stats, lam)\n",
        "            if torch.isnan(loss):\n",
        "                print(\"NaN loss detected. Stopping.\")\n",
        "                return history\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            optimizer.step()\n",
        "            train_loss += loss.item() * xb.size(0)\n",
        "        train_loss /= len(train_loader.dataset)\n",
        "\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        with torch.no_grad():\n",
        "            for xb, yb in val_loader:\n",
        "                xb, yb = xb.to(device), yb.to(device)\n",
        "                pred = model(xb)\n",
        "                loss, _, _ = total_loss(pred, yb, xb, stats, lam)\n",
        "                val_loss += loss.item() * xb.size(0)\n",
        "        val_loss /= len(val_loader.dataset)\n",
        "\n",
        "        scheduler.step(val_loss)\n",
        "        history['train'].append(train_loss)\n",
        "        history['val'].append(val_loss)\n",
        "        print(f\"Epoch {epoch:03d} ┃ Train {train_loss:.4e} ┃ Val {val_loss:.4e}\")\n",
        "\n",
        "        # checkpoint & early stop\n",
        "        if val_loss < best_val - 1e-8:\n",
        "            best_val, wait = val_loss, 0\n",
        "            torch.save(model.state_dict(), \"best_hyperai.pth\")\n",
        "        else:\n",
        "            wait += 1\n",
        "            if wait >= patience:\n",
        "                print(\"Early stopping.\")\n",
        "                break\n",
        "\n",
        "    # load best\n",
        "    if os.path.exists(\"best_hyperai.pth\"):\n",
        "        model.load_state_dict(torch.load(\"best_hyperai.pth\", map_location=device))\n",
        "    else:\n",
        "        print(\"No checkpoint found; using last model.\")\n",
        "\n",
        "    return history\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 6. Visualization Helpers\n",
        "# ------------------------------------------------------------------------------\n",
        "def plot_history(hist):\n",
        "    plt.plot(hist['train'], label='train')\n",
        "    plt.plot(hist['val'],   label='val')\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "def plot_scatter(y_true, y_pred):\n",
        "    plt.scatter(y_true, y_pred, s=5)\n",
        "    m, M = y_true.min(), y_true.max()\n",
        "    plt.plot([m, M], [m, M], 'r--')\n",
        "    plt.xlabel(\"True\")\n",
        "    plt.ylabel(\"Pred\")\n",
        "    plt.show()\n",
        "\n",
        "def plot_uncertainty(model, stats, device):\n",
        "    # vary QEF (idx=0) and EF (idx=2)\n",
        "    Q = np.linspace(0.1, 1.0, 80)\n",
        "    E = np.linspace(0.0, 1.0, 80)\n",
        "    Qm, Em = np.meshgrid(Q, E)\n",
        "    pts = Qm.size\n",
        "\n",
        "    grid = torch.zeros((pts, 5), device=device)\n",
        "    grid[:, 0] = torch.from_numpy(Qm.ravel()).to(device)\n",
        "    grid[:, 2] = torch.from_numpy(Em.ravel()).to(device)\n",
        "    # fix DS, MI, HD at mean\n",
        "    for i in (1, 3, 4):\n",
        "        grid[:, i] = stats['X_mean'][i]\n",
        "\n",
        "    Xn, _ = (grid - stats['X_mean']) / stats['X_std'], None\n",
        "    _, std = mc_dropout_predict(model, Xn, T=100)\n",
        "    U = std[:, 0].cpu().numpy().reshape(Qm.shape)\n",
        "\n",
        "    plt.pcolormesh(Qm, Em, U, cmap='magma')\n",
        "    plt.colorbar(label=\"Uncertainty σ\")\n",
        "    plt.xlabel(\"Quantum Energy Fluctuation\")\n",
        "    plt.ylabel(\"Entanglement Factor\")\n",
        "    plt.show()\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 7. Main Execution\n",
        "# ------------------------------------------------------------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # load dataset\n",
        "    dataset = HyperdimensionalDataset(n_samples=5000)\n",
        "    stats = {\n",
        "        'X_mean': torch.tensor(dataset.X_mean, dtype=torch.float32, device=device),\n",
        "        'X_std' : torch.tensor(dataset.X_std,  dtype=torch.float32, device=device),\n",
        "        'Y_mean': torch.tensor(dataset.Y_mean, dtype=torch.float32, device=device),\n",
        "        'Y_std' : torch.tensor(dataset.Y_std,  dtype=torch.float32, device=device)\n",
        "    }\n",
        "\n",
        "    train_ds, val_ds = random_split(dataset, [len(dataset)-1000, 1000])\n",
        "    train_loader = DataLoader(train_ds, batch_size=128, shuffle=True)\n",
        "    val_loader   = DataLoader(val_ds,   batch_size=256, shuffle=False)\n",
        "\n",
        "    # build & train model\n",
        "    model = HyperdimensionalAI(input_dim=5, hidden_dims=32, output_dim=3).to(device)\n",
        "    history = train(model, train_loader, val_loader, stats, device)\n",
        "\n",
        "    # plots\n",
        "    plot_history(history)\n",
        "    X_all = torch.from_numpy(dataset.X).to(device)\n",
        "    with torch.no_grad():\n",
        "        Y_pred = model(X_all).cpu().numpy()\n",
        "\n",
        "    for i in range(3):\n",
        "        plot_scatter(dataset.Y[:, i], Y_pred[:, i])\n",
        "\n",
        "    plot_uncertainty(model, stats, device)"
      ]
    }
  ]
}