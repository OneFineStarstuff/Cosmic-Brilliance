{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyMc4DsqMj5dKtrvrtRYlw7N",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OneFineStarstuff/Cosmic-Brilliance/blob/main/multiverse_simulation_py.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6AJIUrtwkBxN"
      },
      "outputs": [],
      "source": [
        "#!/usr/bin/env python3\n",
        "# filename: multiverse_simulation.py\n",
        "\n",
        "import os\n",
        "import argparse\n",
        "import json\n",
        "import random\n",
        "from typing import Tuple, Optional\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# Keep TF quieter and nudge determinism before importing tensorflow\n",
        "os.environ.setdefault(\"TF_CPP_MIN_LOG_LEVEL\", \"2\")\n",
        "os.environ.setdefault(\"TF_DETERMINISTIC_OPS\", \"1\")\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
        "\n",
        "\n",
        "# -----------------------\n",
        "# Utilities\n",
        "# -----------------------\n",
        "\n",
        "def set_seed(seed: int = 42) -> None:\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    tf.random.set_seed(seed)\n",
        "\n",
        "\n",
        "def str2bool(v: str) -> bool:\n",
        "    if isinstance(v, bool):\n",
        "        return v\n",
        "    v = v.strip().lower()\n",
        "    if v in (\"yes\", \"true\", \"t\", \"1\", \"y\"):\n",
        "        return True\n",
        "    if v in (\"no\", \"false\", \"f\", \"0\", \"n\"):\n",
        "        return False\n",
        "    raise argparse.ArgumentTypeError(f\"Boolean value expected, got: {v}\")\n",
        "\n",
        "\n",
        "def ensure_outdir(path: str) -> None:\n",
        "    os.makedirs(path, exist_ok=True)\n",
        "\n",
        "\n",
        "# -----------------------\n",
        "# Data generation\n",
        "# -----------------------\n",
        "\n",
        "def generate_multiverse_data(\n",
        "    samples: int,\n",
        "    input_dim: int,\n",
        "    num_classes: int,\n",
        "    noise: float = 0.2,\n",
        "    random_state: int = 42,\n",
        ") -> Tuple[np.ndarray, np.ndarray]:\n",
        "    \"\"\"\n",
        "    Generates learnable synthetic classification data:\n",
        "    - Features X in [0, 1]^input_dim\n",
        "    - Class logits via linear separators per class (+ Gaussian noise)\n",
        "    - Labels are argmax over those logits\n",
        "    \"\"\"\n",
        "    rng = np.random.default_rng(random_state)\n",
        "    X = rng.random((samples, input_dim)).astype(np.float32)\n",
        "\n",
        "    # Class-specific linear template\n",
        "    W = rng.normal(0, 1, size=(input_dim, num_classes)).astype(np.float32)\n",
        "    b = rng.normal(0, 0.5, size=(num_classes,)).astype(np.float32)\n",
        "\n",
        "    logits = X @ W + b  # shape: (N, C)\n",
        "    if noise > 0:\n",
        "        logits += rng.normal(0, noise, size=logits.shape).astype(np.float32)\n",
        "\n",
        "    y = np.argmax(logits, axis=1).astype(np.int32)  # shape: (N,)\n",
        "    return X, y\n",
        "\n",
        "\n",
        "def make_datasets(\n",
        "    X: np.ndarray,\n",
        "    y: np.ndarray,\n",
        "    val_ratio: float,\n",
        "    batch_size: int,\n",
        "    seed: int = 42,\n",
        "):\n",
        "    n = X.shape[0]\n",
        "    idx = np.arange(n)\n",
        "    rng = np.random.default_rng(seed)\n",
        "    rng.shuffle(idx)\n",
        "\n",
        "    n_val = max(1, int(val_ratio * n))\n",
        "    val_idx = idx[:n_val]\n",
        "    train_idx = idx[n_val:]\n",
        "\n",
        "    X_train, y_train = X[train_idx], y[train_idx]\n",
        "    X_val, y_val = X[val_idx], y[val_idx]\n",
        "\n",
        "    train_ds = (\n",
        "        tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
        "        .shuffle(buffer_size=X_train.shape[0], seed=seed, reshuffle_each_iteration=True)\n",
        "        .batch(batch_size)\n",
        "        .prefetch(tf.data.AUTOTUNE)\n",
        "    )\n",
        "    val_ds = (\n",
        "        tf.data.Dataset.from_tensor_slices((X_val, y_val))\n",
        "        .batch(batch_size)\n",
        "        .prefetch(tf.data.AUTOTUNE)\n",
        "    )\n",
        "    return train_ds, val_ds, (X_train, y_train, X_val, y_val)\n",
        "\n",
        "\n",
        "# -----------------------\n",
        "# Model\n",
        "# -----------------------\n",
        "\n",
        "class MultiverseSimulation(Model):\n",
        "    def __init__(\n",
        "        self,\n",
        "        input_dim: int,\n",
        "        num_classes: int,\n",
        "        hidden1: int = 128,\n",
        "        hidden2: int = 128,\n",
        "        activation: str = \"relu\",\n",
        "        dropout: float = 0.0,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.d1 = Dense(hidden1, activation=activation)\n",
        "        self.drop1 = Dropout(dropout) if dropout and dropout > 0 else None\n",
        "        self.d2 = Dense(hidden2, activation=activation)\n",
        "        self.drop2 = Dropout(dropout) if dropout and dropout > 0 else None\n",
        "        self.out = Dense(num_classes, activation=\"softmax\")\n",
        "        # Build weights by calling once with a known input shape\n",
        "        self._input_dim = input_dim\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        # Explicitly build layers with known input shapes for saved model signatures\n",
        "        _ = self.call(tf.zeros((1, self._input_dim), dtype=tf.float32))\n",
        "        super().build(input_shape)\n",
        "\n",
        "    def call(self, inputs, training: Optional[bool] = None):\n",
        "        x = self.d1(inputs)\n",
        "        if self.drop1 is not None:\n",
        "            x = self.drop1(x, training=training)\n",
        "        x = self.d2(x)\n",
        "        if self.drop2 is not None:\n",
        "            x = self.drop2(x, training=training)\n",
        "        return self.out(x)\n",
        "\n",
        "\n",
        "def build_model(\n",
        "    input_dim: int,\n",
        "    num_classes: int,\n",
        "    hidden1: int,\n",
        "    hidden2: int,\n",
        "    activation: str,\n",
        "    dropout: float,\n",
        "    learning_rate: float,\n",
        ") -> tf.keras.Model:\n",
        "    model = MultiverseSimulation(\n",
        "        input_dim=input_dim,\n",
        "        num_classes=num_classes,\n",
        "        hidden1=hidden1,\n",
        "        hidden2=hidden2,\n",
        "        activation=activation,\n",
        "        dropout=dropout,\n",
        "    )\n",
        "    model.compile(\n",
        "        optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
        "        loss=\"sparse_categorical_crossentropy\",\n",
        "        metrics=[\"accuracy\"],\n",
        "    )\n",
        "    return model\n",
        "\n",
        "\n",
        "# -----------------------\n",
        "# Artifacts\n",
        "# -----------------------\n",
        "\n",
        "def save_artifacts(\n",
        "    outdir: str,\n",
        "    model: tf.keras.Model,\n",
        "    config: dict,\n",
        "    history: dict,\n",
        "    metrics: dict,\n",
        "    save_model_flag: bool,\n",
        "):\n",
        "    ensure_outdir(outdir)\n",
        "    if save_model_flag:\n",
        "        final_path = os.path.join(outdir, \"model.keras\")\n",
        "        model.save(final_path)\n",
        "    with open(os.path.join(outdir, \"run_summary.json\"), \"w\") as f:\n",
        "        json.dump({\"config\": config, \"history\": history, \"metrics\": metrics}, f, indent=2)\n",
        "\n",
        "\n",
        "# -----------------------\n",
        "# CLI\n",
        "# -----------------------\n",
        "\n",
        "def parse_args():\n",
        "    parser = argparse.ArgumentParser(\n",
        "        description=\"Train a Keras multiverse classifier on synthetic, learnable data (Jupyter/Colab-safe).\",\n",
        "        formatter_class=argparse.ArgumentDefaultsHelpFormatter,\n",
        "    )\n",
        "    # Data\n",
        "    parser.add_argument(\"--samples\", type=int, default=2000, help=\"Number of samples to generate.\")\n",
        "    parser.add_argument(\"--input_dim\", type=int, default=5, help=\"Number of input features.\")\n",
        "    parser.add_argument(\"--num_classes\", type=int, default=10, help=\"Number of classes.\")\n",
        "    parser.add_argument(\"--val_ratio\", type=float, default=0.2, help=\"Validation split ratio.\")\n",
        "    parser.add_argument(\"--noise\", type=float, default=0.2, help=\"Stddev of Gaussian noise added to class logits.\")\n",
        "\n",
        "    # Model & training\n",
        "    parser.add_argument(\"--hidden1\", type=int, default=128, help=\"Units in first hidden layer.\")\n",
        "    parser.add_argument(\"--hidden2\", type=int, default=128, help=\"Units in second hidden layer.\")\n",
        "    parser.add_argument(\"--activation\", type=str, default=\"relu\", help=\"Activation for hidden layers.\")\n",
        "    parser.add_argument(\"--dropout\", type=float, default=0.0, help=\"Dropout rate (0 disables).\")\n",
        "    parser.add_argument(\"--learning_rate\", type=float, default=1e-3, help=\"Adam learning rate.\")\n",
        "    parser.add_argument(\"--epochs\", type=int, default=50, help=\"Max epochs.\")\n",
        "    parser.add_argument(\"--batch_size\", type=int, default=32, help=\"Batch size.\")\n",
        "    parser.add_argument(\"--patience\", type=int, default=10, help=\"Early stopping patience (epochs).\")\n",
        "\n",
        "    # Run control\n",
        "    parser.add_argument(\"--random_state\", type=int, default=42, help=\"Random seed.\")\n",
        "    parser.add_argument(\"--outdir\", type=str, default=\"outputs_multiverse\", help=\"Directory for artifacts.\")\n",
        "    parser.add_argument(\"--save_model\", type=str, default=\"true\", help=\"Save best/final models (true/false).\")\n",
        "\n",
        "    # Inference\n",
        "    parser.add_argument(\n",
        "        \"--test_input\",\n",
        "        type=str,\n",
        "        default=None,\n",
        "        help=\"Comma-separated feature values for a single prediction, e.g., '0.8,0.1,0.3,0.9,0.2'. Must match input_dim.\",\n",
        "    )\n",
        "\n",
        "    # Important for notebooks: ignore unknown args (e.g., -f <kernel.json>)\n",
        "    args, _ = parser.parse_known_args()\n",
        "    args.save_model = str2bool(args.save_model)\n",
        "    return args\n",
        "\n",
        "\n",
        "# -----------------------\n",
        "# Main\n",
        "# -----------------------\n",
        "\n",
        "def main():\n",
        "    args = parse_args()\n",
        "    set_seed(args.random_state)\n",
        "\n",
        "    # Data\n",
        "    X, y = generate_multiverse_data(\n",
        "        samples=args.samples,\n",
        "        input_dim=args.input_dim,\n",
        "        num_classes=args.num_classes,\n",
        "        noise=args.noise,\n",
        "        random_state=args.random_state,\n",
        "    )\n",
        "    train_ds, val_ds, (X_train, y_train, X_val, y_val) = make_datasets(\n",
        "        X, y, val_ratio=args.val_ratio, batch_size=args.batch_size, seed=args.random_state\n",
        "    )\n",
        "\n",
        "    # Model\n",
        "    model = build_model(\n",
        "        input_dim=args.input_dim,\n",
        "        num_classes=args.num_classes,\n",
        "        hidden1=args.hidden1,\n",
        "        hidden2=args.hidden2,\n",
        "        activation=args.activation,\n",
        "        dropout=args.dropout,\n",
        "        learning_rate=args.learning_rate,\n",
        "    )\n",
        "    # Build and show summary\n",
        "    _ = model(tf.zeros((1, args.input_dim), dtype=tf.float32))  # build weights\n",
        "    model.summary()\n",
        "\n",
        "    # Callbacks\n",
        "    callbacks = [\n",
        "        EarlyStopping(monitor=\"val_loss\", patience=args.patience, restore_best_weights=True, verbose=1),\n",
        "        ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=max(3, args.patience // 2), min_lr=1e-6, verbose=1),\n",
        "    ]\n",
        "    if args.save_model:\n",
        "        ensure_outdir(args.outdir)\n",
        "        callbacks.append(\n",
        "            ModelCheckpoint(\n",
        "                filepath=os.path.join(args.outdir, \"best.keras\"),\n",
        "                monitor=\"val_loss\",\n",
        "                save_best_only=True,\n",
        "                save_weights_only=False,\n",
        "                verbose=1,\n",
        "            )\n",
        "        )\n",
        "\n",
        "    # Train\n",
        "    history = model.fit(\n",
        "        train_ds,\n",
        "        validation_data=val_ds,\n",
        "        epochs=args.epochs,\n",
        "        callbacks=callbacks,\n",
        "        verbose=1,\n",
        "    )\n",
        "\n",
        "    # Evaluate\n",
        "    val_loss, val_acc = model.evaluate(val_ds, verbose=0)\n",
        "    print(f\"Validation — loss: {val_loss:.6f}, accuracy: {val_acc:.4f}\")\n",
        "\n",
        "    # Optional single prediction\n",
        "    test_pred = None\n",
        "    if args.test_input:\n",
        "        try:\n",
        "            feats = np.array([float(x) for x in args.test_input.split(\",\")], dtype=np.float32)\n",
        "            assert feats.shape[0] == args.input_dim, f\"test_input length {feats.shape[0]} != input_dim {args.input_dim}\"\n",
        "            probs = model.predict(feats.reshape(1, -1), verbose=0)[0]\n",
        "            pred_class = int(np.argmax(probs))\n",
        "            test_pred = {\n",
        "                \"input\": feats.tolist(),\n",
        "                \"predicted_class\": pred_class,\n",
        "                \"probabilities\": [float(p) for p in probs.tolist()],\n",
        "                \"top5\": sorted([(int(i), float(p)) for i, p in enumerate(probs)], key=lambda t: t[1], reverse=True)[:5],\n",
        "            }\n",
        "            print(f\"Test input -> class: {pred_class}, probs (first 5): {test_pred['probabilities'][:5]}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Failed to run test prediction: {e}\")\n",
        "\n",
        "    # Save artifacts\n",
        "    config = {\n",
        "        \"samples\": args.samples,\n",
        "        \"input_dim\": args.input_dim,\n",
        "        \"num_classes\": args.num_classes,\n",
        "        \"val_ratio\": args.val_ratio,\n",
        "        \"noise\": args.noise,\n",
        "        \"hidden1\": args.hidden1,\n",
        "        \"hidden2\": args.hidden2,\n",
        "        \"activation\": args.activation,\n",
        "        \"dropout\": args.dropout,\n",
        "        \"learning_rate\": args.learning_rate,\n",
        "        \"epochs\": args.epochs,\n",
        "        \"batch_size\": args.batch_size,\n",
        "        \"patience\": args.patience,\n",
        "        \"random_state\": args.random_state,\n",
        "        \"outdir\": args.outdir,\n",
        "    }\n",
        "    metrics = {\"val_loss\": float(val_loss), \"val_accuracy\": float(val_acc)}\n",
        "    if test_pred is not None:\n",
        "        metrics[\"test_prediction\"] = test_pred\n",
        "\n",
        "    save_artifacts(\n",
        "        outdir=args.outdir,\n",
        "        model=model,\n",
        "        config=config,\n",
        "        history=history.history,\n",
        "        metrics=metrics,\n",
        "        save_model_flag=args.save_model,\n",
        "    )\n",
        "    print(f\"Artifacts saved to: {args.outdir}\")\n",
        "    print(\"Done.\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ]
}