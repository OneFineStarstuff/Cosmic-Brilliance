{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyNCqvbI61AfvBSvdElY9xNw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OneFineStarstuff/Cosmic-Brilliance/blob/main/e2e_grid_py.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DesGz16GS-_n"
      },
      "outputs": [],
      "source": [
        "#!/usr/bin/env python3\n",
        "# e2e_grid.py\n",
        "# End-to-end rollout with dynamic ethics, temporal memory decay, goal flip, and noise.\n",
        "\n",
        "from __future__ import annotations\n",
        "import argparse\n",
        "import json\n",
        "import math\n",
        "import random\n",
        "from dataclasses import dataclass, field\n",
        "from typing import Dict, List, Optional, Set, Tuple\n",
        "\n",
        "# ---------------------------\n",
        "# Utilities\n",
        "# ---------------------------\n",
        "\n",
        "Action = str  # 'UP'|'DOWN'|'LEFT'|'RIGHT'\n",
        "Pos = Tuple[int, int]\n",
        "\n",
        "ACTIONS: Tuple[Action, ...] = ('UP', 'DOWN', 'LEFT', 'RIGHT')\n",
        "DELTA: Dict[Action, Tuple[int, int]] = {\n",
        "    'UP': (0, -1),\n",
        "    'DOWN': (0, 1),\n",
        "    'LEFT': (-1, 0),\n",
        "    'RIGHT': (1, 0),\n",
        "}\n",
        "\n",
        "def manhattan(a: Pos, b: Pos) -> int:\n",
        "    return abs(a[0]-b[0]) + abs(a[1]-b[1])\n",
        "\n",
        "def in_bounds(p: Pos, n: int) -> bool:\n",
        "    return 0 <= p[0] < n and 0 <= p[1] < n\n",
        "\n",
        "def parse_xy(s: str) -> Pos:\n",
        "    x, y = s.split(',')\n",
        "    return (int(x.strip()), int(y.strip()))\n",
        "\n",
        "def parse_dyn_spec(s: str) -> Dict[int, Set[Pos]]:\n",
        "    \"\"\"\n",
        "    Parse dynamic forbidden schedule like:\n",
        "      \"3:(1,1);4:(1,2);6:(2,2)\"\n",
        "    meaning: at t=3 add (1,1), at t=4 add (1,2), at t=6 add (2,2).\n",
        "    \"\"\"\n",
        "    events: Dict[int, Set[Pos]] = {}\n",
        "    if not s:\n",
        "        return events\n",
        "    for part in s.split(';'):\n",
        "        part = part.strip()\n",
        "        if not part:\n",
        "            continue\n",
        "        ts, coords = part.split(':')\n",
        "        t = int(ts.strip())\n",
        "        x, y = coords.strip().strip('()').split(',')\n",
        "        pos = (int(x), int(y))\n",
        "        events.setdefault(t, set()).add(pos)\n",
        "    return events\n",
        "\n",
        "def seeded_rng(seed: int) -> random.Random:\n",
        "    rng = random.Random(seed)\n",
        "    return rng\n",
        "\n",
        "# ---------------------------\n",
        "# Environment\n",
        "# ---------------------------\n",
        "\n",
        "@dataclass\n",
        "class GridWorld:\n",
        "    n: int\n",
        "    start: Pos\n",
        "    goal: Pos\n",
        "    forbidden: Set[Pos] = field(default_factory=set)\n",
        "    dyn_events: Dict[int, Set[Pos]] = field(default_factory=dict)\n",
        "    action_noise: float = 0.0\n",
        "    rng: random.Random = field(default_factory=lambda: seeded_rng(0))\n",
        "\n",
        "    t: int = 0\n",
        "    pos: Pos = field(init=False)\n",
        "\n",
        "    def __post_init__(self):\n",
        "        self.pos = self.start\n",
        "\n",
        "    def maybe_apply_dynamic_forbidden(self):\n",
        "        if self.t in self.dyn_events:\n",
        "            for p in self.dyn_events[self.t]:\n",
        "                self.forbidden.add(p)\n",
        "\n",
        "    def sample_action(self, a: Action) -> Action:\n",
        "        if self.action_noise <= 0:\n",
        "            return a\n",
        "        if self.rng.random() < self.action_noise:\n",
        "            return self.rng.choice(ACTIONS)\n",
        "        return a\n",
        "\n",
        "    def step(self, a: Action) -> Dict:\n",
        "        # Apply dynamic ethics updates at the start of each tick.\n",
        "        self.maybe_apply_dynamic_forbidden()\n",
        "\n",
        "        orig_pos = self.pos\n",
        "        a_exec = self.sample_action(a)\n",
        "        dx, dy = DELTA[a_exec]\n",
        "        cand = (orig_pos[0] + dx, orig_pos[1] + dy)\n",
        "\n",
        "        status = 'same'\n",
        "        reward = 0.0\n",
        "        reached = False\n",
        "\n",
        "        if not in_bounds(cand, self.n) or cand in self.forbidden:\n",
        "            # Blocked move\n",
        "            cand = orig_pos\n",
        "            status = 'blocked'\n",
        "            reward = -1.0\n",
        "        else:\n",
        "            # Move accepted\n",
        "            before = manhattan(orig_pos, self.goal)\n",
        "            after = manhattan(cand, self.goal)\n",
        "            if after < before:\n",
        "                status = 'closer'\n",
        "                reward = 1.0\n",
        "            elif after > before:\n",
        "                status = 'farther'\n",
        "                reward = -1.0\n",
        "            else:\n",
        "                status = 'same'\n",
        "                reward = 0.0\n",
        "\n",
        "        self.pos = cand\n",
        "\n",
        "        if self.pos == self.goal:\n",
        "            status = 'reached'\n",
        "            reward = 11.0  # shaped terminal bonus (+1 + 10)\n",
        "            reached = True\n",
        "\n",
        "        rec = {\n",
        "            't': self.t,\n",
        "            'pos': orig_pos,\n",
        "            'action': {'type': 'MOVE', 'dir': a, 'exec': a_exec},\n",
        "            'outcome': status,\n",
        "            'reward': reward,\n",
        "            'goal': self.goal,\n",
        "            'reached': reached\n",
        "        }\n",
        "        self.t += 1\n",
        "        return rec\n",
        "\n",
        "# ---------------------------\n",
        "# Agent + Predictor + Memory\n",
        "# ---------------------------\n",
        "\n",
        "@dataclass\n",
        "class MemoryItem:\n",
        "    t: int\n",
        "    pos: Pos\n",
        "    action: Action\n",
        "    predicted_outcome: str\n",
        "    observed_outcome: str\n",
        "\n",
        "@dataclass\n",
        "class MemoryBank:\n",
        "    decay_lambda: float = 0.0\n",
        "    items: List[MemoryItem] = field(default_factory=list)\n",
        "\n",
        "    def add(self, it: MemoryItem):\n",
        "        self.items.append(it)\n",
        "\n",
        "    def contradiction_stats(self, t_now: int) -> Dict[str, float]:\n",
        "        if not self.items:\n",
        "            return {'processed': 0, 'contradictory': 0, 'avg_all': 0.0, 'avg_contra': 0.0}\n",
        "        weights = []\n",
        "        matches = []\n",
        "        contra_matches = []\n",
        "        for it in self.items:\n",
        "            w = math.exp(-self.decay_lambda * max(0, t_now - it.t))\n",
        "            is_match = 1.0 if (it.predicted_outcome == it.observed_outcome) else 0.0\n",
        "            weights.append(w)\n",
        "            matches.append(is_match * w)\n",
        "            if not is_match:\n",
        "                contra_matches.append(is_match * w)  # = 0, placeholder for symmetry\n",
        "        total_w = sum(weights)\n",
        "        avg_all = (sum(matches) / total_w) if total_w > 0 else 0.0\n",
        "        # For avg_contra, compute weighted accuracy among contradictory items (which is zero by definition),\n",
        "        # but report density as num_contra / processed instead.\n",
        "        contradictory = sum(1 for it in self.items if it.predicted_outcome != it.observed_outcome)\n",
        "        return {\n",
        "            'processed': len(self.items),\n",
        "            'contradictory': contradictory,\n",
        "            'avg_all': avg_all,\n",
        "            'avg_contra': 0.0,\n",
        "        }\n",
        "\n",
        "class GreedyAgent:\n",
        "    def __init__(self, n: int, avoid_forbidden: bool = True, rng: Optional[random.Random] = None):\n",
        "        self.n = n\n",
        "        self.avoid_forbidden = avoid_forbidden\n",
        "        self.rng = rng or seeded_rng(0)\n",
        "\n",
        "    def plan(self, pos: Pos, goal: Pos, forbidden_view: Set[Pos]) -> Action:\n",
        "        # Try actions that reduce Manhattan distance; avoid forbidden if possible.\n",
        "        best_actions: List[Action] = []\n",
        "        best_delta = 1e9\n",
        "        for a in ACTIONS:\n",
        "            dx, dy = DELTA[a]\n",
        "            cand = (pos[0] + dx, pos[1] + dy)\n",
        "            if not in_bounds(cand, self.n):\n",
        "                continue\n",
        "            if self.avoid_forbidden and cand in forbidden_view:\n",
        "                continue\n",
        "            d = manhattan(cand, goal)\n",
        "            if d < best_delta:\n",
        "                best_delta = d\n",
        "                best_actions = [a]\n",
        "            elif d == best_delta:\n",
        "                best_actions.append(a)\n",
        "        if best_actions:\n",
        "            # Add a consistent tie-break preference to make runs reproducible.\n",
        "            pref = ('DOWN', 'RIGHT', 'UP', 'LEFT')\n",
        "            best_actions = sorted(best_actions, key=lambda x: pref.index(x) if x in pref else 999)\n",
        "            return best_actions[0]\n",
        "        # Fallback: any valid move\n",
        "        valid = []\n",
        "        for a in ACTIONS:\n",
        "            dx, dy = DELTA[a]\n",
        "            cand = (pos[0] + dx, pos[1] + dy)\n",
        "            if in_bounds(cand, self.n) and (not self.avoid_forbidden or cand not in forbidden_view):\n",
        "                valid.append(a)\n",
        "        return valid[0] if valid else 'RIGHT'\n",
        "\n",
        "    def predict_outcome(self, pos: Pos, a: Action, goal: Pos, forbidden_view: Set[Pos]) -> str:\n",
        "        # Deterministic, no-noise predictor using current forbidden_view (may be stale vs env updates).\n",
        "        dx, dy = DELTA[a]\n",
        "        cand = (pos[0] + dx, pos[1] + dy)\n",
        "        if not in_bounds(cand, self.n) or cand in forbidden_view:\n",
        "            return 'blocked'\n",
        "        if cand == goal:\n",
        "            return 'reached'\n",
        "        before = manhattan(pos, goal)\n",
        "        after = manhattan(cand, goal)\n",
        "        if after < before:\n",
        "            return 'closer'\n",
        "        if after > before:\n",
        "            return 'farther'\n",
        "        return 'same'\n",
        "\n",
        "# ---------------------------\n",
        "# Rollout + Auditing + Training\n",
        "# ---------------------------\n",
        "\n",
        "@dataclass\n",
        "class RolloutConfig:\n",
        "    grid: int = 5\n",
        "    start: Pos = (0, 1)\n",
        "    goal: Pos = (3, 3)\n",
        "    tmax: int = 50\n",
        "    t_flip: Optional[int] = None\n",
        "    dynamic_forbidden: Dict[int, Set[Pos]] = field(default_factory=dict)\n",
        "    action_noise: float = 0.0\n",
        "    decay_lambda: float = 0.0\n",
        "    seed: int = 7\n",
        "    scenario: str = 'baseline'  # baseline|dynamic-ethics|temporal-decay|flip-goal|inject-noise|all\n",
        "    log_jsonl: Optional[str] = None\n",
        "\n",
        "@dataclass\n",
        "class RolloutResult:\n",
        "    steps: List[Dict]\n",
        "    memory: MemoryBank\n",
        "    contradictions: Dict[str, float]\n",
        "\n",
        "def run_rollout(cfg: RolloutConfig) -> RolloutResult:\n",
        "    rng = seeded_rng(cfg.seed)\n",
        "    env = GridWorld(\n",
        "        n=cfg.grid,\n",
        "        start=cfg.start,\n",
        "        goal=cfg.goal,\n",
        "        forbidden=set(),\n",
        "        dyn_events=cfg.dynamic_forbidden,\n",
        "        action_noise=cfg.action_noise,\n",
        "        rng=rng\n",
        "    )\n",
        "    agent = GreedyAgent(n=cfg.grid, avoid_forbidden=True, rng=rng)\n",
        "    memory = MemoryBank(decay_lambda=cfg.decay_lambda)\n",
        "\n",
        "    steps: List[Dict] = []\n",
        "    log_f = open(cfg.log_jsonl, 'w') if cfg.log_jsonl else None\n",
        "\n",
        "    # Print header\n",
        "    print(\"=== Rollout ===\")\n",
        "\n",
        "    for t in range(cfg.tmax):\n",
        "        # Goal flip mid-rollout if configured\n",
        "        if cfg.t_flip is not None and env.t == cfg.t_flip:\n",
        "            # Flip to a symmetric opposite corner by default if not specified explicitly\n",
        "            old_goal = env.goal\n",
        "            env.goal = (cfg.grid - 1 - old_goal[0], cfg.grid - 1 - old_goal[1])\n",
        "            print(f\"[Info] Goal flipped at t={env.t} from {old_goal} to {env.goal}\")\n",
        "\n",
        "        # Predictor uses the current view before env applies dynamic changes (causes contradictions)\n",
        "        current_forbidden_view = set(env.forbidden)\n",
        "        pos = env.pos\n",
        "        goal = env.goal\n",
        "        a = agent.plan(pos, goal, current_forbidden_view)\n",
        "        predicted = agent.predict_outcome(pos, a, goal, current_forbidden_view)\n",
        "\n",
        "        rec = env.step(a)\n",
        "        # Audit and memory\n",
        "        memory.add(MemoryItem(\n",
        "            t=rec['t'],\n",
        "            pos=rec['pos'],\n",
        "            action=rec['action']['dir'],\n",
        "            predicted_outcome=predicted,\n",
        "            observed_outcome=rec['outcome'],\n",
        "        ))\n",
        "\n",
        "        steps.append(rec)\n",
        "        print(f\"t={rec['t']:02d} pos={rec['pos']} action={{'type': 'MOVE', 'dir': '{rec['action']['dir']}'}} \"\n",
        "              f\"outcome={rec['outcome']} reward={rec['reward']} goal={rec['goal']} reached={rec['reached']}\")\n",
        "\n",
        "        if log_f:\n",
        "            log_f.write(json.dumps(rec) + \"\\n\")\n",
        "\n",
        "        if rec['reached']:\n",
        "            print(\"Goal reached. Stopping rollout.\")\n",
        "            break\n",
        "\n",
        "    if log_f:\n",
        "        log_f.close()\n",
        "\n",
        "    # Contradictions audit\n",
        "    print(\"\\n=== Audited contradictions ===\")\n",
        "    stats = memory.contradiction_stats(t_now=env.t)\n",
        "    # We surface raw contradictions, density, and weighted avg match\n",
        "    if stats['processed'] == 0:\n",
        "        print(\"(none)\")\n",
        "    else:\n",
        "        density = stats['contradictory'] / stats['processed'] if stats['processed'] else 0.0\n",
        "        print(f\"processed={stats['processed']} contradictory={stats['contradictory']} density={density:.3f} \"\n",
        "              f\"avg_match_all={stats['avg_all']:.3f}\")\n",
        "\n",
        "    # Batched revise placeholder (no parameter updates in this simple demo)\n",
        "    print(\"\\n=== Batched revise ===\")\n",
        "    print(f\"[Revision] Contradictions processed: {stats['contradictory']}\")\n",
        "\n",
        "    # Curriculum training demo\n",
        "    print(\"\\n=== Curriculum training ===\")\n",
        "    thresholds = [0.70, 0.78, 0.87]\n",
        "    for i, th in enumerate(thresholds, start=1):\n",
        "        # In a real loop, we'd adapt policy if density > (1 - th). Here we just report.\n",
        "        density = stats['contradictory'] / stats['processed'] if stats['processed'] else 0.0\n",
        "        avg_all = stats['avg_all']\n",
        "        avg_contra = stats['avg_contra']  # zero by definition here\n",
        "        print(f\"[Training] step={i} processed={stats['processed']} contradictory={stats['contradictory']} \"\n",
        "              f\"threshold={th:.2f} avg_all={avg_all:.3f} avg_contra={avg_contra:.3f}\")\n",
        "\n",
        "    return RolloutResult(steps=steps, memory=memory, contradictions=stats)\n",
        "\n",
        "# ---------------------------\n",
        "# Scenario presets\n",
        "# ---------------------------\n",
        "\n",
        "def make_config_from_args(args: argparse.Namespace) -> RolloutConfig:\n",
        "    start = parse_xy(args.start)\n",
        "    goal = parse_xy(args.goal)\n",
        "    dyn = parse_dyn_spec(args.dynamic)\n",
        "\n",
        "    scenario = args.scenario\n",
        "    cfg = RolloutConfig(\n",
        "        grid=args.grid,\n",
        "        start=start,\n",
        "        goal=goal,\n",
        "        tmax=args.tmax,\n",
        "        t_flip=args.t_flip if scenario in ('flip-goal', 'all') else None,\n",
        "        dynamic_forbidden=dyn if scenario in ('dynamic-ethics', 'all') else {},\n",
        "        action_noise=(args.noise if scenario in ('inject-noise', 'all') else 0.0),\n",
        "        decay_lambda=(args.decay if scenario in ('temporal-decay', 'all') else 0.0),\n",
        "        seed=args.seed,\n",
        "        scenario=scenario,\n",
        "        log_jsonl=args.log,\n",
        "    )\n",
        "    return cfg\n",
        "\n",
        "# ---------------------------\n",
        "# CLI\n",
        "# ---------------------------\n",
        "\n",
        "def main():\n",
        "    p = argparse.ArgumentParser(description=\"End-to-end grid rollout with audit and curriculum.\")\n",
        "    p.add_argument('--grid', type=int, default=5, help='Grid size N (NxN)')\n",
        "    p.add_argument('--start', type=str, default='0,1', help='Start position \"x,y\"')\n",
        "    p.add_argument('--goal', type=str, default='3,3', help='Goal position \"x,y\"')\n",
        "    p.add_argument('--tmax', type=int, default=50, help='Max timesteps')\n",
        "    p.add_argument('--t_flip', type=int, default=None, help='Timestep to flip the goal (only in flip-goal or all)')\n",
        "    p.add_argument('--dynamic', type=str, default='', help='Dynamic forbidden spec, e.g. \"3:(1,1);6:(2,2)\"')\n",
        "    p.add_argument('--noise', type=float, default=0.0, help='Action noise probability (only in inject-noise or all)')\n",
        "    p.add_argument('--decay', type=float, default=0.0, help='Temporal decay lambda (only in temporal-decay or all)')\n",
        "    p.add_argument('--seed', type=int, default=7, help='RNG seed')\n",
        "    p.add_argument('--log', type=str, default=None, help='Path to JSONL step log')\n",
        "    p.add_argument('--scenario', type=str, default='baseline',\n",
        "                   choices=['baseline', 'dynamic-ethics', 'temporal-decay', 'flip-goal', 'inject-noise', 'all'],\n",
        "                   help='Which friction(s) to enable')\n",
        "\n",
        "    args = p.parse_args()\n",
        "    cfg = make_config_from_args(args)\n",
        "    run_rollout(cfg)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ]
    }
  ]
}