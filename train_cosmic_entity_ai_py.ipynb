{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyO2EmFtY4QyQzq6t7NQ0oOq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OneFineStarstuff/Cosmic-Brilliance/blob/main/train_cosmic_entity_ai_py.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5tlrcv0mrqfj"
      },
      "outputs": [],
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "train_cosmic_entity_ai.py\n",
        "\n",
        "End‐to‐end pipeline for CosmicEntityAI:\n",
        "1. Synthetic “cosmic entity” dataset of 6 inputs → 3 targets\n",
        "2. Float32 normalization and dtype consistency\n",
        "3. MLP with LayerNorm, Dropout & ReLU\n",
        "4. Physics‐informed residual enforcing toy transformation laws\n",
        "5. MC‐Dropout for uncertainty quantification\n",
        "6. Training loop with AdamW, ReduceLROnPlateau, gradient clipping, NaN checks, early stopping\n",
        "7. Safe checkpoint loading\n",
        "8. Visualizations: training history, true vs. predicted scatter, uncertainty heatmap\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 1. Synthetic Cosmic Entity Dataset\n",
        "# ------------------------------------------------------------------------------\n",
        "class CosmicEntityDataset(Dataset):\n",
        "    def __init__(self, n_samples=5000, seed=42):\n",
        "        np.random.seed(seed)\n",
        "        # Features in double precision\n",
        "        RC   = np.random.uniform(0.1, 10.0, (n_samples,1))   # Reality control param\n",
        "        OP   = np.random.rand(n_samples,1)                   # Omniscience probability\n",
        "        SEF1 = np.random.uniform(0.1, 5.0, (n_samples,1))    # Synthetic factor 1\n",
        "        SEF2 = np.random.uniform(0.1, 5.0, (n_samples,1))    # Synthetic factor 2\n",
        "        SEF3 = np.random.uniform(0.1, 5.0, (n_samples,1))    # Synthetic factor 3\n",
        "        SEF4 = np.random.uniform(0.1, 5.0, (n_samples,1))    # Synthetic factor 4\n",
        "\n",
        "        X_raw = np.hstack([RC, OP, SEF1, SEF2, SEF3, SEF4]).astype(np.float64)\n",
        "\n",
        "        # Toy “transformation laws” targets\n",
        "        eps = 1e-6\n",
        "        # 1. Transformation Entropy (TE)\n",
        "        TE = RC * OP / (SEF1 + eps)\n",
        "        # 2. Omniversal Emergence (OE)\n",
        "        OE = (SEF1 * SEF2 * SEF3 * SEF4)**0.25 * RC\n",
        "        # 3. Restructuring Power (RP)\n",
        "        RP = np.log1p(RC) * OP\n",
        "\n",
        "        Y_raw = np.hstack([TE, OE, RP]).astype(np.float64)\n",
        "        Y_raw += 0.01 * Y_raw.std(axis=0) * np.random.randn(*Y_raw.shape)\n",
        "\n",
        "        # Compute normalization stats (float64)\n",
        "        self.X_mean = X_raw.mean(axis=0)\n",
        "        self.X_std  = X_raw.std(axis=0) + 1e-8\n",
        "        self.Y_mean = Y_raw.mean(axis=0)\n",
        "        self.Y_std  = Y_raw.std(axis=0) + 1e-8\n",
        "\n",
        "        # Standardize and cast to float32\n",
        "        self.X = ((X_raw - self.X_mean) / self.X_std).astype(np.float32)\n",
        "        self.Y = ((Y_raw - self.Y_mean) / self.Y_std).astype(np.float32)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return torch.from_numpy(self.X[idx]), torch.from_numpy(self.Y[idx])\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 2. CosmicEntityAI Model Definition\n",
        "# ------------------------------------------------------------------------------\n",
        "class CosmicEntityAI(nn.Module):\n",
        "    def __init__(self, input_dim=6, hidden_dims=(64,64), output_dim=3, p_drop=0.1):\n",
        "        super().__init__()\n",
        "        layers, d = [], input_dim\n",
        "        for h in hidden_dims:\n",
        "            layers += [\n",
        "                nn.Linear(d, h),\n",
        "                nn.LayerNorm(h),\n",
        "                nn.ReLU(),\n",
        "                nn.Dropout(p_drop)\n",
        "            ]\n",
        "            d = h\n",
        "        layers.append(nn.Linear(d, output_dim))\n",
        "        self.net = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 3. Physics-Informed Residual Loss\n",
        "# ------------------------------------------------------------------------------\n",
        "def physics_residual(pred, X, stats):\n",
        "    # Denormalize inputs\n",
        "    X_den = X * stats['X_std'] + stats['X_mean']\n",
        "    RC, OP, f1, f2, f3, f4 = X_den.t()\n",
        "    eps = 1e-6\n",
        "\n",
        "    TE_t = RC * OP / (f1 + eps)\n",
        "    OE_t = (f1 * f2 * f3 * f4)**0.25 * RC\n",
        "    RP_t = torch.log1p(RC) * OP\n",
        "\n",
        "    Yt = torch.stack([TE_t, OE_t, RP_t], dim=1)\n",
        "    Yt_norm = (Yt - stats['Y_mean']) / stats['Y_std']\n",
        "    return nn.MSELoss()(pred, Yt_norm)\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 4. Combined Loss Function\n",
        "# ------------------------------------------------------------------------------\n",
        "def total_loss(pred, true, X, stats, λ=1.0):\n",
        "    mse  = nn.MSELoss()(pred, true)\n",
        "    phys = physics_residual(pred, X, stats)\n",
        "    return mse + λ * phys, mse, phys\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 5. MC-Dropout Uncertainty Quantification\n",
        "# ------------------------------------------------------------------------------\n",
        "def mc_dropout_predict(model, X, T=50):\n",
        "    model.train()\n",
        "    preds = []\n",
        "    with torch.no_grad():\n",
        "        for _ in range(T):\n",
        "            preds.append(model(X))\n",
        "    stacked = torch.stack(preds, dim=0)\n",
        "    return stacked.mean(0), stacked.std(0)\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 6. Training Loop with NaN Safety & Checkpointing\n",
        "# ------------------------------------------------------------------------------\n",
        "def train(model, train_loader, val_loader, stats, device,\n",
        "          lr=1e-4, wd=1e-5, λ=1.0, epochs=100, patience=10):\n",
        "    model.to(device)\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=wd)\n",
        "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
        "        optimizer, mode='min', factor=0.5, patience=5\n",
        "    )\n",
        "\n",
        "    best_val, wait = float('inf'), 0\n",
        "    history = {'train': [], 'val': []}\n",
        "\n",
        "    for epoch in range(1, epochs+1):\n",
        "        # Training step\n",
        "        model.train()\n",
        "        run_train = 0.0\n",
        "        for Xb, Yb in train_loader:\n",
        "            Xb, Yb = Xb.to(device), Yb.to(device)\n",
        "            pred = model(Xb)\n",
        "            loss, _, _ = total_loss(pred, Yb, Xb, stats, λ)\n",
        "            if torch.isnan(loss):\n",
        "                print(f\"NaN loss at epoch {epoch}, abort.\")\n",
        "                return history\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "            optimizer.step()\n",
        "            run_train += loss.item() * Xb.size(0)\n",
        "        train_loss = run_train / len(train_loader.dataset)\n",
        "\n",
        "        # Validation step\n",
        "        model.eval()\n",
        "        run_val = 0.0\n",
        "        with torch.no_grad():\n",
        "            for Xv, Yv in val_loader:\n",
        "                Xv, Yv = Xv.to(device), Yv.to(device)\n",
        "                pred = model(Xv)\n",
        "                l, _, _ = total_loss(pred, Yv, Xv, stats, λ)\n",
        "                run_val += l.item() * Xv.size(0)\n",
        "        val_loss = run_val / len(val_loader.dataset)\n",
        "\n",
        "        scheduler.step(val_loss)\n",
        "        history['train'].append(train_loss)\n",
        "        history['val'].append(val_loss)\n",
        "        print(f\"Epoch {epoch:03d} | Train {train_loss:.4e} | Val {val_loss:.4e}\")\n",
        "\n",
        "        # Checkpointing\n",
        "        if val_loss < best_val - 1e-6:\n",
        "            best_val, wait = val_loss, 0\n",
        "            torch.save(model.state_dict(), \"best_cosmic_ai.pth\")\n",
        "        else:\n",
        "            wait += 1\n",
        "            if wait >= patience:\n",
        "                print(\"Early stopping.\")\n",
        "                break\n",
        "\n",
        "    # Safe load best model\n",
        "    if os.path.exists(\"best_cosmic_ai.pth\"):\n",
        "        model.load_state_dict(torch.load(\"best_cosmic_ai.pth\", map_location=device))\n",
        "    return history\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 7. Visualization Helpers\n",
        "# ------------------------------------------------------------------------------\n",
        "def plot_history(history):\n",
        "    plt.figure()\n",
        "    plt.plot(history['train'], label='Train')\n",
        "    plt.plot(history['val'],   label='Val')\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "def plot_scatter(y_true, y_pred, title):\n",
        "    plt.figure()\n",
        "    plt.scatter(y_true, y_pred, s=6, alpha=0.5)\n",
        "    mn, mx = y_true.min(), y_true.max()\n",
        "    plt.plot([mn, mx], [mn, mx], 'r--')\n",
        "    plt.title(title)\n",
        "    plt.show()\n",
        "\n",
        "def plot_uncertainty_heatmap(model, stats, device):\n",
        "    G = 100\n",
        "    REX = np.linspace(0.1, 10.0, G, dtype=np.float32)\n",
        "    OP  = np.linspace(0.0, 1.0,  G, dtype=np.float32)\n",
        "    R, O = np.meshgrid(REX, OP)\n",
        "    pts = G * G\n",
        "\n",
        "    Xg = torch.zeros((pts, 6), device=device, dtype=torch.float32)\n",
        "    # columns 2–5 fixed at mean\n",
        "    Xg[:, 2:] = stats['X_mean'][2:].unsqueeze(0).expand(pts,4)\n",
        "    Xg[:, 0]  = torch.from_numpy(R.ravel()).to(device)\n",
        "    Xg[:, 1]  = torch.from_numpy(O.ravel()).to(device)\n",
        "\n",
        "    Xn = (Xg - stats['X_mean']) / stats['X_std']\n",
        "    _, std = mc_dropout_predict(model, Xn, T=40)\n",
        "    U = std[:, 0].cpu().reshape(G, G)\n",
        "\n",
        "    plt.figure(figsize=(5,4))\n",
        "    plt.pcolormesh(R, O, U, cmap='magma', shading='auto')\n",
        "    plt.colorbar(label=\"Std(TE)\")\n",
        "    plt.xlabel(\"Reality Control (RC)\")\n",
        "    plt.ylabel(\"Omniscience Prob (OP)\")\n",
        "    plt.title(\"Uncertainty: Transformation Entropy\")\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 8. Main Execution\n",
        "# ------------------------------------------------------------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # Prepare dataset and stats\n",
        "    ds = CosmicEntityDataset(n_samples=5000, seed=42)\n",
        "    stats = {\n",
        "        'X_mean': torch.tensor(ds.X_mean, dtype=torch.float32, device=device),\n",
        "        'X_std' : torch.tensor(ds.X_std,  dtype=torch.float32, device=device),\n",
        "        'Y_mean': torch.tensor(ds.Y_mean, dtype=torch.float32, device=device),\n",
        "        'Y_std' : torch.tensor(ds.Y_std,  dtype=torch.float32, device=device),\n",
        "    }\n",
        "\n",
        "    # Split into train/val\n",
        "    val_n = int(0.2 * len(ds))\n",
        "    tr_ds, va_ds = random_split(ds, [len(ds)-val_n, val_n])\n",
        "    tr_ld = DataLoader(tr_ds, batch_size=128, shuffle=True)\n",
        "    va_ld = DataLoader(va_ds, batch_size=256, shuffle=False)\n",
        "\n",
        "    # Build, train, evaluate\n",
        "    model   = CosmicEntityAI().to(device)\n",
        "    history = train(model, tr_ld, va_ld, stats, device)\n",
        "\n",
        "    # Visualize training history\n",
        "    plot_history(history)\n",
        "\n",
        "    # Scatter: true vs. predicted\n",
        "    X_all = torch.from_numpy(ds.X).to(device)\n",
        "    with torch.no_grad():\n",
        "        Yp_n = model(X_all).cpu().numpy()\n",
        "    Yt = ds.Y * ds.Y_std + ds.Y_mean\n",
        "    Yp = Yp_n * ds.Y_std + ds.Y_mean\n",
        "    names = [\"Transformation Entropy\", \"Omniversal Emergence\", \"Restructuring Power\"]\n",
        "    for i, nm in enumerate(names):\n",
        "        plot_scatter(Yt[:, i], Yp[:, i], nm)\n",
        "\n",
        "    # Plot uncertainty heatmap\n",
        "    plot_uncertainty_heatmap(model, stats, device)"
      ]
    }
  ]
}