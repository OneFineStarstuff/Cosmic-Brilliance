{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyOzEhHgLmLGPhBOgvzU5BRA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OneFineStarstuff/Cosmic-Brilliance/blob/main/ultimate_intelligence_pipeline_py.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3nwRbOHr9Mgw"
      },
      "outputs": [],
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "ultimate_intelligence_pipeline.py\n",
        "\n",
        "Complete pipeline for UltimateIntelligenceAI:\n",
        "1. Synthetic dataset generation\n",
        "2. Model definition with MC-Dropout\n",
        "3. Physics-informed regularization\n",
        "4. Combined loss (MSE + physics residual)\n",
        "5. MC-Dropout for uncertainty quantification\n",
        "6. Training loop with AdamW, LR scheduler, grad clipping, early stopping, checkpointing\n",
        "7. Visualizations: loss curves, scatter plots, uncertainty heatmap\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 1. Synthetic “Ultimate Intelligence” Dataset\n",
        "# ------------------------------------------------------------------------------\n",
        "class UltimateIntelligenceDataset(Dataset):\n",
        "    def __init__(self, n_samples=8000, seed=123):\n",
        "        np.random.seed(seed)\n",
        "        # Features:\n",
        "        # AIR: awareness recursion factor ∈ [0.1, 5.0]\n",
        "        # SGR: self-generation rate ∈ [0.01, 2.0]\n",
        "        # IER: infinite expansion ratio ∈ [0.5, 10.0]\n",
        "        # EAV: energy availability ∈ [1.0, 100.0]\n",
        "        # ENV: environmental complexity ∈ [0.1, 3.0]\n",
        "        # ADP: algorithmic adaptability ∈ [0.01, 1.0]\n",
        "        AIR = np.random.uniform(0.1, 5.0,    (n_samples,1))\n",
        "        SGR = np.random.uniform(0.01, 2.0,   (n_samples,1))\n",
        "        IER = np.random.uniform(0.5, 10.0,   (n_samples,1))\n",
        "        EAV = np.random.uniform(1.0, 100.0,  (n_samples,1))\n",
        "        ENV = np.random.uniform(0.1, 3.0,    (n_samples,1))\n",
        "        ADP = np.random.uniform(0.01, 1.0,   (n_samples,1))\n",
        "\n",
        "        X_raw = np.hstack([AIR, SGR, IER, EAV, ENV, ADP]).astype(np.float64)\n",
        "\n",
        "        # Targets:\n",
        "        # SAW: self-awareness growth\n",
        "        # SGR_t: adjusted self-generation\n",
        "        # CST: containment stability\n",
        "        eps   = 1e-8\n",
        "        SAW   = AIR * ADP / (ENV + eps)\n",
        "        SGR_t = SGR * np.sqrt(EAV)\n",
        "        CST   = np.log1p(IER) * ADP\n",
        "\n",
        "        Y_raw = np.hstack([SAW, SGR_t, CST]).astype(np.float64)\n",
        "        Y_raw += 0.02 * Y_raw.std(axis=0) * np.random.randn(*Y_raw.shape)\n",
        "\n",
        "        # Stats for normalization\n",
        "        self.X_mean = X_raw.mean(axis=0)\n",
        "        self.X_std  = X_raw.std(axis=0) + 1e-8\n",
        "        self.Y_mean = Y_raw.mean(axis=0)\n",
        "        self.Y_std  = Y_raw.std(axis=0) + 1e-8\n",
        "\n",
        "        # Normalize to float32\n",
        "        self.X = ((X_raw - self.X_mean) / self.X_std).astype(np.float32)\n",
        "        self.Y = ((Y_raw - self.Y_mean) / self.Y_std).astype(np.float32)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return torch.from_numpy(self.X[idx]), torch.from_numpy(self.Y[idx])\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 2. Model Definition with MC-Dropout\n",
        "# ------------------------------------------------------------------------------\n",
        "class UltimateIntelligenceAI(nn.Module):\n",
        "    def __init__(self, input_dim=6, hidden_dim=32, output_dim=3, p_drop=0.1):\n",
        "        super().__init__()\n",
        "        self.fc1  = nn.Linear(input_dim, hidden_dim)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.drop = nn.Dropout(p_drop)\n",
        "        self.fc2  = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.drop(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 3. Physics-Informed Residual\n",
        "# ------------------------------------------------------------------------------\n",
        "def physics_residual(pred, X, stats):\n",
        "    # Un-normalize inputs\n",
        "    X_den = X * stats['X_std'] + stats['X_mean']\n",
        "    AIR, SGR, IER, EAV, ENV, ADP = X_den.t()\n",
        "    eps = 1e-8\n",
        "\n",
        "    # Recompute targets\n",
        "    SAW_t   = AIR * ADP / (ENV + eps)\n",
        "    SGR_t   = SGR * torch.sqrt(EAV)\n",
        "    CST_t   = torch.log1p(IER) * ADP\n",
        "\n",
        "    Yt      = torch.stack([SAW_t, SGR_t, CST_t], dim=1)\n",
        "    Yt_norm = (Yt - stats['Y_mean']) / stats['Y_std']\n",
        "    return nn.MSELoss()(pred, Yt_norm)\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 4. Combined Loss Function\n",
        "# ------------------------------------------------------------------------------\n",
        "def total_loss(pred, true, X, stats, lambda_phys=1.0):\n",
        "    mse  = nn.MSELoss()(pred, true)\n",
        "    phys = physics_residual(pred, X, stats)\n",
        "    return mse + lambda_phys * phys, mse, phys\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 5. MC-Dropout Uncertainty\n",
        "# ------------------------------------------------------------------------------\n",
        "def mc_dropout_predict(model, X, T=50):\n",
        "    model.train()  # enable dropout\n",
        "    preds = []\n",
        "    with torch.no_grad():\n",
        "        for _ in range(T):\n",
        "            preds.append(model(X))\n",
        "    arr = torch.stack(preds, dim=0)\n",
        "    return arr.mean(dim=0), arr.std(dim=0)\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 6. Training Loop & Checkpointing\n",
        "# ------------------------------------------------------------------------------\n",
        "def train(model, train_loader, val_loader, stats, device,\n",
        "          lr=1e-4, wd=1e-5, lambda_phys=1.0,\n",
        "          epochs=100, patience=10):\n",
        "    model.to(device)\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=wd)\n",
        "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
        "        optimizer, mode='min', factor=0.5, patience=4\n",
        "    )\n",
        "\n",
        "    best_val, wait = float('inf'), 0\n",
        "    history = {'train_loss': [], 'val_loss': []}\n",
        "\n",
        "    for epoch in range(1, epochs+1):\n",
        "        # Training\n",
        "        model.train()\n",
        "        running_tr = 0.0\n",
        "        for Xb, Yb in train_loader:\n",
        "            Xb, Yb = Xb.to(device), Yb.to(device)\n",
        "            pred = model(Xb)\n",
        "            loss, _, _ = total_loss(pred, Yb, Xb, stats, lambda_phys)\n",
        "            if torch.isnan(loss):\n",
        "                raise ValueError(f\"NaN detected at epoch {epoch}\")\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            optimizer.step()\n",
        "            running_tr += loss.item() * Xb.size(0)\n",
        "        train_loss = running_tr / len(train_loader.dataset)\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        running_val = 0.0\n",
        "        with torch.no_grad():\n",
        "            for Xb, Yb in val_loader:\n",
        "                Xb, Yb = Xb.to(device), Yb.to(device)\n",
        "                pred = model(Xb)\n",
        "                loss, _, _ = total_loss(pred, Yb, Xb, stats, lambda_phys)\n",
        "                running_val += loss.item() * Xb.size(0)\n",
        "        val_loss = running_val / len(val_loader.dataset)\n",
        "\n",
        "        scheduler.step(val_loss)\n",
        "        history['train_loss'].append(train_loss)\n",
        "        history['val_loss'].append(val_loss)\n",
        "        print(f\"Epoch {epoch:03d} | Train {train_loss:.4e} | Val {val_loss:.4e}\")\n",
        "\n",
        "        # Early stopping & checkpoint\n",
        "        if val_loss < best_val - 1e-6:\n",
        "            best_val, wait = val_loss, 0\n",
        "            torch.save(model.state_dict(), \"best_ultimate_ai.pth\")\n",
        "        else:\n",
        "            wait += 1\n",
        "            if wait >= patience:\n",
        "                print(\"Early stopping triggered.\")\n",
        "                break\n",
        "\n",
        "    # Load best model\n",
        "    model.load_state_dict(torch.load(\"best_ultimate_ai.pth\", map_location=device))\n",
        "    return history\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 7. Visualizations\n",
        "# ------------------------------------------------------------------------------\n",
        "def plot_history(history):\n",
        "    plt.figure()\n",
        "    plt.plot(history['train_loss'], label=\"Train Loss\")\n",
        "    plt.plot(history['val_loss'],   label=\"Val Loss\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "def plot_scatter(true, pred, title):\n",
        "    plt.figure()\n",
        "    plt.scatter(true, pred, s=4, alpha=0.6)\n",
        "    mn, mx = true.min(), true.max()\n",
        "    plt.plot([mn, mx], [mn, mx], 'r--')\n",
        "    plt.title(title)\n",
        "    plt.show()\n",
        "\n",
        "def plot_uncertainty(model, stats, device):\n",
        "    G = 100\n",
        "    AIR = np.linspace(0.1, 5.0, G, dtype=np.float32)\n",
        "    ADP = np.linspace(0.01,1.0, G, dtype=np.float32)\n",
        "    G1, G2 = np.meshgrid(AIR, ADP)\n",
        "    pts = G * G\n",
        "\n",
        "    Xg = torch.zeros((pts,6), device=device)\n",
        "    # fix SGR, IER, EAV, ENV at mean\n",
        "    for i in [1,2,3,4]:\n",
        "        Xg[:, i] = stats['X_mean'][i]\n",
        "    Xg[:,0] = torch.from_numpy(G1.ravel()).to(device)\n",
        "    Xg[:,5] = torch.from_numpy(G2.ravel()).to(device)\n",
        "\n",
        "    Xn = (Xg - stats['X_mean']) / stats['X_std']\n",
        "    _, std = mc_dropout_predict(model, Xn, T=40)\n",
        "    U = std[:,0].cpu().reshape(G1.shape)\n",
        "\n",
        "    plt.figure(figsize=(5,4))\n",
        "    plt.pcolormesh(AIR, ADP, U, shading='auto', cmap='viridis')\n",
        "    plt.colorbar(label=\"Std(Self-awareness Growth)\")\n",
        "    plt.xlabel(\"Awareness Recursion (AIR)\")\n",
        "    plt.ylabel(\"Adaptability (ADP)\")\n",
        "    plt.title(\"Uncertainty Heatmap\")\n",
        "    plt.show()\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 8. Main Execution\n",
        "# ------------------------------------------------------------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    # Device\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # Dataset & stats\n",
        "    dataset = UltimateIntelligenceDataset(n_samples=8000, seed=123)\n",
        "    stats = {\n",
        "        'X_mean': torch.tensor(dataset.X_mean, dtype=torch.float32, device=device),\n",
        "        'X_std' : torch.tensor(dataset.X_std,  dtype=torch.float32, device=device),\n",
        "        'Y_mean': torch.tensor(dataset.Y_mean, dtype=torch.float32, device=device),\n",
        "        'Y_std' : torch.tensor(dataset.Y_std,  dtype=torch.float32, device=device),\n",
        "    }\n",
        "\n",
        "    # Train/Val split\n",
        "    n_val = int(0.2 * len(dataset))\n",
        "    tr_ds, va_ds = random_split(dataset, [len(dataset)-n_val, n_val])\n",
        "    tr_loader = DataLoader(tr_ds, batch_size=128, shuffle=True)\n",
        "    va_loader = DataLoader(va_ds, batch_size=256, shuffle=False)\n",
        "\n",
        "    # Model, train, visualize\n",
        "    model   = UltimateIntelligenceAI().to(device)\n",
        "    history = train(model, tr_loader, va_loader, stats, device,\n",
        "                    lr=1e-4, wd=1e-5, lambda_phys=1.0,\n",
        "                    epochs=80, patience=10)\n",
        "\n",
        "    plot_history(history)\n",
        "\n",
        "    # Scatter plots\n",
        "    X_all = torch.from_numpy(dataset.X).to(device)\n",
        "    with torch.no_grad():\n",
        "        Y_pred_n = model(X_all).cpu().numpy()\n",
        "    Y_true = dataset.Y * dataset.Y_std + dataset.Y_mean\n",
        "    Y_pred = Y_pred_n * dataset.Y_std + dataset.Y_mean\n",
        "\n",
        "    names = [\"Self-awareness Growth\", \"Self-generation Rate\", \"Containment Stability\"]\n",
        "    for idx, name in enumerate(names):\n",
        "        plot_scatter(Y_true[:,idx], Y_pred[:,idx], name)\n",
        "\n",
        "    # Uncertainty heatmap\n",
        "    plot_uncertainty(model, stats, device)"
      ]
    }
  ]
}