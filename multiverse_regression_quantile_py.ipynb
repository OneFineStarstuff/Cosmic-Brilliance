{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyMf29ufcS7TKpmjUfdKjLhv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OneFineStarstuff/Cosmic-Brilliance/blob/main/multiverse_regression_quantile_py.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ASGLfl4BBVwp"
      },
      "outputs": [],
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "multiverse_regression_quantile.py\n",
        "\n",
        "1. Generate synthetic 5-D compactification parameters and continuous scores.\n",
        "2. Expand features: sin, cos, and pairwise sin-sin interactions.\n",
        "3. Train an MLP regressor with MSE loss, early stopping, and LR scheduling.\n",
        "4. Evaluate continuous MSE and discretize predictions using quantile bins.\n",
        "5. Report binned accuracy, classification report, and confusion matrix.\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import (\n",
        "    mean_squared_error,\n",
        "    accuracy_score,\n",
        "    classification_report,\n",
        "    confusion_matrix\n",
        ")\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "# 1. Data Generation\n",
        "def generate_data(n_samples=15000, input_dim=5, seed=42):\n",
        "    np.random.seed(seed)\n",
        "    # Uniform angles [0, 2π]\n",
        "    params = np.random.uniform(0, 2 * np.pi, size=(n_samples, input_dim)).astype(np.float32)\n",
        "    # Continuous target: sum of sines\n",
        "    scores = np.sum(np.sin(params), axis=1).astype(np.float32)\n",
        "    return params, scores\n",
        "\n",
        "# 2. Feature Expansion\n",
        "def expand_features(params):\n",
        "    \"\"\"\n",
        "    From original params (n,5) build:\n",
        "      - sin(param_i), cos(param_i)  → 10 features\n",
        "      - sin(param_i) * sin(param_j) for i<j → 10 features\n",
        "    Total expanded_dim = 20\n",
        "    \"\"\"\n",
        "    n, d = params.shape\n",
        "    feats = []\n",
        "    # sin and cos\n",
        "    for i in range(d):\n",
        "        feats.append(np.sin(params[:, i]))\n",
        "        feats.append(np.cos(params[:, i]))\n",
        "    # pairwise sin* sin\n",
        "    for i in range(d):\n",
        "        for j in range(i+1, d):\n",
        "            feats.append(np.sin(params[:, i]) * np.sin(params[:, j]))\n",
        "    return np.stack(feats, axis=1).astype(np.float32)\n",
        "\n",
        "# 3. Regression Model\n",
        "class MultiverseRegressor(nn.Module):\n",
        "    def __init__(self, input_dim=20, hidden_dims=[64,32], drop_p=0.3):\n",
        "        super().__init__()\n",
        "        layers = []\n",
        "        dims = [input_dim] + hidden_dims + [1]\n",
        "        for i in range(len(dims) - 1):\n",
        "            layers.append(nn.Linear(dims[i], dims[i+1]))\n",
        "            if i < len(dims) - 2:\n",
        "                layers.append(nn.ReLU())\n",
        "                layers.append(nn.Dropout(drop_p))\n",
        "        self.net = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x).squeeze(-1)\n",
        "\n",
        "# 4. Training Loop with Early Stopping\n",
        "def train_regressor(\n",
        "    model, train_loader, val_loader,\n",
        "    epochs=100, lr=1e-3, patience=10, device=None\n",
        "):\n",
        "    device = device or torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=1e-5)\n",
        "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
        "        optimizer, mode=\"min\", factor=0.5, patience=5\n",
        "    )\n",
        "    criterion = nn.MSELoss()\n",
        "\n",
        "    best_val = float(\"inf\")\n",
        "    no_improve = 0\n",
        "\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        # Training phase\n",
        "        model.train()\n",
        "        train_losses = []\n",
        "        for xb, yb in train_loader:\n",
        "            xb, yb = xb.to(device), yb.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            preds = model(xb)\n",
        "            loss = criterion(preds, yb)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            train_losses.append(loss.item())\n",
        "\n",
        "        # Validation phase\n",
        "        model.eval()\n",
        "        val_losses = []\n",
        "        with torch.no_grad():\n",
        "            for xb, yb in val_loader:\n",
        "                xb, yb = xb.to(device), yb.to(device)\n",
        "                val_losses.append(criterion(model(xb), yb).item())\n",
        "\n",
        "        avg_train = np.mean(train_losses)\n",
        "        avg_val   = np.mean(val_losses)\n",
        "        scheduler.step(avg_val)\n",
        "\n",
        "        if avg_val < best_val:\n",
        "            best_val = avg_val\n",
        "            no_improve = 0\n",
        "            torch.save(model.state_dict(), \"best_regressor.pth\")\n",
        "        else:\n",
        "            no_improve += 1\n",
        "\n",
        "        if epoch == 1 or epoch % 10 == 0:\n",
        "            print(f\"Epoch {epoch:03d} ─ Train MSE: {avg_train:.6f} ─ Val MSE: {avg_val:.6f}\")\n",
        "\n",
        "        if no_improve >= patience:\n",
        "            print(f\"Early stopping at epoch {epoch}. Best Val MSE: {best_val:.6f}\")\n",
        "            break\n",
        "\n",
        "# 5. Main Execution\n",
        "if __name__ == \"__main__\":\n",
        "    # Hyperparameters\n",
        "    ORIG_DIM    = 5\n",
        "    EXP_DIM     = 20\n",
        "    N_SAMPLES   = 15000\n",
        "    TEST_SIZE   = 0.20\n",
        "    BATCH_SIZE  = 256\n",
        "    EPOCHS      = 100\n",
        "    LEARNING_RT = 1e-3\n",
        "    PATIENCE    = 10\n",
        "    N_BINS      = 10\n",
        "\n",
        "    # Generate raw data\n",
        "    params, scores = generate_data(N_SAMPLES, ORIG_DIM)\n",
        "\n",
        "    # Expand features\n",
        "    X = expand_features(params)\n",
        "\n",
        "    # Train/validation split\n",
        "    X_tr, X_val, y_tr, y_val = train_test_split(\n",
        "        X, scores, test_size=TEST_SIZE, random_state=42\n",
        "    )\n",
        "\n",
        "    # DataLoaders\n",
        "    train_ds = TensorDataset(torch.from_numpy(X_tr), torch.from_numpy(y_tr))\n",
        "    val_ds   = TensorDataset(torch.from_numpy(X_val), torch.from_numpy(y_val))\n",
        "    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
        "    val_loader   = DataLoader(val_ds, batch_size=BATCH_SIZE)\n",
        "\n",
        "    # Initialize and train model\n",
        "    model = MultiverseRegressor(input_dim=EXP_DIM, hidden_dims=[64,32], drop_p=0.3)\n",
        "    train_regressor(model, train_loader, val_loader,\n",
        "                    epochs=EPOCHS, lr=LEARNING_RT, patience=PATIENCE)\n",
        "\n",
        "    # Load best model\n",
        "    model.load_state_dict(torch.load(\"best_regressor.pth\"))\n",
        "    model.eval()\n",
        "\n",
        "    # Predictions\n",
        "    with torch.no_grad():\n",
        "        y_pred_tr  = model(torch.from_numpy(X_tr)).numpy()\n",
        "        y_pred_val = model(torch.from_numpy(X_val)).numpy()\n",
        "\n",
        "    # Continuous evaluation\n",
        "    mse_tr  = mean_squared_error(y_tr, y_pred_tr)\n",
        "    mse_val = mean_squared_error(y_val, y_pred_val)\n",
        "    print(f\"\\nFinal MSE ─ Train: {mse_tr:.6f} ─ Val: {mse_val:.6f}\")\n",
        "\n",
        "    # Quantile-based binning\n",
        "    edges     = np.quantile(y_tr, np.linspace(0,1,N_BINS+1))\n",
        "    true_bin  = np.digitize(y_val, edges, right=True) - 1\n",
        "    pred_bin  = np.digitize(y_pred_val, edges, right=True) - 1\n",
        "\n",
        "    # Classification metrics\n",
        "    acc = accuracy_score(true_bin, pred_bin)\n",
        "    print(f\"\\nBinned Accuracy: {acc:.3f}\\n\")\n",
        "    print(\"Classification Report:\\n\", classification_report(true_bin, pred_bin, digits=4))\n",
        "    print(\"Confusion Matrix:\\n\", confusion_matrix(true_bin, pred_bin))"
      ]
    }
  ]
}