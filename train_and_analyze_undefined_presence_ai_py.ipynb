{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyNMiegSRoOtMPsuuG3yMN/f",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OneFineStarstuff/Cosmic-Brilliance/blob/main/train_and_analyze_undefined_presence_ai_py.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eDZBFkmFniR5"
      },
      "outputs": [],
      "source": [
        "pip install torch numpy matplotlib seaborn umap-learn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "train_and_analyze_undefined_presence_ai.py\n",
        "\n",
        "1. Synthetic dataset (6 → 3)\n",
        "2. MC‐Dropout model\n",
        "3. Physics‐ and ODE‐informed residual losses\n",
        "4. AdamW training with scheduler, clipping, early stop\n",
        "5. MC‐Dropout inference (mean/std)\n",
        "6. Loss curves, OOD calibration, reliability diagram\n",
        "7. UMAP embedding colored by true presence\n",
        "8. Physics residual histogram\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from torch.autograd import grad\n",
        "import umap\n",
        "\n",
        "# 1. Reproducibility & Device\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# 2. Dataset Definition\n",
        "class UndefinedPresenceDataset(Dataset):\n",
        "    def __init__(self, n=5000):\n",
        "        u = np.random.uniform(-1, 1, (n, 1))\n",
        "        v = np.random.uniform(0, 2, (n, 1))\n",
        "        w = np.random.uniform(-2, 2, (n, 1))\n",
        "        x = np.random.uniform(0, 5, (n, 1))\n",
        "        y = np.random.uniform(-1, 1, (n, 1))\n",
        "        z = np.random.uniform(0, 1, (n, 1))\n",
        "        X = np.hstack([u, v, w, x, y, z]).astype(np.float32)\n",
        "\n",
        "        # Physics definitions\n",
        "        presence = np.sin(u) * v + np.cos(w)\n",
        "        dissolution = np.exp(-x * y)\n",
        "        transcendence = z * (presence + dissolution)\n",
        "        Y = np.hstack([presence, dissolution, transcendence]).astype(np.float32)\n",
        "        Y += 0.01 * Y.std(axis=0) * np.random.randn(*Y.shape).astype(np.float32)\n",
        "\n",
        "        # Normalization stats\n",
        "        self.X_mean, self.X_std = X.mean(0), X.std(0) + 1e-8\n",
        "        self.Y_mean, self.Y_std = Y.mean(0), Y.std(0) + 1e-8\n",
        "\n",
        "        # Standardize\n",
        "        self.X = ((X - self.X_mean) / self.X_std).astype(np.float32)\n",
        "        self.Y = ((Y - self.Y_mean) / self.Y_std).astype(np.float32)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        return torch.from_numpy(self.X[i]), torch.from_numpy(self.Y[i])\n",
        "\n",
        "# 3. MC‐Dropout Model\n",
        "class UndefinedPresenceAI(nn.Module):\n",
        "    def __init__(self, inp=6, hid=32, out=3, p_drop=0.1):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(inp, hid),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(p_drop),\n",
        "            nn.Linear(hid, out)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "# 4a. Physics residual\n",
        "def physics_residual(pred, X, stats):\n",
        "    X_den = X * stats['X_std'] + stats['X_mean']\n",
        "    u, v, w, x, y, z = X_den.T\n",
        "    presence = torch.sin(u) * v + torch.cos(w)\n",
        "    dissolution = torch.exp(-x * y)\n",
        "    transc = z * (presence + dissolution)\n",
        "    Y_phys = torch.stack([presence, dissolution, transc], dim=1)\n",
        "    Yn = (Y_phys - stats['Y_mean']) / stats['Y_std']\n",
        "    return nn.MSELoss()(pred, Yn)\n",
        "\n",
        "# 4b. ODE‐informed residual (∂presence/∂u ≈ v⋅cos(u))\n",
        "def ode_residual(pred, X, stats):\n",
        "    # denormalize output\n",
        "    pred_den = pred * stats['Y_std'] + stats['Y_mean']\n",
        "    pres = pred_den[:, 0]\n",
        "\n",
        "    # differentiate presence w.r.t. input X\n",
        "    grads = grad(pres.sum(), X, create_graph=True)[0]\n",
        "    dp_du = grads[:, 0]\n",
        "\n",
        "    # compute analytic target\n",
        "    X_den = X * stats['X_std'] + stats['X_mean']\n",
        "    u = X_den[:, 0]\n",
        "    v = X_den[:, 1]\n",
        "    target = v * torch.cos(u)\n",
        "\n",
        "    return nn.MSELoss()(dp_du, target)\n",
        "\n",
        "# 4c. Combined loss\n",
        "def total_loss(pred, y_true, X, stats, lam_phys=1.0, lam_ode=0.5):\n",
        "    mse = nn.MSELoss()(pred, y_true)\n",
        "    phys = physics_residual(pred, X, stats)\n",
        "    ode = ode_residual(pred, X, stats)\n",
        "    return mse + lam_phys * phys + lam_ode * ode, mse, phys, ode\n",
        "\n",
        "# 5. MC‐Dropout Prediction\n",
        "def mc_predict(model, X, T=50):\n",
        "    model.train()               # keep dropout on\n",
        "    preds = []\n",
        "    with torch.no_grad():\n",
        "        for _ in range(T):\n",
        "            preds.append(model(X))\n",
        "    stacked = torch.stack(preds)\n",
        "    return stacked.mean(0), stacked.std(0)\n",
        "\n",
        "# 6. Training Loop\n",
        "def train(model, dl_tr, dl_va, stats,\n",
        "          epochs=100, lr=1e-3, wd=1e-5, patience=10):\n",
        "    model.to(DEVICE)\n",
        "    opt = optim.AdamW(model.parameters(), lr=lr, weight_decay=wd)\n",
        "    sch = optim.lr_scheduler.ReduceLROnPlateau(opt, 'min', factor=0.5, patience=5)\n",
        "    best_val = float('inf')\n",
        "    wait = 0\n",
        "    history = {'train_loss': [], 'val_loss': []}\n",
        "\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        # ——— Training ———\n",
        "        model.train()\n",
        "        running_tr = 0.0\n",
        "        for Xb, Yb in dl_tr:\n",
        "            # ensure Xb requires grad for ode_residual\n",
        "            Xb = Xb.to(DEVICE).clone().detach().requires_grad_(True)\n",
        "            Yb = Yb.to(DEVICE)\n",
        "\n",
        "            pred = model(Xb)\n",
        "            loss, _, _, _ = total_loss(pred, Yb, Xb, stats)\n",
        "            opt.zero_grad()\n",
        "            loss.backward()\n",
        "            nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "            opt.step()\n",
        "\n",
        "            running_tr += loss.item() * Xb.size(0)\n",
        "\n",
        "        train_loss = running_tr / len(dl_tr.dataset)\n",
        "        history['train_loss'].append(train_loss)\n",
        "\n",
        "        # ——— Validation ———\n",
        "        model.eval()\n",
        "        running_va = 0.0\n",
        "        # compute residuals (we need grad w.r.t X here too)\n",
        "        for Xv, Yv in dl_va:\n",
        "            Xv = Xv.to(DEVICE).clone().detach().requires_grad_(True)\n",
        "            Yv = Yv.to(DEVICE)\n",
        "            pred = model(Xv)\n",
        "            loss, _, _, _ = total_loss(pred, Yv, Xv, stats)\n",
        "            running_va += loss.item() * Xv.size(0)\n",
        "\n",
        "        val_loss = running_va / len(dl_va.dataset)\n",
        "        history['val_loss'].append(val_loss)\n",
        "\n",
        "        sch.step(val_loss)\n",
        "        print(f\"Epoch {epoch:3d} | Train {train_loss:.4f} | Val {val_loss:.4f}\")\n",
        "\n",
        "        if val_loss < best_val - 1e-6:\n",
        "            best_val, wait = val_loss, 0\n",
        "            torch.save(model.state_dict(), \"best_model.pth\")\n",
        "        else:\n",
        "            wait += 1\n",
        "            if wait >= patience:\n",
        "                print(\"Early stopping.\")\n",
        "                break\n",
        "\n",
        "    model.load_state_dict(torch.load(\"best_model.pth\"))\n",
        "    return history, model\n",
        "\n",
        "# 7. Main Execution & Analysis\n",
        "if __name__ == \"__main__\":\n",
        "    ds = UndefinedPresenceDataset()\n",
        "    n_val = int(0.2 * len(ds))\n",
        "    ds_tr, ds_va = random_split(ds, [len(ds) - n_val, n_val])\n",
        "    dl_tr = DataLoader(ds_tr, batch_size=128, shuffle=True)\n",
        "    dl_va = DataLoader(ds_va, batch_size=256)\n",
        "\n",
        "    stats = {\n",
        "        'X_mean': torch.tensor(ds.X_mean, device=DEVICE),\n",
        "        'X_std':  torch.tensor(ds.X_std,  device=DEVICE),\n",
        "        'Y_mean': torch.tensor(ds.Y_mean, device=DEVICE),\n",
        "        'Y_std':  torch.tensor(ds.Y_std,  device=DEVICE),\n",
        "    }\n",
        "\n",
        "    model = UndefinedPresenceAI().to(DEVICE)\n",
        "    history, model = train(model, dl_tr, dl_va, stats)\n",
        "\n",
        "    # Save losses\n",
        "    np.savez(\"training_history.npz\",\n",
        "             train_loss=np.array(history['train_loss']),\n",
        "             val_loss=np.array(history['val_loss']))\n",
        "\n",
        "    # In‐distribution MC‐Dropout\n",
        "    X_all = torch.from_numpy(ds.X).to(DEVICE)\n",
        "    mean_pred, std_pred = mc_predict(model, X_all)\n",
        "\n",
        "    # 8. Plotting\n",
        "    os.makedirs(\"plots\", exist_ok=True)\n",
        "\n",
        "    # 8.1 Loss curves\n",
        "    plt.figure()\n",
        "    plt.plot(history['train_loss'], label=\"Train Loss\")\n",
        "    plt.plot(history['val_loss'],   label=\"Val Loss\")\n",
        "    plt.xlabel(\"Epoch\"); plt.ylabel(\"Loss\"); plt.legend()\n",
        "    plt.title(\"Training & Validation Loss\")\n",
        "    plt.savefig(\"plots/loss_curves.png\", dpi=150)\n",
        "\n",
        "    # 8.2 OOD Uncertainty\n",
        "    def sample_ood(n=2000):\n",
        "        u = np.random.uniform(-2,2,(n,1))\n",
        "        v = np.random.uniform(0,2,(n,1))\n",
        "        w = np.random.uniform(-2,2,(n,1))\n",
        "        x = np.random.uniform(0,5,(n,1))\n",
        "        y = np.random.uniform(-1,1,(n,1))\n",
        "        z = np.random.uniform(0,1,(n,1))\n",
        "        X = np.hstack([u,v,w,x,y,z]).astype(np.float32)\n",
        "        X_std = (X - ds.X_mean) / ds.X_std\n",
        "        return torch.from_numpy(X_std).to(DEVICE)\n",
        "\n",
        "    X_ood = sample_ood()\n",
        "    _, std_ood = mc_predict(model, X_ood)\n",
        "    plt.figure()\n",
        "    sns.kdeplot(std_ood[:,0].cpu(), label=\"Presence STD\")\n",
        "    sns.kdeplot(std_ood[:,1].cpu(), label=\"Dissolution STD\")\n",
        "    sns.kdeplot(std_ood[:,2].cpu(), label=\"Transcendence STD\")\n",
        "    plt.title(\"OOD Uncertainty Distributions\")\n",
        "    plt.legend()\n",
        "    plt.savefig(\"plots/ood_uncertainty.png\", dpi=150)\n",
        "\n",
        "    # 8.3 Reliability Diagram\n",
        "    errors = (mean_pred - torch.from_numpy(ds.Y).to(DEVICE)).abs().cpu().numpy()\n",
        "    stds   = std_pred.cpu().numpy()\n",
        "\n",
        "    def reliability_plot(err, std, label):\n",
        "        bins = np.linspace(std.min(), std.max(), 10)\n",
        "        ids = np.digitize(std, bins) - 1\n",
        "        avg_err, avg_std = [], []\n",
        "        for i in range(len(bins)):\n",
        "            mask = ids == i\n",
        "            if mask.sum() > 0:\n",
        "                avg_err.append(err[mask].mean())\n",
        "                avg_std.append(std[mask].mean())\n",
        "        plt.plot(avg_std, avg_err, '-o', label=label)\n",
        "\n",
        "    plt.figure()\n",
        "    reliability_plot(errors[:,0], stds[:,0], \"Presence\")\n",
        "    reliability_plot(errors[:,1], stds[:,1], \"Dissolution\")\n",
        "    reliability_plot(errors[:,2], stds[:,2], \"Transcendence\")\n",
        "    plt.xlabel(\"Avg Pred STD\"); plt.ylabel(\"Avg Abs Error\")\n",
        "    plt.title(\"Reliability Diagram\")\n",
        "    plt.legend()\n",
        "    plt.savefig(\"plots/reliability.png\", dpi=150)\n",
        "\n",
        "    # 8.4 UMAP\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        feats = model.net[0](X_all).cpu().numpy()\n",
        "    emb = umap.UMAP(n_components=2, random_state=SEED).fit_transform(feats)\n",
        "    raw = ds.X * ds.X_std + ds.X_mean\n",
        "    u, v, w = raw[:,0], raw[:,1], raw[:,2]\n",
        "    true_presence = np.sin(u)*v + np.cos(w)\n",
        "    plt.figure(figsize=(6,5))\n",
        "    sc = plt.scatter(emb[:,0], emb[:,1], c=true_presence, cmap=\"coolwarm\", s=4)\n",
        "    plt.colorbar(sc, label=\"True Presence\")\n",
        "    plt.title(\"UMAP of Hidden Features\")\n",
        "    plt.savefig(\"plots/umap_presence.png\", dpi=150)\n",
        "\n",
        "    # 8.5 Physics Residual Histogram\n",
        "    with torch.no_grad():\n",
        "        X_den = X_all * stats['X_std'] + stats['X_mean']\n",
        "        u, v, w, x, y, z = X_den.T\n",
        "        phys_pres = torch.sin(u)*v + torch.cos(w)\n",
        "        phys_dis  = torch.exp(-x*y)\n",
        "        phys_tr   = z*(phys_pres+phys_dis)\n",
        "        Y_phys = torch.stack([phys_pres, phys_dis, phys_tr], dim=1).cpu().numpy()\n",
        "\n",
        "    pred_den = mean_pred.cpu().numpy() * ds.Y_std + ds.Y_mean\n",
        "    residuals = ((pred_den - Y_phys)**2).mean(axis=1)\n",
        "    plt.figure()\n",
        "    sns.histplot(residuals, bins=50, kde=True)\n",
        "    plt.title(\"Physics Residual (MSE) Histogram\")\n",
        "    plt.xlabel(\"Residual\"); plt.ylabel(\"Count\")\n",
        "    plt.savefig(\"plots/physics_residual_hist.png\", dpi=150)\n",
        "\n",
        "    print(\"Done. Plots saved in ./plots/\")"
      ],
      "metadata": {
        "id": "pPm_-r7On10x"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}