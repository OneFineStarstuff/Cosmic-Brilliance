{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyNRJEAEafAGogAvWJyTtsGB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OneFineStarstuff/Cosmic-Brilliance/blob/main/MetaIntelligence_end_to_end_toolkit_(notebook_Colab_safe).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4gDEBPF705MV"
      },
      "outputs": [],
      "source": [
        "#!/usr/bin/env python3\n",
        "# MetaIntelligence end-to-end toolkit (notebook/Colab-safe)\n",
        "# - Train (supervised/selfsup/hybrid)\n",
        "# - Eval on fresh synthetic data\n",
        "# - Predict on points/CSV\n",
        "# - Benchmark grid across modes/datasets/seeds\n",
        "# - Export ONNX\n",
        "# Safe for notebooks: main([]) prints help; plotting uses Agg backend\n",
        "\n",
        "import argparse\n",
        "from typing import Optional, List, Tuple, Dict, Any\n",
        "import os\n",
        "import sys\n",
        "import json\n",
        "import time\n",
        "import random\n",
        "from dataclasses import dataclass\n",
        "import numpy as np\n",
        "\n",
        "# Safe headless plotting\n",
        "import matplotlib\n",
        "matplotlib.use(\"Agg\")\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.datasets import make_moons, make_circles, make_blobs, make_classification\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "\n",
        "# ----------------------------\n",
        "# Utilities\n",
        "# ----------------------------\n",
        "\n",
        "def set_seed(seed: int) -> None:\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "\n",
        "def get_device(force_cpu: bool = False) -> torch.device:\n",
        "    if force_cpu:\n",
        "        return torch.device(\"cpu\")\n",
        "    return torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "def ensure_dir(path: str) -> None:\n",
        "    if path and not os.path.exists(path):\n",
        "        os.makedirs(path, exist_ok=True)\n",
        "\n",
        "\n",
        "def now_ts() -> str:\n",
        "    return time.strftime(\"%Y%m%d-%H%M%S\")\n",
        "\n",
        "\n",
        "def sanitize_filename(name: str) -> str:\n",
        "    return \"\".join(ch for ch in name if ch.isalnum() or ch in \"-_.\")\n",
        "\n",
        "\n",
        "# ----------------------------\n",
        "# Data generation\n",
        "# ----------------------------\n",
        "\n",
        "@dataclass\n",
        "class DataConfig:\n",
        "    dataset: str\n",
        "    dims: int\n",
        "    classes: int\n",
        "    samples: int\n",
        "    noise: float\n",
        "    valfrac: float\n",
        "    seed: int\n",
        "\n",
        "\n",
        "def gen_synthetic(cfg: DataConfig) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
        "    rng = np.random.RandomState(cfg.seed)\n",
        "    n_train = int((1.0 - cfg.valfrac) * cfg.samples)\n",
        "    n_val = cfg.samples - n_train\n",
        "\n",
        "    if cfg.dataset == \"moons\":\n",
        "        if cfg.dims != 2:\n",
        "            raise ValueError(\"moons requires --dims=2\")\n",
        "        X, y = make_moons(n_samples=cfg.samples, noise=cfg.noise, random_state=cfg.seed)\n",
        "    elif cfg.dataset == \"circles\":\n",
        "        if cfg.dims != 2:\n",
        "            raise ValueError(\"circles requires --dims=2\")\n",
        "        X, y = make_circles(n_samples=cfg.samples, noise=cfg.noise, factor=0.5, random_state=cfg.seed)\n",
        "    elif cfg.dataset == \"blobs\":\n",
        "        X, y = make_blobs(\n",
        "            n_samples=cfg.samples, centers=cfg.classes, n_features=cfg.dims,\n",
        "            cluster_std=max(cfg.noise, 0.05), random_state=cfg.seed\n",
        "        )\n",
        "    elif cfg.dataset == \"classification\":\n",
        "        X, y = make_classification(\n",
        "            n_samples=cfg.samples,\n",
        "            n_features=cfg.dims,\n",
        "            n_informative=max(2, min(cfg.dims, cfg.dims - 0)),\n",
        "            n_redundant=0,\n",
        "            n_repeated=0,\n",
        "            n_classes=cfg.classes,\n",
        "            n_clusters_per_class=1,\n",
        "            flip_y=min(cfg.noise, 0.4),\n",
        "            class_sep=max(0.5, 2.0 - cfg.noise),\n",
        "            random_state=cfg.seed,\n",
        "        )\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown dataset: {cfg.dataset}\")\n",
        "\n",
        "    # Shuffle and split\n",
        "    idx = np.arange(cfg.samples)\n",
        "    rng.shuffle(idx)\n",
        "    X = X[idx]\n",
        "    y = y[idx]\n",
        "    X_train, y_train = X[:n_train], y[:n_train]\n",
        "    X_val, y_val = X[n_train:], y[n_train:]\n",
        "\n",
        "    # Standardize features for stability\n",
        "    mu = X_train.mean(axis=0, keepdims=True)\n",
        "    sigma = X_train.std(axis=0, keepdims=True) + 1e-8\n",
        "    X_train = (X_train - mu) / sigma\n",
        "    X_val = (X_val - mu) / sigma\n",
        "\n",
        "    return X_train.astype(np.float32), y_train.astype(np.int64), X_val.astype(np.float32), y_val.astype(np.int64)\n",
        "\n",
        "\n",
        "def make_loaders(\n",
        "    X_train: np.ndarray, y_train: np.ndarray,\n",
        "    X_val: np.ndarray, y_val: np.ndarray,\n",
        "    batch_size: int,\n",
        "    labeled_frac: float = 1.0,\n",
        "    seed: int = 42\n",
        ") -> Tuple[DataLoader, DataLoader, Optional[DataLoader]]:\n",
        "    # Full supervised loader\n",
        "    tds = TensorDataset(torch.from_numpy(X_train), torch.from_numpy(y_train))\n",
        "    vds = TensorDataset(torch.from_numpy(X_val), torch.from_numpy(y_val))\n",
        "\n",
        "    if labeled_frac >= 1.0:\n",
        "        return (\n",
        "            DataLoader(tds, batch_size=batch_size, shuffle=True, drop_last=False),\n",
        "            DataLoader(vds, batch_size=batch_size, shuffle=False, drop_last=False),\n",
        "            None\n",
        "        )\n",
        "\n",
        "    # Split labeled/unlabeled\n",
        "    n = len(tds)\n",
        "    n_lab = max(1, int(labeled_frac * n))\n",
        "    rng = np.random.RandomState(seed)\n",
        "    perm = rng.permutation(n)\n",
        "    lab_idx = perm[:n_lab]\n",
        "    unlab_idx = perm[n_lab:]\n",
        "\n",
        "    X_lab = torch.from_numpy(X_train[lab_idx])\n",
        "    y_lab = torch.from_numpy(y_train[lab_idx])\n",
        "    X_unlab = torch.from_numpy(X_train[unlab_idx])\n",
        "\n",
        "    lab_loader = DataLoader(TensorDataset(X_lab, y_lab), batch_size=batch_size, shuffle=True, drop_last=False)\n",
        "    unlab_loader = DataLoader(TensorDataset(X_unlab, torch.zeros(len(X_unlab)).long()), batch_size=batch_size, shuffle=True, drop_last=False)\n",
        "    val_loader = DataLoader(vds, batch_size=batch_size, shuffle=False, drop_last=False)\n",
        "    return lab_loader, val_loader, unlab_loader\n",
        "\n",
        "\n",
        "# ----------------------------\n",
        "# Model\n",
        "# ----------------------------\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, in_dim: int, num_classes: int, width: int = 64, depth: int = 2, dropout: float = 0.0):\n",
        "        super().__init__()\n",
        "        layers = []\n",
        "        prev = in_dim\n",
        "        for _ in range(max(0, depth)):\n",
        "            layers.append(nn.Linear(prev, width))\n",
        "            layers.append(nn.ReLU(inplace=True))\n",
        "            if dropout > 0:\n",
        "                layers.append(nn.Dropout(dropout))\n",
        "            prev = width\n",
        "        self.backbone = nn.Sequential(*layers) if layers else nn.Identity()\n",
        "        self.head = nn.Linear(prev, num_classes)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        h = self.backbone(x)\n",
        "        return self.head(h)\n",
        "\n",
        "\n",
        "# ----------------------------\n",
        "# Training and evaluation\n",
        "# ----------------------------\n",
        "\n",
        "def train_supervised(\n",
        "    model: nn.Module,\n",
        "    lab_loader: DataLoader,\n",
        "    val_loader: DataLoader,\n",
        "    device: torch.device,\n",
        "    epochs: int,\n",
        "    lr: float,\n",
        "    weight_decay: float\n",
        ") -> Dict[str, Any]:\n",
        "    model.to(device)\n",
        "    opt = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "    best = {\"val_acc\": -1.0, \"state_dict\": None, \"epoch\": -1}\n",
        "    history = []\n",
        "\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        model.train()\n",
        "        losses = []\n",
        "        for xb, yb in lab_loader:\n",
        "            xb = xb.to(device, non_blocking=True)\n",
        "            yb = yb.to(device, non_blocking=True)\n",
        "            logits = model(xb)\n",
        "            loss = F.cross_entropy(logits, yb)\n",
        "            opt.zero_grad(set_to_none=True)\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "            losses.append(loss.item())\n",
        "\n",
        "        val_acc = evaluate_accuracy(model, val_loader, device)\n",
        "        avg_loss = float(np.mean(losses)) if losses else 0.0\n",
        "        history.append({\"epoch\": epoch, \"train_loss\": avg_loss, \"val_acc\": val_acc})\n",
        "\n",
        "        if val_acc > best[\"val_acc\"]:\n",
        "            best = {\"val_acc\": val_acc, \"state_dict\": {k: v.cpu() for k, v in model.state_dict().items()}, \"epoch\": epoch}\n",
        "\n",
        "    # Load best state before returning\n",
        "    if best[\"state_dict\"] is not None:\n",
        "        model.load_state_dict(best[\"state_dict\"], strict=True)\n",
        "    return {\"best_val_acc\": best[\"val_acc\"], \"best_epoch\": best[\"epoch\"], \"history\": history}\n",
        "\n",
        "\n",
        "def train_selfsup_or_hybrid(\n",
        "    mode: str,\n",
        "    model: nn.Module,\n",
        "    lab_loader: DataLoader,\n",
        "    unlab_loader: DataLoader,\n",
        "    val_loader: DataLoader,\n",
        "    device: torch.device,\n",
        "    epochs: int,\n",
        "    lr: float,\n",
        "    weight_decay: float,\n",
        "    warmup_epochs: int,\n",
        "    pseudo_thresh: float,\n",
        "    lambda_consistency: float,\n",
        "    aug_noise: float\n",
        ") -> Dict[str, Any]:\n",
        "    assert mode in (\"selfsup\", \"hybrid\")\n",
        "    model.to(device)\n",
        "    opt = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "    best = {\"val_acc\": -1.0, \"state_dict\": None, \"epoch\": -1}\n",
        "    history = []\n",
        "\n",
        "    # Cycle through unlabeled loader independently\n",
        "    unlab_iter = None\n",
        "\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        model.train()\n",
        "        losses = []\n",
        "\n",
        "        # Warmup: supervised on labeled only\n",
        "        if epoch <= warmup_epochs:\n",
        "            for xb, yb in lab_loader:\n",
        "                xb = xb.to(device, non_blocking=True)\n",
        "                yb = yb.to(device, non_blocking=True)\n",
        "                logits = model(xb)\n",
        "                loss = F.cross_entropy(logits, yb)\n",
        "                opt.zero_grad(set_to_none=True)\n",
        "                loss.backward()\n",
        "                opt.step()\n",
        "                losses.append(loss.item())\n",
        "        else:\n",
        "            # Train over labeled batches; add pseudo-labels + consistency from unlabeled\n",
        "            if unlab_iter is None:\n",
        "                unlab_iter = iter(unlab_loader)\n",
        "            for xb_l, yb_l in lab_loader:\n",
        "                xb_l = xb_l.to(device, non_blocking=True)\n",
        "                yb_l = yb_l.to(device, non_blocking=True)\n",
        "\n",
        "                # Supervised loss on labeled (only if hybrid or we keep a small anchor in selfsup)\n",
        "                logits_l = model(xb_l)\n",
        "                loss_sup = F.cross_entropy(logits_l, yb_l) if mode == \"hybrid\" else 0.0\n",
        "\n",
        "                # Fetch an unlabeled batch (recycle iterator)\n",
        "                try:\n",
        "                    xb_u, _ = next(unlab_iter)\n",
        "                except StopIteration:\n",
        "                    unlab_iter = iter(unlab_loader)\n",
        "                    xb_u, _ = next(unlab_iter)\n",
        "                xb_u = xb_u.to(device, non_blocking=True)\n",
        "\n",
        "                with torch.no_grad():\n",
        "                    logits_u = model(xb_u)\n",
        "                    probs_u = F.softmax(logits_u, dim=-1)\n",
        "                    conf, pseudo = probs_u.max(dim=-1)\n",
        "                    mask = conf >= pseudo_thresh\n",
        "\n",
        "                # Pseudo-label supervised loss\n",
        "                if mask.any():\n",
        "                    logits_u_mask = model(xb_u[mask])\n",
        "                    loss_pseudo = F.cross_entropy(logits_u_mask, pseudo[mask])\n",
        "                else:\n",
        "                    loss_pseudo = 0.0\n",
        "\n",
        "                # Consistency regularization\n",
        "                noise = torch.randn_like(xb_u) * aug_noise\n",
        "                logits_orig = model(xb_u)\n",
        "                logits_aug = model(xb_u + noise)\n",
        "                # KL divergence between distributions\n",
        "                p = F.softmax(logits_orig.detach(), dim=-1)\n",
        "                log_q = F.log_softmax(logits_aug, dim=-1)\n",
        "                loss_cons = F.kl_div(log_q, p, reduction=\"batchmean\")\n",
        "\n",
        "                # Total loss\n",
        "                total = (loss_sup if isinstance(loss_sup, torch.Tensor) else torch.tensor(0.0, device=device))\n",
        "                total = total + (loss_pseudo if isinstance(loss_pseudo, torch.Tensor) else torch.tensor(0.0, device=device))\n",
        "                total = total + lambda_consistency * loss_cons\n",
        "\n",
        "                opt.zero_grad(set_to_none=True)\n",
        "                total.backward()\n",
        "                opt.step()\n",
        "                losses.append(float(total.item()))\n",
        "\n",
        "        val_acc = evaluate_accuracy(model, val_loader, device)\n",
        "        avg_loss = float(np.mean(losses)) if losses else 0.0\n",
        "        history.append({\"epoch\": epoch, \"train_loss\": avg_loss, \"val_acc\": val_acc})\n",
        "\n",
        "        if val_acc > best[\"val_acc\"]:\n",
        "            best = {\"val_acc\": val_acc, \"state_dict\": {k: v.cpu() for k, v in model.state_dict().items()}, \"epoch\": epoch}\n",
        "\n",
        "    if best[\"state_dict\"] is not None:\n",
        "        model.load_state_dict(best[\"state_dict\"], strict=True)\n",
        "    return {\"best_val_acc\": best[\"val_acc\"], \"best_epoch\": best[\"epoch\"], \"history\": history}\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate_accuracy(model: nn.Module, loader: DataLoader, device: torch.device) -> float:\n",
        "    model.eval().to(device)\n",
        "    preds = []\n",
        "    gts = []\n",
        "    for xb, yb in loader:\n",
        "        xb = xb.to(device, non_blocking=True)\n",
        "        logits = model(xb)\n",
        "        yhat = logits.argmax(dim=-1).cpu().numpy()\n",
        "        preds.append(yhat)\n",
        "        gts.append(yb.numpy())\n",
        "    y_pred = np.concatenate(preds) if preds else np.array([], dtype=np.int64)\n",
        "    y_true = np.concatenate(gts) if gts else np.array([], dtype=np.int64)\n",
        "    if len(y_true) == 0:\n",
        "        return 0.0\n",
        "    return float(accuracy_score(y_true, y_pred))\n",
        "\n",
        "\n",
        "def save_checkpoint(\n",
        "    path: str,\n",
        "    model: nn.Module,\n",
        "    train_cfg: Dict[str, Any],\n",
        "    data_cfg: Dict[str, Any],\n",
        "    metrics: Dict[str, Any]\n",
        ") -> None:\n",
        "    ensure_dir(os.path.dirname(path))\n",
        "    payload = {\n",
        "        \"meta\": {\n",
        "            \"timestamp\": now_ts(),\n",
        "            \"framework\": \"torch\",\n",
        "            \"task\": \"synthetic-classification\",\n",
        "        },\n",
        "        \"model\": {\n",
        "            \"state_dict\": {k: v.cpu() for k, v in model.state_dict().items()},\n",
        "            \"in_dim\": train_cfg[\"in_dim\"],\n",
        "            \"num_classes\": train_cfg[\"num_classes\"],\n",
        "            \"width\": train_cfg[\"width\"],\n",
        "            \"depth\": train_cfg[\"depth\"],\n",
        "            \"dropout\": train_cfg[\"dropout\"],\n",
        "        },\n",
        "        \"data_cfg\": data_cfg,\n",
        "        \"train_cfg\": train_cfg,\n",
        "        \"metrics\": metrics,\n",
        "    }\n",
        "    torch.save(payload, path)\n",
        "\n",
        "\n",
        "def load_model_from_ckpt(ckpt_path: str, device: torch.device) -> Tuple[nn.Module, Dict[str, Any]]:\n",
        "    ckpt = torch.load(ckpt_path, map_location=device)\n",
        "    mcfg = ckpt[\"model\"]\n",
        "    model = MLP(\n",
        "        in_dim=int(mcfg[\"in_dim\"]),\n",
        "        num_classes=int(mcfg[\"num_classes\"]),\n",
        "        width=int(mcfg[\"width\"]),\n",
        "        depth=int(mcfg[\"depth\"]),\n",
        "        dropout=float(mcfg[\"dropout\"]),\n",
        "    )\n",
        "    model.load_state_dict({k: v for k, v in mcfg[\"state_dict\"].items()}, strict=True)\n",
        "    model.to(device).eval()\n",
        "    return model, ckpt\n",
        "\n",
        "\n",
        "def plot_decision_boundary(\n",
        "    model: nn.Module,\n",
        "    X: np.ndarray,\n",
        "    y: np.ndarray,\n",
        "    device: torch.device,\n",
        "    title: str,\n",
        "    save_path: Optional[str] = None\n",
        ") -> Optional[str]:\n",
        "    if X.shape[1] != 2:\n",
        "        return None\n",
        "    x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\n",
        "    y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5\n",
        "    xx, yy = np.meshgrid(\n",
        "        np.linspace(x_min, x_max, 300),\n",
        "        np.linspace(y_min, y_max, 300)\n",
        "    )\n",
        "    grid = np.c_[xx.ravel(), yy.ravel()].astype(np.float32)\n",
        "    with torch.no_grad():\n",
        "        logits = model(torch.from_numpy(grid).to(device))\n",
        "        Z = logits.argmax(dim=-1).cpu().numpy().reshape(xx.shape)\n",
        "\n",
        "    plt.figure(figsize=(6, 5))\n",
        "    plt.contourf(xx, yy, Z, alpha=0.3, cmap=\"coolwarm\")\n",
        "    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=\"coolwarm\", s=12, edgecolors=\"k\", linewidths=0.2)\n",
        "    plt.title(title)\n",
        "    plt.tight_layout()\n",
        "    if save_path:\n",
        "        ensure_dir(os.path.dirname(save_path))\n",
        "        plt.savefig(save_path, dpi=150)\n",
        "        plt.close()\n",
        "        return save_path\n",
        "    return None\n",
        "\n",
        "\n",
        "# ----------------------------\n",
        "# Command handlers\n",
        "# ----------------------------\n",
        "\n",
        "def cmdtrain(args) -> None:\n",
        "    set_seed(args.seed)\n",
        "    device = get_device(args.cpu)\n",
        "\n",
        "    data_cfg = DataConfig(\n",
        "        dataset=args.dataset,\n",
        "        dims=args.dims,\n",
        "        classes=args.classes,\n",
        "        samples=args.samples,\n",
        "        noise=args.noise,\n",
        "        valfrac=args.valfrac,\n",
        "        seed=args.seed,\n",
        "    )\n",
        "    Xtr, ytr, Xval, yval = gensynthetic(datacfg)\n",
        "    labloader, valloader, unlabloader = makeloaders(\n",
        "        Xtr, ytr, Xval, yval,\n",
        "        batchsize=args.batchsize,\n",
        "        labeledfrac=args.labeledfrac if args.mode in (\"selfsup\", \"hybrid\") else 1.0,\n",
        "        seed=args.seed\n",
        "    )\n",
        "\n",
        "    model = MLP(\n",
        "        indim=datacfg.dims,\n",
        "        numclasses=datacfg.classes,\n",
        "        width=args.width,\n",
        "        depth=args.depth,\n",
        "        dropout=args.dropout\n",
        "    )\n",
        "\n",
        "    if args.mode == \"supervised\" or unlab_loader is None:\n",
        "        result = train_supervised(\n",
        "            model, labloader, valloader, device,\n",
        "            epochs=args.epochs, lr=args.lr, weightdecay=args.weightdecay\n",
        "        )\n",
        "    else:\n",
        "        result = trainselfsupor_hybrid(\n",
        "            args.mode, model, labloader, unlabloader, val_loader, device,\n",
        "            epochs=args.epochs, lr=args.lr, weightdecay=args.weightdecay,\n",
        "            warmupepochs=args.warmupepochs, pseudothresh=args.pseudothresh,\n",
        "            lambdaconsistency=args.lambdaconsistency, augnoise=args.augnoise\n",
        "        )\n",
        "\n",
        "    # Final eval on validation\n",
        "    valacc = evaluateaccuracy(model, val_loader, device)\n",
        "\n",
        "    # Save outputs\n",
        "    ensure_dir(args.out)\n",
        "    ckptpath = os.path.join(args.out, f\"checkpoint{sanitizefilename(args.dataset)}{now_ts()}.pt\")\n",
        "    train_cfg = {\n",
        "        \"mode\": args.mode,\n",
        "        \"epochs\": args.epochs,\n",
        "        \"batchsize\": args.batchsize,\n",
        "        \"lr\": args.lr,\n",
        "        \"weightdecay\": args.weightdecay,\n",
        "        \"width\": args.width,\n",
        "        \"depth\": args.depth,\n",
        "        \"dropout\": args.dropout,\n",
        "        \"indim\": datacfg.dims,\n",
        "        \"numclasses\": datacfg.classes,\n",
        "        \"seed\": args.seed,\n",
        "        \"labeledfrac\": args.labeledfrac,\n",
        "        \"pseudothresh\": args.pseudothresh,\n",
        "        \"warmupepochs\": args.warmupepochs,\n",
        "        \"lambdaconsistency\": args.lambdaconsistency,\n",
        "        \"augnoise\": args.augnoise,\n",
        "    }\n",
        "    metrics = {\n",
        "        \"bestvalacc\": result[\"bestvalacc\"],\n",
        "        \"bestepoch\": result[\"bestepoch\"],\n",
        "        \"finalvalacc\": val_acc,\n",
        "        \"history\": result[\"history\"],\n",
        "    }\n",
        "    savecheckpoint(ckptpath, model, traincfg, datacfg.dict, metrics)\n",
        "\n",
        "    # Optional plot for 2D data\n",
        "    fig_path = None\n",
        "    if args.plot and data_cfg.dims == 2:\n",
        "        figpath = os.path.join(args.out, f\"decisionboundary{sanitizefilename(args.dataset)}.png\")\n",
        "        plotdecisionboundary(model, Xtr, ytr, device, f\"Train ({args.dataset})\", fig_path)\n",
        "\n",
        "    # Write summary JSON\n",
        "    summary = {\n",
        "        \"checkpoint\": ckpt_path,\n",
        "        \"dataset\": datacfg.dict_,\n",
        "        \"traincfg\": traincfg,\n",
        "        \"metrics\": metrics,\n",
        "        \"plot\": fig_path or \"\",\n",
        "    }\n",
        "    with open(os.path.join(args.out, \"train_summary.json\"), \"w\") as f:\n",
        "        json.dump(summary, f, indent=2)\n",
        "\n",
        "    print(f\"[train] saved checkpoint: {ckpt_path}\")\n",
        "    print(f\"[train] bestvalacc={metrics['bestvalacc']:.4f} at epoch {metrics['bestepoch']} | finalvalacc={valacc:.4f}\")\n",
        "    if fig_path:\n",
        "        print(f\"[train] saved plot: {fig_path}\")\n",
        "\n",
        "\n",
        "def cmdeval(args) -> None:\n",
        "    device = get_device(args.cpu)\n",
        "    model, ckpt = loadmodelfrom_ckpt(args.ckpt, device)\n",
        "\n",
        "    dcfg = ckpt[\"data_cfg\"]\n",
        "    # Fresh test set based on training config; new seed for variation\n",
        "    test_seed = int(dcfg.get(\"seed\", 42)) + 997\n",
        "    data_cfg = DataConfig(\n",
        "        dataset=dcfg[\"dataset\"],\n",
        "        dims=int(dcfg[\"dims\"]),\n",
        "        classes=int(dcfg[\"classes\"]),\n",
        "        samples=int(args.samples),\n",
        "        noise=float(dcfg[\"noise\"]),\n",
        "        valfrac=0.0,  # not used for test-only\n",
        "        seed=test_seed,\n",
        "    )\n",
        "    Xtr, ytr, Xte, yte = gensynthetic(datacfg)  # Xtr unused; we want Xte from val slot\n",
        "    Xtest, ytest = Xte, yte\n",
        "    testloader = DataLoader(TensorDataset(torch.fromnumpy(Xtest), torch.from_numpy(ytest)),\n",
        "                             batch_size=256, shuffle=False)\n",
        "\n",
        "    acc = evaluateaccuracy(model, testloader, device)\n",
        "\n",
        "    out_dir = args.out\n",
        "    ensuredir(outdir) if out_dir else None\n",
        "\n",
        "    plot_path = None\n",
        "    if args.plot and data_cfg.dims == 2:\n",
        "        if not out_dir:\n",
        "            out_dir = os.path.dirname(args.ckpt) or \".\"\n",
        "            ensuredir(outdir)\n",
        "        plotpath = os.path.join(outdir, f\"evalboundary{now_ts()}.png\")\n",
        "        plotdecisionboundary(model, Xtest, ytest, device, f\"Eval ({dcfg['dataset']})\", plot_path)\n",
        "\n",
        "    result = {\n",
        "        \"ckpt\": args.ckpt,\n",
        "        \"dataset\": datacfg.dict_,\n",
        "        \"accuracy\": acc,\n",
        "        \"plot\": plot_path or \"\",\n",
        "        \"timestamp\": now_ts(),\n",
        "    }\n",
        "    if out_dir:\n",
        "        with open(os.path.join(outdir, \"evalsummary.json\"), \"w\") as f:\n",
        "            json.dump(result, f, indent=2)\n",
        "\n",
        "    print(f\"[eval] accuracy={acc:.4f} on {len(Xtest)} samples\")\n",
        "    if plot_path:\n",
        "        print(f\"[eval] saved plot: {plot_path}\")\n",
        "\n",
        "\n",
        "def parsepoints(pointsstr: str, dims: int) -> np.ndarray:\n",
        "    pts = []\n",
        "    for seg in points_str.split(\";\"):\n",
        "        seg = seg.strip()\n",
        "        if not seg:\n",
        "            continue\n",
        "        comps = [c.strip() for c in seg.split(\",\")]\n",
        "        if len(comps) != dims:\n",
        "            raise ValueError(f\"Point '{seg}' does not match dims={dims}\")\n",
        "        pts.append([float(x) for x in comps])\n",
        "    if not pts:\n",
        "        raise ValueError(\"No valid points parsed from --points\")\n",
        "    return np.asarray(pts, dtype=np.float32)\n",
        "\n",
        "\n",
        "def cmdpredict(args) -> None:\n",
        "    device = get_device(args.cpu)\n",
        "    model, ckpt = loadmodelfrom_ckpt(args.ckpt, device)\n",
        "    dcfg = ckpt[\"data_cfg\"]\n",
        "    dims = int(dcfg[\"dims\"])\n",
        "\n",
        "    X = None\n",
        "    if args.points:\n",
        "        X = parse_points(args.points, dims)\n",
        "    elif args.csv:\n",
        "        import csv\n",
        "        rows = []\n",
        "        with open(args.csv, \"r\", newline=\"\") as f:\n",
        "            reader = csv.reader(f)\n",
        "            for row in reader:\n",
        "                if not row:\n",
        "                    continue\n",
        "                vals = [float(x) for x in row]\n",
        "                rows.append(vals)\n",
        "        if not rows:\n",
        "            raise ValueError(\"CSV is empty or invalid\")\n",
        "        X = np.asarray(rows, dtype=np.float32)\n",
        "        if X.shape[1] != dims:\n",
        "            raise ValueError(f\"CSV has {X.shape[1]} columns but model expects dims={dims}\")\n",
        "    else:\n",
        "        raise ValueError(\"Provide either --points or --csv\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        logits = model(torch.from_numpy(X).to(device))\n",
        "        probs = F.softmax(logits, dim=-1).cpu().numpy()\n",
        "        yhat = probs.argmax(axis=1)\n",
        "\n",
        "    out_dir = args.out\n",
        "    ensuredir(outdir) if out_dir else None\n",
        "\n",
        "    # Print to console\n",
        "    for i, (p, pr) in enumerate(zip(yhat, probs)):\n",
        "        print(f\"sample[{i}] -> class={int(p)} prob={pr[int(p)]:.4f} dist={np.array2string(pr, precision=3)}\")\n",
        "\n",
        "    # Optional plot for 2D\n",
        "    plot_path = None\n",
        "    if args.plot and X.shape[1] == 2:\n",
        "        if not out_dir:\n",
        "            out_dir = os.path.dirname(args.ckpt) or \".\"\n",
        "            ensuredir(outdir)\n",
        "        plt.figure(figsize=(5, 4))\n",
        "        plt.scatter(X[:, 0], X[:, 1], c=yhat, cmap=\"coolwarm\", s=24, edgecolors=\"k\", linewidths=0.5)\n",
        "        plt.title(\"Predictions\")\n",
        "        plt.tight_layout()\n",
        "        plotpath = os.path.join(outdir, f\"predictpoints{now_ts()}.png\")\n",
        "        plt.savefig(plot_path, dpi=150)\n",
        "        plt.close()\n",
        "        print(f\"[predict] saved plot: {plot_path}\")\n",
        "\n",
        "    # Optional CSV export\n",
        "    if out_dir:\n",
        "        outcsv = os.path.join(outdir, f\"predictions{nowts()}.csv\")\n",
        "        import csv\n",
        "        with open(out_csv, \"w\", newline=\"\") as f:\n",
        "            writer = csv.writer(f)\n",
        "            header = [f\"x{i+1}\" for i in range(X.shape[1])] + [\"predclass\"] + [f\"p{i}\" for i in range(probs.shape[1])]\n",
        "            writer.writerow(header)\n",
        "            for xi, cls, pr in zip(X, yhat, probs):\n",
        "                writer.writerow([map(lambda z: f\"{z:.6f}\", xi.tolist()), int(cls), [f\"{v:.6f}\" for v in pr.tolist()]])\n",
        "        print(f\"[predict] saved CSV: {out_csv}\")\n",
        "\n",
        "\n",
        "def cmdbenchmark(args) -> None:\n",
        "    datasets = [s.strip() for s in args.datasets.split(\",\") if s.strip()]\n",
        "    modes = [s.strip() for s in args.modes.split(\",\") if s.strip()]\n",
        "    seeds = [args.seed + i for i in range(args.seeds)]\n",
        "\n",
        "    ensure_dir(args.out)\n",
        "    summary_rows = []\n",
        "    print(f\"[benchmark] datasets={datasets} modes={modes} seeds={len(seeds)}\")\n",
        "\n",
        "    for ds in datasets:\n",
        "        for mode in modes:\n",
        "            accs = []\n",
        "            for sd in seeds:\n",
        "                # Train quick run\n",
        "                runout = os.path.join(args.out, f\"{sanitizefilename(ds)}{sanitizefilename(mode)}seed{sd}{now_ts()}\")\n",
        "                os.makedirs(runout, existok=True)\n",
        "                train_args = argparse.Namespace(\n",
        "                    dataset=ds,\n",
        "                    dims=args.dims,\n",
        "                    classes=args.classes,\n",
        "                    samples=args.samples,\n",
        "                    noise=args.noise,\n",
        "                    valfrac=args.valfrac,\n",
        "                    mode=mode,\n",
        "                    epochs=args.epochs,\n",
        "                    batchsize=args.batchsize,\n",
        "                    lr=args.lr,\n",
        "                    weightdecay=args.weightdecay,\n",
        "                    width=args.width,\n",
        "                    depth=args.depth,\n",
        "                    dropout=args.dropout,\n",
        "                    augnoise=args.augnoise,\n",
        "                    labeledfrac=args.labeledfrac,\n",
        "                    pseudothresh=args.pseudothresh,\n",
        "                    warmupepochs=args.warmupepochs,\n",
        "                    lambdaconsistency=args.lambdaconsistency,\n",
        "                    plot=False,\n",
        "                    seed=sd,\n",
        "                    cpu=args.cpu,\n",
        "                    out=run_out,\n",
        "                )\n",
        "                # Train\n",
        "                cmdtrain(train_args)\n",
        "\n",
        "                # Find checkpoint saved by train\n",
        "                ckpts = [f for f in os.listdir(run_out) if f.endswith(\".pt\")]\n",
        "                if not ckpts:\n",
        "                    print(f\"[benchmark][warn] no checkpoint in {run_out}\")\n",
        "                    continue\n",
        "                ckptpath = os.path.join(runout, sorted(ckpts)[-1])\n",
        "\n",
        "                # Eval\n",
        "                eval_args = argparse.Namespace(\n",
        "                    ckpt=ckpt_path,\n",
        "                    samples=args.samples,\n",
        "                    plot=False,\n",
        "                    cpu=args.cpu,\n",
        "                    out=run_out\n",
        "                )\n",
        "                # Capture eval output (accuracy)\n",
        "                device = get_device(args.cpu)\n",
        "                model, ckpt = loadmodelfromckpt(evalargs.ckpt, device)\n",
        "                dcfg = ckpt[\"data_cfg\"]\n",
        "                test_cfg = DataConfig(\n",
        "                    dataset=dcfg[\"dataset\"],\n",
        "                    dims=int(dcfg[\"dims\"]),\n",
        "                    classes=int(dcfg[\"classes\"]),\n",
        "                    samples=int(eval_args.samples),\n",
        "                    noise=float(dcfg[\"noise\"]),\n",
        "                    valfrac=0.0,\n",
        "                    seed=int(dcfg.get(\"seed\", 42)) + 997,\n",
        "                )\n",
        "                , , Xte, yte = gensynthetic(testcfg)\n",
        "                testloader = DataLoader(TensorDataset(torch.fromnumpy(Xte), torch.fromnumpy(yte)), batchsize=256)\n",
        "                acc = evaluateaccuracy(model, testloader, device)\n",
        "                accs.append(acc)\n",
        "\n",
        "            if accs:\n",
        "                mean = float(np.mean(accs))\n",
        "                std = float(np.std(accs))\n",
        "            else:\n",
        "                mean, std = 0.0, 0.0\n",
        "            summary_rows.append({\n",
        "                \"dataset\": ds,\n",
        "                \"mode\": mode,\n",
        "                \"mean_acc\": mean,\n",
        "                \"std_acc\": std,\n",
        "                \"runs\": len(accs),\n",
        "            })\n",
        "            print(f\"[benchmark] {ds:>12} | {mode:>9} -> mean_acc={mean:.4f} ± {std:.4f} over {len(accs)} runs\")\n",
        "\n",
        "    # Save CSV\n",
        "    csvpath = os.path.join(args.out, f\"benchmark{now_ts()}.csv\")\n",
        "    import csv\n",
        "    with open(csv_path, \"w\", newline=\"\") as f:\n",
        "        writer = csv.writer(f)\n",
        "        writer.writerow([\"dataset\", \"mode\", \"meanacc\", \"stdacc\", \"runs\"])\n",
        "        for row in summary_rows:\n",
        "            writer.writerow([row[\"dataset\"], row[\"mode\"], f\"{row['meanacc']:.6f}\", f\"{row['stdacc']:.6f}\", row[\"runs\"]])\n",
        "    print(f\"[benchmark] saved summary CSV: {csv_path}\")\n",
        "\n",
        "\n",
        "def cmdexport_onnx(args) -> None:\n",
        "    device = get_device(args.cpu)\n",
        "    model, ckpt = loadmodelfrom_ckpt(args.ckpt, device)\n",
        "    mcfg = ckpt[\"model\"]\n",
        "    indim = int(mcfg[\"indim\"])\n",
        "    dummy = torch.zeros(1, in_dim, dtype=torch.float32)\n",
        "\n",
        "    # Export on CPU for portability\n",
        "    model_cpu = model.to(\"cpu\").eval()\n",
        "    torch.onnx.export(\n",
        "        model_cpu,\n",
        "        dummy,\n",
        "        args.out,\n",
        "        input_names=[\"input\"],\n",
        "        output_names=[\"logits\"],\n",
        "        dynamic_axes={\"input\": {0: \"batch\"}, \"logits\": {0: \"batch\"}},\n",
        "        opset_version=13,\n",
        "    )\n",
        "    print(f\"[export-onnx] saved ONNX to {args.out}\")\n",
        "\n",
        "# ----------------------------\n",
        "# Argparse\n",
        "# ----------------------------\n",
        "\n",
        "def build_parser():\n",
        "    p = argparse.ArgumentParser(\n",
        "        description=\"MetaIntelligence end-to-end toolkit (notebook/Colab-safe)\"\n",
        "    )\n",
        "    sub = p.add_subparsers(dest=\"cmd\", required=True)\n",
        "\n",
        "    # Train\n",
        "    t = sub.add_parser(\"train\", help=\"Train a model (self/supervised/hybrid)\")\n",
        "    t.add_argument(\"--dataset\", type=str, default=\"moons\", choices=[\"moons\", \"circles\", \"blobs\", \"classification\"])\n",
        "    t.add_argument(\"--dims\", type=int, default=2)\n",
        "    t.add_argument(\"--classes\", type=int, default=2)\n",
        "    t.add_argument(\"--samples\", type=int, default=2000)\n",
        "    t.add_argument(\"--noise\", type=float, default=0.2)\n",
        "    t.add_argument(\"--val-frac\", dest=\"valfrac\", type=float, default=0.2)\n",
        "\n",
        "    t.add_argument(\"--mode\", type=str, default=\"supervised\", choices=[\"supervised\", \"selfsup\", \"hybrid\"])\n",
        "    t.add_argument(\"--epochs\", type=int, default=50)\n",
        "    t.add_argument(\"--batch-size\", type=int, default=128)\n",
        "    t.add_argument(\"--lr\", type=float, default=3e-3)\n",
        "    t.add_argument(\"--weight-decay\", type=float, default=0.0)\n",
        "    t.add_argument(\"--width\", type=int, default=64)\n",
        "    t.add_argument(\"--depth\", type=int, default=2)\n",
        "    t.add_argument(\"--dropout\", type=float, default=0.0)\n",
        "    t.add_argument(\"--aug-noise\", type=float, default=0.05, help=\"Gaussian feature noise for augmentation\")\n",
        "\n",
        "    # self-training/hybrid knobs\n",
        "    t.add_argument(\"--labeled-frac\", type=float, default=0.1)\n",
        "    t.add_argument(\"--pseudo-thresh\", type=float, default=0.9)\n",
        "    t.add_argument(\"--warmup-epochs\", type=int, default=5)\n",
        "    t.add_argument(\"--lambda-consistency\", type=float, default=0.5)\n",
        "\n",
        "    t.add_argument(\"--plot\", action=\"store_true\")\n",
        "    t.add_argument(\"--seed\", type=int, default=42)\n",
        "    t.add_argument(\"--cpu\", action=\"store_true\")\n",
        "    t.add_argument(\"--out\", type=str, required=True)\n",
        "\n",
        "    # Eval\n",
        "    e = sub.add_parser(\"eval\", help=\"Evaluate a saved checkpoint on a fresh synthetic test set\")\n",
        "    e.add_argument(\"--ckpt\", type=str, required=True)\n",
        "    e.add_argument(\"--samples\", type=int, default=2000)\n",
        "    e.add_argument(\"--plot\", action=\"store_true\")\n",
        "    e.add_argument(\"--cpu\", action=\"store_true\")\n",
        "    e.add_argument(\"--out\", type=str, default=\"\")\n",
        "\n",
        "    # Predict\n",
        "    pr = sub.add_parser(\"predict\", help=\"Predict for given points\")\n",
        "    pr.add_argument(\"--ckpt\", type=str, required=True)\n",
        "    pr.add_argument(\"--points\", type=str, default=\"\", help='Inline points: \"x1,x2; y1,y2; ...\"')\n",
        "    pr.add_argument(\"--csv\", type=str, default=\"\", help=\"CSV file with samples (rows) and features (columns)\")\n",
        "    pr.add_argument(\"--plot\", action=\"store_true\")\n",
        "    pr.add_argument(\"--cpu\", action=\"store_true\")\n",
        "    pr.add_argument(\"--out\", type=str, default=\"\")\n",
        "\n",
        "    # Benchmark\n",
        "    b = sub.add_parser(\"benchmark\", help=\"Run grid of modes/datasets/dims across seeds and summarize\")\n",
        "    b.add_argument(\"--datasets\", type=str, default=\"moons,blobs\")\n",
        "    b.add_argument(\"--modes\", type=str, default=\"supervised,selfsup,hybrid\")\n",
        "    b.add_argument(\"--dims\", type=int, default=2)\n",
        "    b.add_argument(\"--classes\", type=int, default=2)\n",
        "    b.add_argument(\"--samples\", type=int, default=2000)\n",
        "    b.add_argument(\"--noise\", type=float, default=0.2)\n",
        "    b.add_argument(\"--val-frac\", dest=\"valfrac\", type=float, default=0.2)\n",
        "    b.add_argument(\"--epochs\", type=int, default=40)\n",
        "    b.add_argument(\"--batch-size\", type=int, default=128)\n",
        "    b.add_argument(\"--lr\", type=float, default=3e-3)\n",
        "    b.add_argument(\"--weight-decay\", type=float, default=0.0)\n",
        "    b.add_argument(\"--width\", type=int, default=64)\n",
        "    b.add_argument(\"--depth\", type=int, default=2)\n",
        "    b.add_argument(\"--dropout\", type=float, default=0.0)\n",
        "    b.add_argument(\"--aug-noise\", type=float, default=0.05)\n",
        "    b.add_argument(\"--labeled-frac\", type=float, default=0.1)\n",
        "    b.add_argument(\"--pseudo-thresh\", type=float, default=0.9)\n",
        "    b.add_argument(\"--warmup-epochs\", type=int, default=5)\n",
        "    b.add_argument(\"--lambda-consistency\", type=float, default=0.5)\n",
        "    b.add_argument(\"--seeds\", type=int, default=3, help=\"number of seeds to run starting from --seed\")\n",
        "    b.add_argument(\"--seed\", type=int, default=42)\n",
        "    b.add_argument(\"--cpu\", action=\"store_true\")\n",
        "    b.add_argument(\"--out\", type=str, required=True)\n",
        "\n",
        "    # Export ONNX\n",
        "    x = sub.add_parser(\"export-onnx\", help=\"Export a trained checkpoint to ONNX\")\n",
        "    x.add_argument(\"--ckpt\", type=str, required=True)\n",
        "    x.add_argument(\"--out\", type=str, required=True)\n",
        "    x.add_argument(\"--cpu\", action=\"store_true\")\n",
        "\n",
        "    return p\n",
        "\n",
        "\n",
        "def main(argv: Optional[List[str]] = None):\n",
        "    parser = build_parser()\n",
        "\n",
        "    # Notebook/Colab-safe: running with no args prints help and returns\n",
        "    if argv is None:\n",
        "        argv = []\n",
        "    if len(argv) == 0:\n",
        "        parser.print_help()\n",
        "        return\n",
        "\n",
        "    args = parser.parse_args(argv)\n",
        "\n",
        "    # Basic sanitization for file paths\n",
        "    for attr in [\"ckpt\", \"csv\", \"out\"]:\n",
        "        if hasattr(args, attr):\n",
        "            val = getattr(args, attr)\n",
        "            if isinstance(val, str) and any(ch in val for ch in [\"..\", \"|\", \";\", \"`\"]):\n",
        "                raise ValueError(f\"Unsafe characters in path argument: --{attr}\")\n",
        "\n",
        "    # Dispatch after handlers are defined\n",
        "    dispatch = {\n",
        "        \"train\": cmdtrain,\n",
        "        \"eval\": cmdeval,\n",
        "        \"predict\": cmdpredict,\n",
        "        \"benchmark\": cmdbenchmark,\n",
        "        \"export-onnx\": cmdexport_onnx,\n",
        "    }\n",
        "    handler = dispatch.get(args.cmd)\n",
        "    if handler is None:\n",
        "        parser.error(f\"Unknown command: {args.cmd}\")\n",
        "    return handler(args)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Safe for notebooks and scripts\n",
        "    main()"
      ]
    }
  ]
}