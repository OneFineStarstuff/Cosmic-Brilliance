{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyNxjOAaTibm3m5bpdcbd1kA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OneFineStarstuff/Cosmic-Brilliance/blob/main/train_infinite_recursion_ai_py.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JKzFcOXHBl51"
      },
      "outputs": [],
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "train_infinite_recursion_ai.py\n",
        "\n",
        "Pipeline for InfiniteRecursionAI:\n",
        "1. Synthetic “infinite recursion” dataset of 6 inputs → 3 targets\n",
        "2. float32 normalization & dtype consistency\n",
        "3. MLP with ReLU (simple architecture)\n",
        "4. Physics‐informed residual enforcing toy recursion laws\n",
        "5. MC‐Dropout for uncertainty quantification\n",
        "6. Training loop: AdamW, ReduceLROnPlateau, gradient clipping, NaN checks, early stopping, checkpointing\n",
        "7. Visualizations: loss curves, scatter plots, uncertainty heatmap\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 1. Synthetic Infinite Recursion Dataset\n",
        "# ------------------------------------------------------------------------------\n",
        "class InfiniteRecursionDataset(Dataset):\n",
        "    def __init__(self, n_samples=6000, seed=42):\n",
        "        np.random.seed(seed)\n",
        "        # Inputs:\n",
        "        # RIE: recursive expansion rate ∈ [0.1, 5.0]\n",
        "        # FAS: fractal AGI synthesis scale ∈ [0.01, 2.0]\n",
        "        # ORD: omniversal recursion depth ∈ [1, 100]\n",
        "        # EIR: energy injection rate ∈ [1, 1000]\n",
        "        # CXS: complexity scaling ∈ [0.1, 10.0]\n",
        "        # DIT: dimensional iteration threshold ∈ [0.5, 5.0]\n",
        "        RIE = np.random.uniform(0.1, 5.0,   (n_samples,1))\n",
        "        FAS = np.random.uniform(0.01,2.0,   (n_samples,1))\n",
        "        ORD = np.random.uniform(1, 100,     (n_samples,1))\n",
        "        EIR = np.random.uniform(1, 1000,    (n_samples,1))\n",
        "        CXS = np.random.uniform(0.1,10.0,   (n_samples,1))\n",
        "        DIT = np.random.uniform(0.5,5.0,    (n_samples,1))\n",
        "\n",
        "        X_raw = np.hstack([RIE, FAS, ORD, EIR, CXS, DIT]).astype(np.float64)\n",
        "\n",
        "        # Toy targets:\n",
        "        # IRS: infinite recursion stability = RIE * np.log1p(ORD) / (FAS + eps)\n",
        "        # SGR: self-generation rate = FAS * np.sqrt(EIR)\n",
        "        # RCS: recursive complexity scaling = CXS * DIT / (RIE + eps)\n",
        "        eps = 1e-8\n",
        "        IRS = RIE * np.log1p(ORD) / (FAS + eps)\n",
        "        SGR = FAS * np.sqrt(EIR)\n",
        "        RCS = CXS * DIT / (RIE + eps)\n",
        "\n",
        "        Y_raw = np.hstack([IRS, SGR, RCS]).astype(np.float64)\n",
        "        Y_raw += 0.01 * Y_raw.std(axis=0) * np.random.randn(*Y_raw.shape)\n",
        "\n",
        "        # Stats\n",
        "        self.X_mean = X_raw.mean(axis=0)\n",
        "        self.X_std  = X_raw.std(axis=0) + eps\n",
        "        self.Y_mean = Y_raw.mean(axis=0)\n",
        "        self.Y_std  = Y_raw.std(axis=0) + eps\n",
        "\n",
        "        # Normalize to float32\n",
        "        self.X = ((X_raw - self.X_mean) / self.X_std).astype(np.float32)\n",
        "        self.Y = ((Y_raw - self.Y_mean) / self.Y_std).astype(np.float32)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return torch.from_numpy(self.X[idx]), torch.from_numpy(self.Y[idx])\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 2. InfiniteRecursionAI Model\n",
        "# ------------------------------------------------------------------------------\n",
        "class InfiniteRecursionAI(nn.Module):\n",
        "    def __init__(self, input_dim=6, hidden_dim=32, output_dim=3, p_drop=0.1):\n",
        "        super().__init__()\n",
        "        self.fc1  = nn.Linear(input_dim, hidden_dim)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.drop = nn.Dropout(p_drop)\n",
        "        self.fc2  = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.drop(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 3. Physics-Informed Residual Loss\n",
        "# ------------------------------------------------------------------------------\n",
        "def physics_residual(pred, X, stats):\n",
        "    X_den = X * stats['X_std'] + stats['X_mean']\n",
        "    RIE, FAS, ORD, EIR, CXS, DIT = X_den.t()\n",
        "    eps = 1e-8\n",
        "\n",
        "    IRS_t = RIE * torch.log1p(ORD) / (FAS + eps)\n",
        "    SGR_t = FAS * torch.sqrt(EIR)\n",
        "    RCS_t = CXS * DIT / (RIE + eps)\n",
        "\n",
        "    Yt      = torch.stack([IRS_t, SGR_t, RCS_t], dim=1)\n",
        "    Yt_norm = (Yt - stats['Y_mean']) / stats['Y_std']\n",
        "    return nn.MSELoss()(pred, Yt_norm)\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 4. Combined Loss Function\n",
        "# ------------------------------------------------------------------------------\n",
        "def total_loss(pred, true, X, stats, lambda_phys=1.0):\n",
        "    mse  = nn.MSELoss()(pred, true)\n",
        "    phys = physics_residual(pred, X, stats)\n",
        "    return mse + lambda_phys * phys, mse, phys\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 5. MC-Dropout Uncertainty Estimation\n",
        "# ------------------------------------------------------------------------------\n",
        "def mc_dropout_predict(model, X, T=50):\n",
        "    model.train()\n",
        "    preds = []\n",
        "    with torch.no_grad():\n",
        "        for _ in range(T):\n",
        "            preds.append(model(X))\n",
        "    arr = torch.stack(preds, dim=0)\n",
        "    return arr.mean(dim=0), arr.std(dim=0)\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 6. Training Loop & Checkpointing\n",
        "# ------------------------------------------------------------------------------\n",
        "def train(model, train_loader, val_loader, stats, device,\n",
        "          lr=1e-4, wd=1e-5, lambda_phys=1.0, epochs=100, patience=10):\n",
        "    model.to(device)\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=wd)\n",
        "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
        "        optimizer, mode='min', factor=0.5, patience=5\n",
        "    )\n",
        "\n",
        "    best_val, wait = float('inf'), 0\n",
        "    history = {'train': [], 'val': []}\n",
        "\n",
        "    for ep in range(1, epochs+1):\n",
        "        model.train()\n",
        "        run_tr = 0.0\n",
        "        for xb, yb in train_loader:\n",
        "            xb, yb = xb.to(device), yb.to(device)\n",
        "            pred = model(xb)\n",
        "            loss, _, _ = total_loss(pred, yb, xb, stats, lambda_phys)\n",
        "            if torch.isnan(loss):\n",
        "                raise RuntimeError(f\"NaN loss at epoch {ep}\")\n",
        "            optimizer.zero_grad(); loss.backward()\n",
        "            nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            optimizer.step()\n",
        "            run_tr += loss.item() * xb.size(0)\n",
        "        train_loss = run_tr / len(train_loader.dataset)\n",
        "\n",
        "        model.eval()\n",
        "        run_va = 0.0\n",
        "        with torch.no_grad():\n",
        "            for xb, yb in val_loader:\n",
        "                xb, yb = xb.to(device), yb.to(device)\n",
        "                pred = model(xb)\n",
        "                l, _, _ = total_loss(pred, yb, xb, stats, lambda_phys)\n",
        "                run_va += l.item() * xb.size(0)\n",
        "        val_loss = run_va / len(val_loader.dataset)\n",
        "\n",
        "        scheduler.step(val_loss)\n",
        "        history['train'].append(train_loss)\n",
        "        history['val'].append(val_loss)\n",
        "        print(f\"Epoch {ep:03d} | Train {train_loss:.4e} | Val {val_loss:.4e}\")\n",
        "\n",
        "        if val_loss < best_val - 1e-6:\n",
        "            best_val, wait = val_loss, 0\n",
        "            torch.save(model.state_dict(), \"best_infinite_recursion_ai.pth\")\n",
        "        else:\n",
        "            wait += 1\n",
        "            if wait >= patience:\n",
        "                print(\"Early stopping.\")\n",
        "                break\n",
        "\n",
        "    model.load_state_dict(torch.load(\n",
        "        \"best_infinite_recursion_ai.pth\", map_location=device))\n",
        "    return history\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 7. Visualization Helpers\n",
        "# ------------------------------------------------------------------------------\n",
        "def plot_history(history):\n",
        "    plt.figure()\n",
        "    plt.plot(history['train'], label='Train Loss')\n",
        "    plt.plot(history['val'],   label='Val Loss')\n",
        "    plt.xlabel(\"Epoch\"); plt.ylabel(\"Loss\"); plt.legend(); plt.show()\n",
        "\n",
        "def plot_scatter(y_true, y_pred, title):\n",
        "    plt.figure()\n",
        "    plt.scatter(y_true, y_pred, s=6, alpha=0.5)\n",
        "    mn, mx = y_true.min(), y_true.max()\n",
        "    plt.plot([mn,mx],[mn,mx],'r--')\n",
        "    plt.title(title); plt.show()\n",
        "\n",
        "def plot_uncertainty(model, stats, device):\n",
        "    G = 100\n",
        "    RIF = np.linspace(0.1,5.0,G, dtype=np.float32)\n",
        "    ORD = np.linspace(1,100, G, dtype=np.float32)\n",
        "    G1, G2 = np.meshgrid(RIF, ORD)\n",
        "    pts = G*G\n",
        "\n",
        "    Xg = torch.zeros((pts,6), device=device)\n",
        "    # fix FAS,EIR,CXS,DIT at mean\n",
        "    for i in [1,3,4,5]:\n",
        "        Xg[:, i] = stats['X_mean'][i]\n",
        "    Xg[:,0] = torch.from_numpy(G1.ravel()).to(device)\n",
        "    Xg[:,2] = torch.from_numpy(G2.ravel()).to(device)\n",
        "\n",
        "    Xn = (Xg - stats['X_mean']) / stats['X_std']\n",
        "    _, std = mc_dropout_predict(model, Xn, T=40)\n",
        "    U = std[:,0].cpu().reshape(G1.shape)\n",
        "\n",
        "    plt.figure(figsize=(5,4))\n",
        "    plt.pcolormesh(RIF, ORD, U, shading='auto', cmap='magma')\n",
        "    plt.colorbar(label=\"Std(Infinite Stability)\")\n",
        "    plt.xlabel(\"Recursion Rate (RIE)\")\n",
        "    plt.ylabel(\"Recursion Depth (ORD)\")\n",
        "    plt.title(\"Uncertainty Heatmap\")\n",
        "    plt.show()\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 8. Main Execution\n",
        "# ------------------------------------------------------------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    ds    = InfiniteRecursionDataset(n_samples=6000, seed=42)\n",
        "    stats = {\n",
        "        'X_mean': torch.tensor(ds.X_mean, dtype=torch.float32, device=device),\n",
        "        'X_std' : torch.tensor(ds.X_std,  dtype=torch.float32, device=device),\n",
        "        'Y_mean': torch.tensor(ds.Y_mean, dtype=torch.float32, device=device),\n",
        "        'Y_std' : torch.tensor(ds.Y_std,  dtype=torch.float32, device=device),\n",
        "    }\n",
        "\n",
        "    n_val = int(0.2 * len(ds))\n",
        "    tr_ds, va_ds = random_split(ds, [len(ds)-n_val, n_val])\n",
        "    tr_ld = DataLoader(tr_ds, batch_size=128, shuffle=True)\n",
        "    va_ld = DataLoader(va_ds, batch_size=256, shuffle=False)\n",
        "\n",
        "    model   = InfiniteRecursionAI().to(device)\n",
        "    history = train(model, tr_ld, va_ld, stats, device)\n",
        "\n",
        "    plot_history(history)\n",
        "\n",
        "    X_all = torch.from_numpy(ds.X).to(device)\n",
        "    with torch.no_grad():\n",
        "        Yp_n = model(X_all).cpu().numpy()\n",
        "    Yt = ds.Y * ds.Y_std + ds.Y_mean\n",
        "    Yp = Yp_n * ds.Y_std + ds.Y_mean\n",
        "\n",
        "    names = [\"Recursion Stability\",\"Self-Generation Rate\",\"Complexity Scaling\"]\n",
        "    for i, nm in enumerate(names):\n",
        "        plot_scatter(Yt[:,i], Yp[:,i], nm)\n",
        "\n",
        "    plot_uncertainty(model, stats, device)"
      ]
    }
  ]
}