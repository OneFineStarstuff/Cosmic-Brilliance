{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyOLHS62zg+Z8fuy4Lb3jNvc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OneFineStarstuff/Cosmic-Brilliance/blob/main/parity_qnn_reupload_tuned_py.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ca-NXpNLVLMB"
      },
      "outputs": [],
      "source": [
        "pip install pennylane torch tqdm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "parity_qnn_reupload_tuned.py\n",
        "\n",
        "Tuned hybrid parity QNN pipeline with:\n",
        "- Increased expressivity (6 layers, dual‐axis embedding)\n",
        "- Larger classical post‐net (batch norm + 64‐unit hidden layer)\n",
        "- No dropout\n",
        "- AdamW optimizer + gradient clipping + cosine annealing LR scheduler\n",
        "- MC‐Dropout, temperature scaling & reliability diagram\n",
        "- Robust argparse (ignores Colab/IPython flags)\n",
        "\"\"\"\n",
        "\n",
        "import argparse\n",
        "import numpy as np\n",
        "import pennylane as qml\n",
        "from pennylane.templates.layers import StronglyEntanglingLayers\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.calibration import calibration_curve\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "def parse_args():\n",
        "    parser = argparse.ArgumentParser(\n",
        "        description=\"Tuned Parity QNN with data re‐uploading\"\n",
        "    )\n",
        "    parser.add_argument(\"--n_qubits\",   type=int,   default=3,\n",
        "                        help=\"Number of qubits / input bits\")\n",
        "    parser.add_argument(\"--n_layers\",   type=int,   default=6,\n",
        "                        help=\"Number of quantum re‐uploading layers\")\n",
        "    parser.add_argument(\"--hidden_dim\", type=int,   default=64,\n",
        "                        help=\"Hidden dimension of classical MLP post‐net\")\n",
        "    parser.add_argument(\"--dropout\",    type=float, default=0.0,\n",
        "                        help=\"Dropout probability in classical post‐net\")\n",
        "    parser.add_argument(\"--mc_runs\",    type=int,   default=50,\n",
        "                        help=\"Number of Monte Carlo dropout runs\")\n",
        "    parser.add_argument(\"--epochs\",     type=int,   default=30,\n",
        "                        help=\"Number of training epochs\")\n",
        "    parser.add_argument(\"--lr\",         type=float, default=2e-2,\n",
        "                        help=\"Initial learning rate for AdamW\")\n",
        "    args, _ = parser.parse_known_args()  # ignore unknown IPython flags\n",
        "    return args\n",
        "\n",
        "def generate_parity_dataset(n_qubits):\n",
        "    \"\"\"Generate all 2^n_qubits bitstrings and parity labels.\"\"\"\n",
        "    X = np.array(\n",
        "        [list(map(int, np.binary_repr(i, width=n_qubits)))\n",
        "         for i in range(2**n_qubits)],\n",
        "        dtype=np.float32,\n",
        "    )\n",
        "    y = X.sum(axis=1) % 2\n",
        "    return X, y.astype(np.int64)\n",
        "\n",
        "class HybridParityReupload(nn.Module):\n",
        "    \"\"\"Hybrid model: classical pre‐net, quantum layer, and classical post‐net.\"\"\"\n",
        "    def __init__(self, qlayer, n_qubits, hidden_dim, dropout_p):\n",
        "        super().__init__()\n",
        "        self.pre_net = nn.Sequential(\n",
        "            nn.Linear(n_qubits, n_qubits),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "        self.qlayer  = qlayer\n",
        "        self.dropout = nn.Dropout(dropout_p)\n",
        "        self.post_net = nn.Sequential(\n",
        "            nn.BatchNorm1d(n_qubits),\n",
        "            nn.Linear(n_qubits, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, 2),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pre_net(x)\n",
        "        x = self.qlayer(x)           # quantum features\n",
        "        x = self.dropout(x)\n",
        "        return self.post_net(x)\n",
        "\n",
        "def main():\n",
        "    args = parse_args()\n",
        "\n",
        "    # 1) Prepare data\n",
        "    X, y = generate_parity_dataset(args.n_qubits)\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y, test_size=0.2, random_state=42, stratify=y\n",
        "    )\n",
        "    train_ds = TensorDataset(\n",
        "        torch.tensor(X_train, dtype=torch.float32),\n",
        "        torch.tensor(y_train, dtype=torch.int64),\n",
        "    )\n",
        "    test_ds  = TensorDataset(\n",
        "        torch.tensor(X_test, dtype=torch.float32),\n",
        "        torch.tensor(y_test, dtype=torch.int64),\n",
        "    )\n",
        "    train_loader = DataLoader(train_ds, batch_size=32, shuffle=True)\n",
        "    test_loader  = DataLoader(test_ds,  batch_size=32)\n",
        "\n",
        "    # 2) Quantum device & QNode\n",
        "    dev = qml.device(\"default.qubit\", wires=args.n_qubits)\n",
        "\n",
        "    @qml.qnode(dev, interface=\"torch\", diff_method=\"backprop\")\n",
        "    def circuit(inputs, weights):\n",
        "        wires = range(args.n_qubits)\n",
        "        for layer in range(args.n_layers):\n",
        "            # Dual‐axis embedding for extra nonlinearity\n",
        "            qml.templates.AngleEmbedding(inputs, wires=wires, rotation=\"Y\")\n",
        "            qml.templates.AngleEmbedding(inputs, wires=wires, rotation=\"Z\")\n",
        "            w = weights[layer : layer + 1]  # shape (1, n_qubits, 3)\n",
        "            StronglyEntanglingLayers(w, wires=wires)\n",
        "        # final embedding boost\n",
        "        qml.templates.AngleEmbedding(inputs, wires=wires, rotation=\"Y\")\n",
        "        return [qml.expval(qml.PauliZ(i)) for i in wires]\n",
        "\n",
        "    weight_shapes = {\"weights\": (args.n_layers, args.n_qubits, 3)}\n",
        "    qlayer = qml.qnn.TorchLayer(circuit, weight_shapes)\n",
        "\n",
        "    # 3) Model, optimizer, loss, scheduler\n",
        "    model     = HybridParityReupload(qlayer, args.n_qubits, args.hidden_dim, args.dropout)\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=args.lr, weight_decay=1e-3)\n",
        "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=args.epochs)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # 4) Training loop with gradient clipping\n",
        "    for epoch in range(1, args.epochs + 1):\n",
        "        model.train()\n",
        "        total_loss, correct = 0.0, 0\n",
        "        for xb, yb in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            logits = model(xb)\n",
        "            loss   = criterion(logits, yb)\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item() * xb.size(0)\n",
        "            correct    += (logits.argmax(1) == yb).sum().item()\n",
        "\n",
        "        scheduler.step()\n",
        "        train_acc = correct / len(train_loader.dataset)\n",
        "\n",
        "        model.eval()\n",
        "        correct = 0\n",
        "        with torch.no_grad():\n",
        "            for xb, yb in test_loader:\n",
        "                correct += (model(xb).argmax(1) == yb).sum().item()\n",
        "        test_acc = correct / len(test_loader.dataset)\n",
        "\n",
        "        print(\n",
        "            f\"Epoch {epoch:2d} | \"\n",
        "            f\"Loss {total_loss/len(train_loader.dataset):.4f} | \"\n",
        "            f\"Train {train_acc:.3f} | Test {test_acc:.3f}\"\n",
        "        )\n",
        "\n",
        "    # 5) MC‐Dropout uncertainty (dropout=0 → zero variance here)\n",
        "    model.train()\n",
        "    all_probs = []\n",
        "    with torch.no_grad():\n",
        "        for _ in range(args.mc_runs):\n",
        "            batch_ps = []\n",
        "            for xb, _ in test_loader:\n",
        "                ps = torch.softmax(model(xb), dim=1).cpu().numpy()\n",
        "                batch_ps.append(ps)\n",
        "            all_probs.append(np.vstack(batch_ps))\n",
        "    var_est = np.stack(all_probs).var(axis=0).mean(axis=1)\n",
        "    plt.hist(var_est, bins=20)\n",
        "    plt.title(\"MC‐Dropout Variance\")\n",
        "    plt.savefig(\"mc_variance.png\")\n",
        "    print(\"Saved MC‐Dropout variance histogram\")\n",
        "\n",
        "    # 6) Temperature scaling on training set\n",
        "    model.eval()\n",
        "    logits_list, labels_list = [], []\n",
        "    with torch.no_grad():\n",
        "        for xb, yb in train_loader:\n",
        "            logits_list.append(model(xb))\n",
        "            labels_list.append(yb)\n",
        "    logits_stack = torch.cat(logits_list)\n",
        "    labels_stack = torch.cat(labels_list)\n",
        "\n",
        "    T = torch.ones(1, requires_grad=True)\n",
        "    def loss_T(): return criterion(logits_stack / T, labels_stack)\n",
        "    optim.LBFGS([T], lr=0.1, max_iter=50).step(lambda: loss_T())\n",
        "    T = T.detach()\n",
        "    print(f\"Optimal temperature T = {T.item():.3f}\")\n",
        "\n",
        "    # 7) Reliability diagram on test set\n",
        "    model.eval()\n",
        "    logits_test = torch.cat([model(xb) for xb, _ in test_loader])\n",
        "    probs_test  = (\n",
        "        torch.softmax(logits_test / T, dim=1)\n",
        "        .detach()\n",
        "        .cpu()\n",
        "        .numpy()[:, 1]\n",
        "    )\n",
        "    frac_pos, mean_pred = calibration_curve(y_test, probs_test, n_bins=10)\n",
        "    plt.figure()\n",
        "    plt.plot(mean_pred, frac_pos, \"s-\", label=\"Model\")\n",
        "    plt.plot([0, 1], [0, 1], \"--\", label=\"Ideal\")\n",
        "    plt.xlabel(\"Mean predicted probability\")\n",
        "    plt.ylabel(\"Fraction of positives\")\n",
        "    plt.title(\"Reliability Diagram\")\n",
        "    plt.legend()\n",
        "    plt.savefig(\"reliability_diagram.png\")\n",
        "    print(\"Saved reliability diagram\")\n",
        "\n",
        "    # 8) Final test accuracy\n",
        "    final_acc = (logits_test.argmax(1).cpu().numpy() == y_test).mean()\n",
        "    print(f\"Final Test Accuracy: {final_acc:.3f}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "eqgAIYwnVa02"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}