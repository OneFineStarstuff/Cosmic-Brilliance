{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyOZ2e0paRa9MkbXseJXIP3C",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OneFineStarstuff/Cosmic-Brilliance/blob/main/quantum_gravity_rl_py.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install torch gymnasium numpy"
      ],
      "metadata": {
        "id": "oxymkn-SYVoe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "Deep RL for Quantum Gravity Corrections using DDPG.\n",
        "State dim = 4, Action dim = 4 (quantum metric corrections).\n",
        "\"\"\"\n",
        "\n",
        "import random\n",
        "import numpy as np\n",
        "import gym\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import gymnasium as gym\n",
        "from gymnasium import spaces\n",
        "\n",
        "# ------------------------\n",
        "# 1) Dummy Env Definition\n",
        "# ------------------------\n",
        "class QuantumGravityEnv(gym.Env):\n",
        "    \"\"\"\n",
        "    4-D continuous state/action dummy env.\n",
        "    Reward = –‖state‖², state updates by action + noise.\n",
        "    \"\"\"\n",
        "    metadata = {\"render_modes\": []}\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.observation_space = spaces.Box(-10, 10, shape=(4,), dtype=np.float32)\n",
        "        self.action_space = spaces.Box(-1, 1, shape=(4,), dtype=np.float32)\n",
        "        self.state = np.zeros(4, dtype=np.float32)\n",
        "\n",
        "    def reset(self, seed=None, options=None):\n",
        "        super().reset(seed=seed)\n",
        "        self.state = self.np_random.uniform(-1, 1, size=4).astype(np.float32)\n",
        "        # Gymnasium reset returns (obs, info)\n",
        "        return self.state, {}\n",
        "\n",
        "    def step(self, action):\n",
        "        # next state, reward, terminated/truncated, info\n",
        "        self.state = self.state + action + 0.1 * self.np_random.standard_normal(4).astype(np.float32)\n",
        "        reward = -np.linalg.norm(self.state)**2\n",
        "        terminated = False # no natural termination in dummy\n",
        "        truncated = False # no time limit wrapper\n",
        "        return self.state, reward, terminated, truncated, {}\n",
        "\n",
        "# ------------------------\n",
        "# 2) Replay Buffer\n",
        "# ------------------------\n",
        "class ReplayBuffer:\n",
        "    def __init__(self, capacity=100000):\n",
        "        self.buffer = []\n",
        "        self.cap    = capacity\n",
        "\n",
        "    def push(self, s, a, r, s2):\n",
        "        if len(self.buffer) >= self.cap:\n",
        "            self.buffer.pop(0)\n",
        "        self.buffer.append((s, a, r, s2))\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        batch = random.sample(self.buffer, batch_size)\n",
        "        s, a, r, s2 = map(np.stack, zip(*batch))\n",
        "        return s, a, r, s2\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.buffer)\n",
        "\n",
        "# ------------------------\n",
        "# 3) Network Definitions\n",
        "# ------------------------\n",
        "class QuantumGravityAI(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "        super().__init__()\n",
        "        self.fc1  = nn.Linear(input_dim, hidden_dim)\n",
        "        self.fc2  = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        return torch.tanh(self.fc2(x))  # ensure actions ∈ [-1,1]\n",
        "\n",
        "class Critic(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim, hidden_dim):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(state_dim + action_dim, hidden_dim)\n",
        "        self.fc2 = nn.Linear(hidden_dim, 1)\n",
        "\n",
        "    def forward(self, s, a):\n",
        "        x = torch.relu(self.fc1(torch.cat([s, a], dim=-1)))\n",
        "        return self.fc2(x)\n",
        "\n",
        "# ------------------------\n",
        "# 4) DDPG Agent\n",
        "# ------------------------\n",
        "class DDPGAgent:\n",
        "    def __init__(self, env):\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        s_dim = env.observation_space.shape[0]\n",
        "        a_dim = env.action_space.shape[0]\n",
        "\n",
        "        # actor & critic + targets\n",
        "        self.actor        = QuantumGravityAI(s_dim, 16, a_dim).to(self.device)\n",
        "        self.actor_target = QuantumGravityAI(s_dim, 16, a_dim).to(self.device)\n",
        "        self.critic       = Critic(s_dim, a_dim, 16).to(self.device)\n",
        "        self.critic_target= Critic(s_dim, a_dim, 16).to(self.device)\n",
        "\n",
        "        # copy initial weights\n",
        "        self.actor_target.load_state_dict(self.actor.state_dict())\n",
        "        self.critic_target.load_state_dict(self.critic.state_dict())\n",
        "\n",
        "        # optimizers\n",
        "        self.a_opt = optim.Adam(self.actor.parameters(), lr=1e-3)\n",
        "        self.c_opt = optim.Adam(self.critic.parameters(), lr=1e-3)\n",
        "\n",
        "        self.buffer = ReplayBuffer(50000)\n",
        "        self.gamma  = 0.99\n",
        "        self.tau    = 0.005\n",
        "\n",
        "    def select_action(self, state, noise_scale=0.1):\n",
        "        s = torch.FloatTensor(state).to(self.device).unsqueeze(0)\n",
        "        a = self.actor(s).cpu().detach().numpy()[0]\n",
        "        return np.clip(a + noise_scale * np.random.randn(*a.shape), -1, 1)\n",
        "\n",
        "    def update(self, batch_size=64):\n",
        "        if len(self.buffer) < batch_size:\n",
        "            return\n",
        "\n",
        "        s, a, r, s2 = self.buffer.sample(batch_size)\n",
        "        s  = torch.FloatTensor(s).to(self.device)\n",
        "        a  = torch.FloatTensor(a).to(self.device)\n",
        "        r  = torch.FloatTensor(r).unsqueeze(1).to(self.device)\n",
        "        s2 = torch.FloatTensor(s2).to(self.device)\n",
        "\n",
        "        # Critic update\n",
        "        with torch.no_grad():\n",
        "            a2 = self.actor_target(s2)\n",
        "            q2 = self.critic_target(s2, a2)\n",
        "            y  = r + self.gamma * q2\n",
        "        q1 = self.critic(s, a)\n",
        "        c_loss = nn.MSELoss()(q1, y)\n",
        "        self.c_opt.zero_grad()\n",
        "        c_loss.backward()\n",
        "        self.c_opt.step()\n",
        "\n",
        "        # Actor update\n",
        "        a_pred = self.actor(s)\n",
        "        a_loss = -self.critic(s, a_pred).mean()\n",
        "        self.a_opt.zero_grad()\n",
        "        a_loss.backward()\n",
        "        self.a_opt.step()\n",
        "\n",
        "        # Safe soft-update (out-of-place ops)\n",
        "        with torch.no_grad():\n",
        "            for p, pt in zip(self.actor.parameters(), self.actor_target.parameters()):\n",
        "                new_val = pt.data * (1 - self.tau) + p.data * self.tau\n",
        "                pt.data.copy_(new_val)\n",
        "            for p, pt in zip(self.critic.parameters(), self.critic_target.parameters()):\n",
        "                new_val = pt.data * (1 - self.tau) + p.data * self.tau\n",
        "                pt.data.copy_(new_val)\n",
        "\n",
        "    def store(self, *args):\n",
        "        self.buffer.push(*args)\n",
        "\n",
        "# ------------------------\n",
        "# 5) Training Loop\n",
        "# ------------------------\n",
        "def train():\n",
        "    env   = QuantumGravityEnv()\n",
        "    agent = DDPGAgent(env)\n",
        "    episodes = 200\n",
        "\n",
        "    for ep in range(1, episodes + 1):\n",
        "        state, info = env.reset(seed=ep)\n",
        "        ep_reward = 0.0\n",
        "\n",
        "        for t in range(100):\n",
        "            action = agent.select_action(state)\n",
        "            next_state, reward, terminated, truncated, info = env.step(action)\n",
        "\n",
        "            done = terminated or truncated\n",
        "            agent.store(state, action, reward, next_state)\n",
        "            agent.update()\n",
        "\n",
        "            state = next_state\n",
        "            ep_reward += reward\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "        if ep % 10 == 0:\n",
        "            print(f\"Episode {ep:03d} → Reward: {ep_reward:.2f}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    train()"
      ],
      "metadata": {
        "id": "wpKBV9BDaN8a"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}