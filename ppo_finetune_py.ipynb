{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyMypG8ZK6ALKaBp7EpdT2c4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OneFineStarstuff/Cosmic-Brilliance/blob/main/ppo_finetune_py.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install trl"
      ],
      "metadata": {
        "id": "qZsfAmnnXuWu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T866BQ_eXf9e"
      },
      "outputs": [],
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "ppo_finetune.py\n",
        "\n",
        "Fine‑tune a causal language model with PPO using a pretrained reward model.\n",
        "\"\"\"\n",
        "\n",
        "from trl import PPOTrainer, PPOConfig, AutoModelForCausalLMWithValueHead\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "from datasets import load_dataset\n",
        "import torch\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "# -----------------------\n",
        "# Config & seeds\n",
        "# -----------------------\n",
        "BASE_MODEL_ID = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
        "REWARD_MODEL_DIR = \"rm_final\"\n",
        "UNLABELED_CONTEXTS_FILE = \"unlabeled_contexts.jsonl\"\n",
        "OUTPUT_DIR = \"questioner_ppo\"\n",
        "\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "# -----------------------\n",
        "# Load policy & tokenizer\n",
        "# -----------------------\n",
        "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_ID)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "policy = AutoModelForCausalLMWithValueHead.from_pretrained(\n",
        "    BASE_MODEL_ID,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "# Optionally reduce memory usage\n",
        "policy.gradient_checkpointing_enable()\n",
        "\n",
        "# -----------------------\n",
        "# Load reward model\n",
        "# -----------------------\n",
        "reward_model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    REWARD_MODEL_DIR,\n",
        "    device_map=\"auto\"\n",
        ").eval()\n",
        "\n",
        "# -----------------------\n",
        "# PPO config & trainer\n",
        "# -----------------------\n",
        "ppo_config = PPOConfig(\n",
        "    batch_size=128,\n",
        "    ppo_epochs=4,\n",
        "    learning_rate=1.1e-6,\n",
        "    clip_range=0.2,\n",
        "    target_kl=0.1,\n",
        ")\n",
        "\n",
        "trainer = PPOTrainer(\n",
        "    config=ppo_config,\n",
        "    model=policy,\n",
        "    tokenizer=tokenizer,\n",
        "    reward_model=reward_model\n",
        ")\n",
        "\n",
        "# -----------------------\n",
        "# Prompt formatting\n",
        "# -----------------------\n",
        "def format_prompt(example):\n",
        "    return f\"\"\"You are an AI generating deep, open-ended questions.\n",
        "\n",
        "Context:\n",
        "{example['context']}\n",
        "\n",
        "Question:\"\"\"\n",
        "\n",
        "# -----------------------\n",
        "# Load unlabeled contexts\n",
        "# -----------------------\n",
        "dataset = load_dataset(\"json\", data_files=UNLABELED_CONTEXTS_FILE)[\"train\"]\n",
        "\n",
        "# -----------------------\n",
        "# PPO training loop\n",
        "# -----------------------\n",
        "for epoch in range(3):\n",
        "    batch = random.sample(list(dataset), ppo_config.batch_size)\n",
        "    prompts = [format_prompt(b) for b in batch]\n",
        "\n",
        "    # Generate candidate questions\n",
        "    gen_outputs = trainer.generate(\n",
        "        prompts,\n",
        "        max_new_tokens=64,\n",
        "        temperature=1.0,\n",
        "        top_p=0.95\n",
        "    )\n",
        "\n",
        "    questions = tokenizer.batch_decode(gen_outputs, skip_special_tokens=True)\n",
        "\n",
        "    # Score with reward model\n",
        "    rewards = []\n",
        "    with torch.no_grad():\n",
        "        for p, q in zip(prompts, questions):\n",
        "            inp = tokenizer(\n",
        "                f\"{p} {q}\",\n",
        "                return_tensors=\"pt\",\n",
        "                truncation=True,\n",
        "                max_length=128\n",
        "            ).to(policy.device)\n",
        "            reward = reward_model(**inp).logits.item()\n",
        "            rewards.append(reward)\n",
        "\n",
        "    # PPO update\n",
        "    trainer.step(prompts, gen_outputs, rewards)\n",
        "\n",
        "# -----------------------\n",
        "# Save final model\n",
        "# -----------------------\n",
        "policy.save_pretrained(OUTPUT_DIR)\n",
        "tokenizer.save_pretrained(OUTPUT_DIR)\n",
        "print(f\"PPO‑tuned model saved to {OUTPUT_DIR}\")"
      ]
    }
  ]
}