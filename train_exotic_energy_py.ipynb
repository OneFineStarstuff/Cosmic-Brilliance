{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyNLs4s/VMnQ7RHO0PbhW7/L",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OneFineStarstuff/Cosmic-Brilliance/blob/main/train_exotic_energy_py.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WFoB0A1dwsMz"
      },
      "outputs": [],
      "source": [
        "pip install torch numpy matplotlib"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "train_exotic_energy.py\n",
        "\n",
        "Physics-Informed AI pipeline for predicting optimized negative Casimir energy\n",
        "as a function of plate distance and permittivity.\n",
        "\n",
        "Pipeline steps:\n",
        " 1. Synthetic dataset of (distance, permittivity) → Casimir energy\n",
        " 2. Physics‐informed loss: supervised MSE + analytic Casimir residual\n",
        " 3. MLP with LayerNorm & Dropout for uncertainty estimation\n",
        " 4. MC‐Dropout inference to quantify predictive variance\n",
        " 5. Training loop with AdamW, ReduceLROnPlateau, early stopping\n",
        " 6. Visualizations: loss curves, true vs. predicted scatter\n",
        "\"\"\"\n",
        "\n",
        "import math\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 1. Synthetic Dataset\n",
        "# ------------------------------------------------------------------------------\n",
        "def casimir_effect(d, eps):\n",
        "    \"\"\"Analytic Casimir energy between parallel plates.\"\"\"\n",
        "    return -np.pi**2 / (240 * d**4) * eps\n",
        "\n",
        "class CasimirDataset(Dataset):\n",
        "    def __init__(self, n_samples=5000, seed=42):\n",
        "        np.random.seed(seed)\n",
        "        d   = np.random.uniform(0.1, 1.0, size=(n_samples,1)).astype(np.float32)\n",
        "        eps = np.random.uniform(1.0, 10.0, size=(n_samples,1)).astype(np.float32)\n",
        "        X_raw = np.hstack([d, eps])\n",
        "\n",
        "        Y_raw = casimir_effect(d, eps)\n",
        "        Y_raw += 0.01 * np.random.randn(*Y_raw.shape).astype(np.float32)\n",
        "\n",
        "        # Compute normalization statistics (NumPy)\n",
        "        self.X_mean, self.X_std = X_raw.mean(0), X_raw.std(0)\n",
        "        self.Y_mean, self.Y_std = Y_raw.mean(0), Y_raw.std(0)\n",
        "\n",
        "        # Normalize (NumPy)\n",
        "        self.X = (X_raw - self.X_mean) / self.X_std\n",
        "        self.Y = (Y_raw - self.Y_mean) / self.Y_std\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x = torch.from_numpy(self.X[idx])\n",
        "        y = torch.from_numpy(self.Y[idx])\n",
        "        return x, y\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 2. Model Architecture\n",
        "# ------------------------------------------------------------------------------\n",
        "class ExoticEnergyAI(nn.Module):\n",
        "    def __init__(self, input_dim=2, hidden_dims=(64,64), output_dim=1, p_drop=0.1):\n",
        "        super().__init__()\n",
        "        layers = []\n",
        "        dim = input_dim\n",
        "        for h in hidden_dims:\n",
        "            layers += [\n",
        "                nn.Linear(dim, h),\n",
        "                nn.LayerNorm(h),\n",
        "                nn.ReLU(),\n",
        "                nn.Dropout(p_drop)\n",
        "            ]\n",
        "            dim = h\n",
        "        layers.append(nn.Linear(dim, output_dim))\n",
        "        self.net = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 3. Physics‐Informed Loss\n",
        "# ------------------------------------------------------------------------------\n",
        "def physics_residual(pred, inp, stats):\n",
        "    \"\"\"\n",
        "    Enforce analytic Casimir relation:\n",
        "      E_true = -π² / (240 · d⁴) · ε\n",
        "    All operations in torch to avoid dtype mismatches.\n",
        "    \"\"\"\n",
        "    # Denormalize inputs (torch)\n",
        "    d   = inp[:, 0] * stats['X_std'][0] + stats['X_mean'][0]\n",
        "    eps = inp[:, 1] * stats['X_std'][1] + stats['X_mean'][1]\n",
        "\n",
        "    # True Casimir energy (torch)\n",
        "    E_true = - (math.pi ** 2) / 240.0 * eps / (d ** 4)\n",
        "\n",
        "    # Denormalize prediction\n",
        "    E_pred = pred.squeeze() * stats['Y_std'] + stats['Y_mean']\n",
        "\n",
        "    return nn.MSELoss()(E_pred, E_true)\n",
        "\n",
        "def total_loss(pred, true, inp, stats, lambda_phys=1.0):\n",
        "    mse_loss  = nn.MSELoss()(pred, true)\n",
        "    phys_loss = physics_residual(pred, inp, stats)\n",
        "    return mse_loss + lambda_phys * phys_loss, mse_loss, phys_loss\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 4. MC‐Dropout Inference\n",
        "# ------------------------------------------------------------------------------\n",
        "def mc_dropout_predict(model, x, n_samples=100):\n",
        "    model.train()\n",
        "    preds = []\n",
        "    with torch.no_grad():\n",
        "        for _ in range(n_samples):\n",
        "            preds.append(model(x).cpu().numpy())\n",
        "    arr = np.stack(preds, axis=0)\n",
        "    return arr.mean(0), arr.std(0)\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 5. Training Loop\n",
        "# ------------------------------------------------------------------------------\n",
        "def train(model, train_loader, val_loader, stats, device,\n",
        "          lr=1e-3, wd=1e-5, lambda_phys=1.0,\n",
        "          max_epochs=100, patience=10):\n",
        "    model.to(device)\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=wd)\n",
        "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
        "        optimizer, mode='min', factor=0.5, patience=5\n",
        "    )\n",
        "\n",
        "    best_val, wait = float('inf'), 0\n",
        "    history = {'train': [], 'val': []}\n",
        "\n",
        "    for epoch in range(1, max_epochs + 1):\n",
        "        # Training\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "        for xb, yb in train_loader:\n",
        "            xb, yb = xb.to(device), yb.to(device)\n",
        "            pred = model(xb)\n",
        "            loss, _, _ = total_loss(pred, yb, xb, stats, lambda_phys)\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            train_loss += loss.item() * xb.size(0)\n",
        "        train_loss /= len(train_loader.dataset)\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        with torch.no_grad():\n",
        "            for xb, yb in val_loader:\n",
        "                xb, yb = xb.to(device), yb.to(device)\n",
        "                pred = model(xb)\n",
        "                loss, _, _ = total_loss(pred, yb, xb, stats, lambda_phys)\n",
        "                val_loss += loss.item() * xb.size(0)\n",
        "        val_loss /= len(val_loader.dataset)\n",
        "\n",
        "        scheduler.step(val_loss)\n",
        "        history['train'].append(train_loss)\n",
        "        history['val'].append(val_loss)\n",
        "        print(f\"Epoch {epoch:03d} | Train {train_loss:.4e} | Val {val_loss:.4e}\")\n",
        "\n",
        "        # Early stopping\n",
        "        if val_loss < best_val - 1e-6:\n",
        "            best_val, wait = val_loss, 0\n",
        "            torch.save(model.state_dict(), \"best_exotic_energy_ai.pth\")\n",
        "        else:\n",
        "            wait += 1\n",
        "            if wait >= patience:\n",
        "                print(f\"Early stopping at epoch {epoch}\")\n",
        "                break\n",
        "\n",
        "    model.load_state_dict(torch.load(\"best_exotic_energy_ai.pth\"))\n",
        "    return history\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 6. Visualization Helpers\n",
        "# ------------------------------------------------------------------------------\n",
        "def plot_losses(history):\n",
        "    plt.figure()\n",
        "    plt.plot(history['train'], label='Train')\n",
        "    plt.plot(history['val'],   label='Val')\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.legend()\n",
        "    plt.title(\"Training Curve\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def plot_scatter(y_true, y_pred):\n",
        "    plt.figure()\n",
        "    plt.scatter(y_true, y_pred, s=5, alpha=0.6)\n",
        "    m, M = y_true.min(), y_true.max()\n",
        "    plt.plot([m, M], [m, M], 'r--')\n",
        "    plt.xlabel(\"True Energy\")\n",
        "    plt.ylabel(\"Pred Energy\")\n",
        "    plt.title(\"True vs Predicted Casimir Energy\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 7. Main Execution\n",
        "# ------------------------------------------------------------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # Prepare dataset and normalization stats\n",
        "    ds = CasimirDataset(n_samples=8000)\n",
        "    stats = {\n",
        "        'X_mean': torch.tensor(ds.X_mean, device=device),\n",
        "        'X_std':  torch.tensor(ds.X_std,  device=device),\n",
        "        'Y_mean': torch.tensor(ds.Y_mean, device=device),\n",
        "        'Y_std':  torch.tensor(ds.Y_std,  device=device),\n",
        "    }\n",
        "\n",
        "    # Train/validation split\n",
        "    n_val = int(0.2 * len(ds))\n",
        "    n_trn = len(ds) - n_val\n",
        "    trn_ds, val_ds = random_split(ds, [n_trn, n_val])\n",
        "    trn_loader = DataLoader(trn_ds, batch_size=128, shuffle=True)\n",
        "    val_loader = DataLoader(val_ds, batch_size=256)\n",
        "\n",
        "    # Initialize and train model\n",
        "    model   = ExoticEnergyAI().to(device)\n",
        "    history = train(model, trn_loader, val_loader, stats, device)\n",
        "\n",
        "    # Plot training history\n",
        "    plot_losses(history)\n",
        "\n",
        "    # Evaluate on full dataset\n",
        "    X_all = torch.from_numpy(ds.X).to(device).float()\n",
        "    with torch.no_grad():\n",
        "        y_pred_norm = model(X_all).cpu().numpy().squeeze()\n",
        "\n",
        "    # Denormalize for final scatter\n",
        "    y_true = (ds.Y * ds.Y_std + ds.Y_mean).squeeze()\n",
        "    y_pred = y_pred_norm * ds.Y_std + ds.Y_mean\n",
        "    plot_scatter(y_true, y_pred)"
      ],
      "metadata": {
        "id": "T_IzKiGnw6zA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}