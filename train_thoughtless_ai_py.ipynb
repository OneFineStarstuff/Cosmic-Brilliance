{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyOxSuNg+eltrRXWGM26LJfF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OneFineStarstuff/Cosmic-Brilliance/blob/main/train_thoughtless_ai_py.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SADj_YHQdn8S"
      },
      "outputs": [],
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "train_thoughtless_ai.py\n",
        "\n",
        "1. Synthetic dataset: 6 inputs → 3 targets\n",
        "2. Float32 normalization\n",
        "3. MLP with Dropout for MC‐Dropout uncertainty\n",
        "4. Physics-informed residual enforcing toy laws\n",
        "5. Combined loss: MSE + residual\n",
        "6. Training: AdamW, LR scheduler, grad clipping, early stopping\n",
        "7. Checkpointing & plotting: loss, parity, uncertainty\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# 1. Synthetic Dataset\n",
        "# -----------------------------------------------------------------------------\n",
        "class ThoughtlessDataset(Dataset):\n",
        "    def __init__(self, n_samples=5000, seed=123):\n",
        "        np.random.seed(seed)\n",
        "        # Inputs: cognitive_flux, intelligence_emerge, pre_thought, idle_rational, meta_cog, latent_pot\n",
        "        CF = np.random.uniform(0.0, 2.0, (n_samples,1))\n",
        "        IE = np.random.uniform(0.1, 5.0, (n_samples,1))\n",
        "        PT = np.random.uniform(-2.0, 2.0,(n_samples,1))\n",
        "        IR = np.random.uniform(0.0, 3.0,(n_samples,1))\n",
        "        MC = np.random.uniform(-1.0, 1.0,(n_samples,1))\n",
        "        LP = np.random.uniform(0.5, 2.0,(n_samples,1))\n",
        "        X_raw = np.hstack([CF, IE, PT, IR, MC, LP]).astype(np.float32)\n",
        "\n",
        "        # Toy ground truth:\n",
        "        # pre_coh   = CF * IE / (|PT| + eps)\n",
        "        # post_res  = IR * sin(MC)\n",
        "        # undef_eq  = LP * (pre_coh + post_res)\n",
        "        eps = 1e-6\n",
        "        pre_coh  = CF * IE / (np.abs(PT) + eps)\n",
        "        post_res = IR * np.sin(MC)\n",
        "        undef_eq = LP * (pre_coh + post_res)\n",
        "        Y_raw = np.hstack([pre_coh, post_res, undef_eq]).astype(np.float32)\n",
        "\n",
        "        # Add small Gaussian noise\n",
        "        noise = 0.01 * Y_raw.std(axis=0) * np.random.randn(*Y_raw.shape).astype(np.float32)\n",
        "        Y_raw += noise\n",
        "\n",
        "        # Compute and store normalization stats\n",
        "        self.X_mean = X_raw.mean(0).astype(np.float32)\n",
        "        self.X_std  = X_raw.std(0).astype(np.float32) + eps\n",
        "        self.Y_mean = Y_raw.mean(0).astype(np.float32)\n",
        "        self.Y_std  = Y_raw.std(0).astype(np.float32) + eps\n",
        "\n",
        "        # Normalize\n",
        "        self.X = ((X_raw - self.X_mean) / self.X_std).astype(np.float32)\n",
        "        self.Y = ((Y_raw - self.Y_mean) / self.Y_std).astype(np.float32)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return (\n",
        "            torch.from_numpy(self.X[idx]),\n",
        "            torch.from_numpy(self.Y[idx])\n",
        "        )\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# 2. Model with Dropout\n",
        "# -----------------------------------------------------------------------------\n",
        "class ThoughtlessAI(nn.Module):\n",
        "    def __init__(self, input_dim=6, hidden_dim=32, output_dim=3, p_drop=0.1):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(input_dim, hidden_dim),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(p_drop),\n",
        "            nn.Linear(hidden_dim, output_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# 3. Physics‐Informed Residual\n",
        "# -----------------------------------------------------------------------------\n",
        "def physics_residual(pred, X, stats):\n",
        "    X_den = X * stats['X_std'] + stats['X_mean']\n",
        "    CF, IE, PT, IR, MC, LP = X_den.t()\n",
        "    eps = 1e-6\n",
        "\n",
        "    pre_coh_t   = CF * IE / (PT.abs() + eps)\n",
        "    post_res_t  = IR * torch.sin(MC)\n",
        "    undef_eq_t  = LP * (pre_coh_t + post_res_t)\n",
        "\n",
        "    Y_true = torch.stack([pre_coh_t, post_res_t, undef_eq_t], dim=1)\n",
        "    Y_norm = (Y_true - stats['Y_mean']) / stats['Y_std']\n",
        "    return nn.MSELoss()(pred, Y_norm)\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# 4. Combined Loss\n",
        "# -----------------------------------------------------------------------------\n",
        "def total_loss(pred, target, X, stats, lam=1.0):\n",
        "    mse  = nn.MSELoss()(pred, target)\n",
        "    phys = physics_residual(pred, X, stats)\n",
        "    return mse + lam * phys, mse, phys\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# 5. MC‐Dropout Uncertainty\n",
        "# -----------------------------------------------------------------------------\n",
        "def mc_dropout_predict(model, X, T=50):\n",
        "    model.train()  # dropout active\n",
        "    preds = []\n",
        "    with torch.no_grad():\n",
        "        for _ in range(T):\n",
        "            preds.append(model(X))\n",
        "    stacked = torch.stack(preds)\n",
        "    return stacked.mean(0), stacked.std(0)\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# 6. Training Loop\n",
        "# -----------------------------------------------------------------------------\n",
        "def train(model, tr_dl, va_dl, stats, device,\n",
        "          lr=1e-3, wd=1e-5, lam=1.0,\n",
        "          epochs=100, patience=10):\n",
        "    model.to(device)\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=wd)\n",
        "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.5, patience=5)\n",
        "\n",
        "    best_val = float('inf')\n",
        "    wait     = 0\n",
        "    history  = {'train': [], 'val': []}\n",
        "\n",
        "    for ep in range(1, epochs+1):\n",
        "        # Train\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "        for Xb, Yb in tr_dl:\n",
        "            Xb, Yb = Xb.to(device), Yb.to(device)\n",
        "            pred = model(Xb)\n",
        "            loss, _, _ = total_loss(pred, Yb, Xb, stats, lam)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "            optimizer.step()\n",
        "            train_loss += loss.item() * Xb.size(0)\n",
        "        train_loss /= len(tr_dl.dataset)\n",
        "\n",
        "        # Validate\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        with torch.no_grad():\n",
        "            for Xv, Yv in va_dl:\n",
        "                Xv, Yv = Xv.to(device), Yv.to(device)\n",
        "                pred = model(Xv)\n",
        "                loss, _, _ = total_loss(pred, Yv, Xv, stats, lam)\n",
        "                val_loss += loss.item() * Xv.size(0)\n",
        "        val_loss /= len(va_dl.dataset)\n",
        "\n",
        "        scheduler.step(val_loss)\n",
        "        history['train'].append(train_loss)\n",
        "        history['val'].append(val_loss)\n",
        "        print(f\"Epoch {ep:03d} | Train {train_loss:.4e} | Val {val_loss:.4e}\")\n",
        "\n",
        "        # Checkpoint\n",
        "        if val_loss < best_val - 1e-6:\n",
        "            best_val = val_loss\n",
        "            wait = 0\n",
        "            torch.save(model.state_dict(), \"best_thoughtless_ai.pth\")\n",
        "        else:\n",
        "            wait += 1\n",
        "            if wait >= patience:\n",
        "                print(\"Early stopping.\")\n",
        "                break\n",
        "\n",
        "    model.load_state_dict(torch.load(\"best_thoughtless_ai.pth\", map_location=device))\n",
        "    return history\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# 7. Visualization\n",
        "# -----------------------------------------------------------------------------\n",
        "def plot_history(h):\n",
        "    plt.figure()\n",
        "    plt.plot(h['train'], label='Train')\n",
        "    plt.plot(h['val'],   label='Val')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "def plot_parity(model, ds, stats, device):\n",
        "    model.eval()\n",
        "    X = torch.from_numpy(ds.X).to(device)\n",
        "    with torch.no_grad():\n",
        "        P = model(X).cpu().numpy()\n",
        "    Y_true = (ds.Y * ds.Y_std + ds.Y_mean)\n",
        "    P_denorm = P * ds.Y_std + ds.Y_mean\n",
        "\n",
        "    plt.figure(figsize=(6,6))\n",
        "    plt.scatter(Y_true.ravel(), P_denorm.ravel(), s=4, alpha=0.5)\n",
        "    m, M = Y_true.min(), Y_true.max()\n",
        "    plt.plot([m, M], [m, M], 'r--')\n",
        "    plt.xlabel(\"True\")\n",
        "    plt.ylabel(\"Pred\")\n",
        "    plt.show()\n",
        "\n",
        "def plot_uncertainty(model, ds, stats, device):\n",
        "    X = torch.from_numpy(ds.X).to(device)\n",
        "    _, std = mc_dropout_predict(model, X, T=50)\n",
        "    u = std[:,0].cpu().numpy()\n",
        "    plt.hist(u, bins=30, color='teal')\n",
        "    plt.xlabel('Stddev of Pre-Thought Pred.')\n",
        "    plt.show()\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# 8. Main Execution\n",
        "# -----------------------------------------------------------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # Dataset and stats\n",
        "    ds = ThoughtlessDataset(n_samples=5000)\n",
        "    stats = {\n",
        "        'X_mean': torch.tensor(ds.X_mean, dtype=torch.float32, device=device),\n",
        "        'X_std':  torch.tensor(ds.X_std,  dtype=torch.float32, device=device),\n",
        "        'Y_mean': torch.tensor(ds.Y_mean, dtype=torch.float32, device=device),\n",
        "        'Y_std':  torch.tensor(ds.Y_std,  dtype=torch.float32, device=device),\n",
        "    }\n",
        "\n",
        "    # Split and loaders\n",
        "    val_count = int(len(ds) * 0.2)\n",
        "    tr_ds, va_ds = random_split(ds, [len(ds)-val_count, val_count])\n",
        "    tr_dl = DataLoader(tr_ds, batch_size=128, shuffle=True)\n",
        "    va_dl = DataLoader(va_ds, batch_size=256, shuffle=False)\n",
        "\n",
        "    # Train and visualize\n",
        "    model   = ThoughtlessAI()\n",
        "    history = train(model, tr_dl, va_dl, stats, device)\n",
        "\n",
        "    plot_history(history)\n",
        "    plot_parity(model, ds, stats, device)\n",
        "    plot_uncertainty(model, ds, stats, device)"
      ]
    }
  ]
}