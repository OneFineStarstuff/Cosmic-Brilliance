{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyPKDvH4b0sIbx0J80R5SCy9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OneFineStarstuff/Cosmic-Brilliance/blob/main/train_thoughtless_ai_py.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8T259nvcyJqN"
      },
      "outputs": [],
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "train_thoughtless_ai.py\n",
        "\n",
        "Full pipeline for ThoughtlessAI:\n",
        "1. Synthetic “thoughtless” dataset of 6 inputs → 3 targets\n",
        "2. Float32 normalization\n",
        "3. MLP with LayerNorm, Dropout & ReLU\n",
        "4. Theory‐informed residual enforcing toy “meta‐intelligence” laws\n",
        "5. MC‐Dropout for uncertainty quantification\n",
        "6. Training loop with AdamW, ReduceLROnPlateau, gradient clipping, NaN checks, early stopping\n",
        "7. Safe checkpointing and reload\n",
        "8. Visualizations: loss curves, scatter plots, uncertainty heatmap\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 1. Synthetic Thoughtless Dataset\n",
        "# ------------------------------------------------------------------------------\n",
        "class ThoughtlessDataset(Dataset):\n",
        "    def __init__(self, n_samples=5000, seed=42):\n",
        "        np.random.seed(seed)\n",
        "        # Inputs:\n",
        "        # NLI: non‐logical intelligence factor ∈ [0,1]\n",
        "        # NCC: non‐causal computation measure ∈ [0,5]\n",
        "        # HPC1–HPC4: hyper‐consciousness params ∈ [−π,π]\n",
        "        NLI   = np.random.rand(n_samples,1)\n",
        "        NCC   = np.random.uniform(0, 5.0, (n_samples,1))\n",
        "        HPC1  = np.random.uniform(-np.pi, np.pi, (n_samples,1))\n",
        "        HPC2  = np.random.uniform(-np.pi, np.pi, (n_samples,1))\n",
        "        HPC3  = np.random.uniform(-np.pi, np.pi, (n_samples,1))\n",
        "        HPC4  = np.random.uniform(-np.pi, np.pi, (n_samples,1))\n",
        "\n",
        "        X_raw = np.hstack([NLI, NCC, HPC1, HPC2, HPC3, HPC4]).astype(np.float64)\n",
        "\n",
        "        # Toy meta‐intelligence targets:\n",
        "        # IC  = NLI * tanh(NCC + ε)\n",
        "        # MAB = sin(HPC1 * HPC2) / (1 + abs(HPC3))\n",
        "        # HCE = cos(HPC4) * NLI\n",
        "        eps = 1e-6\n",
        "        IC  = NLI * np.tanh(NCC + eps)\n",
        "        MAB = np.sin(HPC1 * HPC2) / (1 + np.abs(HPC3))\n",
        "        HCE = np.cos(HPC4) * NLI\n",
        "\n",
        "        Y_raw = np.hstack([IC, MAB, HCE]).astype(np.float64)\n",
        "        # add 1% relative noise\n",
        "        Y_raw += 0.01 * Y_raw.std(axis=0) * np.random.randn(*Y_raw.shape)\n",
        "\n",
        "        # Stats for normalization\n",
        "        self.X_mean = X_raw.mean(axis=0)\n",
        "        self.X_std  = X_raw.std(axis=0) + 1e-8\n",
        "        self.Y_mean = Y_raw.mean(axis=0)\n",
        "        self.Y_std  = Y_raw.std(axis=0) + 1e-8\n",
        "\n",
        "        # Normalize to float32\n",
        "        self.X = ((X_raw - self.X_mean) / self.X_std).astype(np.float32)\n",
        "        self.Y = ((Y_raw - self.Y_mean) / self.Y_std).astype(np.float32)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return (\n",
        "            torch.from_numpy(self.X[idx]),\n",
        "            torch.from_numpy(self.Y[idx])\n",
        "        )\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 2. ThoughtlessAI Model\n",
        "# ------------------------------------------------------------------------------\n",
        "class ThoughtlessAI(nn.Module):\n",
        "    def __init__(self, input_dim=6, hidden_dims=(64,64), output_dim=3, p_drop=0.1):\n",
        "        super().__init__()\n",
        "        layers, d = [], input_dim\n",
        "        for h in hidden_dims:\n",
        "            layers += [\n",
        "                nn.Linear(d, h),\n",
        "                nn.LayerNorm(h),\n",
        "                nn.ReLU(),\n",
        "                nn.Dropout(p_drop)\n",
        "            ]\n",
        "            d = h\n",
        "        layers.append(nn.Linear(d, output_dim))\n",
        "        self.net = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 3. Theory‐Informed Residual Loss\n",
        "# ------------------------------------------------------------------------------\n",
        "def theory_residual(pred, X, stats):\n",
        "    # Denormalize inputs\n",
        "    X_den = X * stats['X_std'] + stats['X_mean']\n",
        "    NLI, NCC, HPC1, HPC2, HPC3, HPC4 = X_den.t()\n",
        "    eps = 1e-6\n",
        "\n",
        "    IC_t  = NLI * torch.tanh(NCC + eps)\n",
        "    MAB_t = torch.sin(HPC1 * HPC2) / (1 + torch.abs(HPC3))\n",
        "    HCE_t = torch.cos(HPC4) * NLI\n",
        "\n",
        "    Yt = torch.stack([IC_t, MAB_t, HCE_t], dim=1)\n",
        "    Yt_n = (Yt - stats['Y_mean']) / stats['Y_std']\n",
        "    return nn.MSELoss()(pred, Yt_n)\n",
        "\n",
        "def total_loss(pred, true, X, stats, lam=1.0):\n",
        "    mse  = nn.MSELoss()(pred, true)\n",
        "    phys = theory_residual(pred, X, stats)\n",
        "    return mse + lam * phys, mse, phys\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 4. MC‐Dropout Uncertainty Quantification\n",
        "# ------------------------------------------------------------------------------\n",
        "def mc_dropout_predict(model, X, T=50):\n",
        "    model.train()\n",
        "    preds = []\n",
        "    with torch.no_grad():\n",
        "        for _ in range(T):\n",
        "            preds.append(model(X))\n",
        "    stacked = torch.stack(preds, dim=0)\n",
        "    return stacked.mean(0), stacked.std(0)\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 5. Training Loop with Safety & Checkpointing\n",
        "# ------------------------------------------------------------------------------\n",
        "def train(model, train_loader, val_loader, stats, device,\n",
        "          lr=1e-4, wd=1e-5, lam=1.0, epochs=100, patience=10):\n",
        "    model.to(device)\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=wd)\n",
        "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
        "        optimizer, mode='min', factor=0.5, patience=5\n",
        "    )\n",
        "\n",
        "    best_val, wait = float('inf'), 0\n",
        "    history = {'train': [], 'val': []}\n",
        "\n",
        "    for ep in range(1, epochs+1):\n",
        "        # Training step\n",
        "        model.train()\n",
        "        run = 0.0\n",
        "        for xb, yb in train_loader:\n",
        "            xb, yb = xb.to(device), yb.to(device)\n",
        "            pred = model(xb)\n",
        "            loss, _, _ = total_loss(pred, yb, xb, stats, lam)\n",
        "            if torch.isnan(loss):\n",
        "                print(f\"NaN at epoch {ep}, abort.\")\n",
        "                return history\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "            optimizer.step()\n",
        "            run += loss.item() * xb.size(0)\n",
        "        train_loss = run / len(train_loader.dataset)\n",
        "\n",
        "        # Validation step\n",
        "        model.eval()\n",
        "        run = 0.0\n",
        "        with torch.no_grad():\n",
        "            for xb, yb in val_loader:\n",
        "                xb, yb = xb.to(device), yb.to(device)\n",
        "                pred = model(xb)\n",
        "                l, _, _ = total_loss(pred, yb, xb, stats, lam)\n",
        "                run += l.item() * xb.size(0)\n",
        "        val_loss = run / len(val_loader.dataset)\n",
        "\n",
        "        scheduler.step(val_loss)\n",
        "        history['train'].append(train_loss)\n",
        "        history['val'].append(val_loss)\n",
        "        print(f\"Epoch {ep:03d} | Train {train_loss:.4e} | Val {val_loss:.4e}\")\n",
        "\n",
        "        # Checkpoint & early stop\n",
        "        if val_loss < best_val - 1e-6:\n",
        "            best_val, wait = val_loss, 0\n",
        "            torch.save(model.state_dict(), \"best_thoughtless_ai.pth\")\n",
        "        else:\n",
        "            wait += 1\n",
        "            if wait >= patience:\n",
        "                print(\"Early stopping.\")\n",
        "                break\n",
        "\n",
        "    # Safe reload\n",
        "    if os.path.exists(\"best_thoughtless_ai.pth\"):\n",
        "        model.load_state_dict(\n",
        "            torch.load(\"best_thoughtless_ai.pth\", map_location=device)\n",
        "        )\n",
        "    return history\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 6. Visualization Helpers\n",
        "# ------------------------------------------------------------------------------\n",
        "def plot_history(history):\n",
        "    plt.plot(history['train'], label='Train')\n",
        "    plt.plot(history['val'],   label='Val')\n",
        "    plt.xlabel(\"Epoch\"); plt.ylabel(\"Loss\"); plt.legend(); plt.show()\n",
        "\n",
        "def plot_scatter(y_true, y_pred, title):\n",
        "    plt.figure()\n",
        "    plt.scatter(y_true, y_pred, s=8, alpha=0.5)\n",
        "    m, M = y_true.min(), y_true.max()\n",
        "    plt.plot([m, M], [m, M], 'r--')\n",
        "    plt.title(title); plt.show()\n",
        "\n",
        "def plot_uncertainty_heatmap(model, stats, device):\n",
        "    # vary NLI vs. NCC; fix HPCs at mean\n",
        "    G = 100\n",
        "    NLI = np.linspace(0,1,G, dtype=np.float32)\n",
        "    NCC = np.linspace(0,5,G,   dtype=np.float32)\n",
        "    G1,G2 = np.meshgrid(NLI,NCC)\n",
        "    pts = G*G\n",
        "\n",
        "    # Build grid tensor\n",
        "    Xg = torch.zeros((pts,6), device=device, dtype=torch.float32)\n",
        "    # HPC1–4 means\n",
        "    Xg[:,2:] = stats['X_mean'][2:].unsqueeze(0).expand(pts,4)\n",
        "    Xg[:,0]  = torch.from_numpy(G1.ravel()).to(device)\n",
        "    Xg[:,1]  = torch.from_numpy(G2.ravel()).to(device)\n",
        "\n",
        "    # Normalize\n",
        "    Xn = (Xg - stats['X_mean']) / stats['X_std']\n",
        "    _, std = mc_dropout_predict(model, Xn, T=100)\n",
        "    U = std[:,0].cpu().reshape(G1.shape)\n",
        "\n",
        "    plt.figure(figsize=(5,4))\n",
        "    plt.pcolormesh(G1, G2, U, cmap='magma', shading='auto')\n",
        "    plt.colorbar(label=\"Std(IC)\")\n",
        "    plt.xlabel(\"NLI\"); plt.ylabel(\"NCC\")\n",
        "    plt.title(\"Uncertainty: Intelligence Coherence\")\n",
        "    plt.show()\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 7. Main Execution\n",
        "# ------------------------------------------------------------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    ds = ThoughtlessDataset(n_samples=5000, seed=42)\n",
        "    stats = {\n",
        "        'X_mean': torch.tensor(ds.X_mean, dtype=torch.float32, device=device),\n",
        "        'X_std' : torch.tensor(ds.X_std,  dtype=torch.float32, device=device),\n",
        "        'Y_mean': torch.tensor(ds.Y_mean, dtype=torch.float32, device=device),\n",
        "        'Y_std' : torch.tensor(ds.Y_std,  dtype=torch.float32, device=device),\n",
        "    }\n",
        "\n",
        "    # splits and loaders\n",
        "    n_val = int(0.2 * len(ds))\n",
        "    tr_ds, va_ds = random_split(ds, [len(ds)-n_val, n_val])\n",
        "    tr_ld = DataLoader(tr_ds, batch_size=128, shuffle=True)\n",
        "    va_ld = DataLoader(va_ds, batch_size=256, shuffle=False)\n",
        "\n",
        "    model   = ThoughtlessAI().to(device)\n",
        "    history = train(model, tr_ld, va_ld, stats, device)\n",
        "\n",
        "    plot_history(history)\n",
        "\n",
        "    # scatter true vs. pred\n",
        "    X_all = torch.from_numpy(ds.X).to(device)\n",
        "    with torch.no_grad():\n",
        "        Yp = model(X_all).cpu().numpy() * ds.Y_std + ds.Y_mean\n",
        "    Yt = ds.Y * ds.Y_std + ds.Y_mean\n",
        "    names = [\"Intelligence Coherence\",\"Meta-Awareness Balance\",\"Hyper-Conscious Evolution\"]\n",
        "    for i, nm in enumerate(names):\n",
        "        plot_scatter(Yt[:,i], Yp[:,i], nm)\n",
        "\n",
        "    plot_uncertainty_heatmap(model, stats, device)"
      ]
    }
  ]
}