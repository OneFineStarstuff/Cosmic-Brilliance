{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyMUe8Vg4EfCg2jXxZAavHpR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OneFineStarstuff/Cosmic-Brilliance/blob/main/colab_kernel_launcher_py.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-zRk7jBK17uX"
      },
      "outputs": [],
      "source": [
        "#!/usr/bin/env python3\n",
        "# -*- coding: utf-8 -*-\n",
        "\n",
        "\"\"\"\n",
        "colab_kernel_launcher.py\n",
        "\n",
        "Notebook/Colab-safe CLI training script with:\n",
        "- Modes: self, supervised, hybrid\n",
        "- Robust argparse that ignores Jupyter's injected -f kernel.json\n",
        "- Deterministic seeding\n",
        "- CSV logging, matplotlib plot, and model checkpoint\n",
        "- Simple synthetic dataset and a compact MLP classifier\n",
        "\"\"\"\n",
        "\n",
        "import argparse\n",
        "import csv\n",
        "import json\n",
        "import math\n",
        "import os\n",
        "import random\n",
        "import sys\n",
        "import time\n",
        "from dataclasses import asdict, dataclass\n",
        "from typing import List, Optional, Tuple\n",
        "\n",
        "# Optional plotting (gracefully skipped if matplotlib is not available)\n",
        "try:\n",
        "    import matplotlib.pyplot as plt\n",
        "    _HAS_MPL = True\n",
        "except Exception:\n",
        "    _HAS_MPL = False\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# Optional: If torch isn't available in your environment, install it first.\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "# ----------------------------\n",
        "# Utilities\n",
        "# ----------------------------\n",
        "\n",
        "def sanitize_argv(argv: Optional[List[str]] = None) -> List[str]:\n",
        "    \"\"\"\n",
        "    Remove Jupyter/Colab injected args like:\n",
        "      -f /root/.local/share/jupyter/runtime/kernel-xxxx.json\n",
        "    and any stray JSON path that might follow a flag.\n",
        "    \"\"\"\n",
        "    if argv is None:\n",
        "        argv = sys.argv[1:]\n",
        "    cleaned = []\n",
        "    skip_next = False\n",
        "    for i, a in enumerate(argv):\n",
        "        if skip_next:\n",
        "            skip_next = False\n",
        "            continue\n",
        "        if a == \"-f\":\n",
        "            # Skip the -f and its value (usually a kernel json)\n",
        "            skip_next = True\n",
        "            continue\n",
        "        if a.endswith(\".json\") and \"jupyter\" in a or \"kernel\" in a:\n",
        "            # Defensive: drop any lonely kernel json arg\n",
        "            continue\n",
        "        cleaned.append(a)\n",
        "    return cleaned\n",
        "\n",
        "\n",
        "def parse_args(argv: Optional[List[str]] = None) -> argparse.Namespace:\n",
        "    parser = argparse.ArgumentParser(\n",
        "        description=\"Notebook-safe training launcher with self/supervised/hybrid modes\"\n",
        "    )\n",
        "    parser.add_argument(\"--mode\", type=str, choices=[\"self\", \"supervised\", \"hybrid\"], default=\"supervised\",\n",
        "                        help=\"Training mode\")\n",
        "    parser.add_argument(\"--steps\", type=int, default=500, help=\"Number of optimization steps\")\n",
        "    parser.add_argument(\"--batch-size\", type=int, default=128, help=\"Batch size\")\n",
        "    parser.add_argument(\"--lr\", type=float, default=1e-3, help=\"Learning rate\")\n",
        "    parser.add_argument(\"--wd\", type=float, default=0.0, help=\"Weight decay\")\n",
        "    parser.add_argument(\"--entropy-bonus\", type=float, default=0.0,\n",
        "                        help=\"Coefficient for entropy bonus (encourages higher output entropy if > 0)\")\n",
        "    parser.add_argument(\"--label-sharpen\", type=float, default=0.0,\n",
        "                        help=\"Label smoothing factor in [0, 1). 0=one-hot, >0 mixes with uniform\")\n",
        "    parser.add_argument(\"--seed\", type=int, default=42, help=\"Random seed\")\n",
        "    parser.add_argument(\"--outdir\", type=str, default=\"runs\", help=\"Base output directory\")\n",
        "    parser.add_argument(\"--log-every\", type=int, default=50, help=\"Log every N steps\")\n",
        "    parser.add_argument(\"--cycle\", type=str, default=\"0\", help=\"Optional cycle tag for output grouping\")\n",
        "\n",
        "    # Notebook-safety: ignore unknowns to avoid SystemExit\n",
        "    cleaned = sanitize_argv(argv)\n",
        "    args, _unknown = parser.parse_known_args(cleaned)\n",
        "    return args\n",
        "\n",
        "\n",
        "def set_seed(seed: int) -> None:\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    # Determinism hints (may slightly reduce performance)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "\n",
        "def timestamp() -> str:\n",
        "    return time.strftime(\"%Y%m%d-%H%M%S\")\n",
        "\n",
        "\n",
        "def ensure_dir(path: str) -> None:\n",
        "    os.makedirs(path, exist_ok=True)\n",
        "\n",
        "\n",
        "# ----------------------------\n",
        "# Data generation (synthetic)\n",
        "# ----------------------------\n",
        "\n",
        "@dataclass\n",
        "class DataConfig:\n",
        "    dim: int = 2\n",
        "    n_classes: int = 2\n",
        "    radius: float = 3.0\n",
        "    spread: float = 1.0\n",
        "\n",
        "\n",
        "def sample_blobs(batch_size: int, cfg: DataConfig, device: torch.device) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "    \"\"\"\n",
        "    Create two Gaussian blobs in 2D (or higher dim with padding noise).\n",
        "    Returns:\n",
        "      x: [B, dim], y: [B]\n",
        "    \"\"\"\n",
        "    half = batch_size // 2\n",
        "    rem = batch_size - half\n",
        "\n",
        "    # Centers on a circle\n",
        "    angle0, angle1 = 0.0, math.pi\n",
        "    c0 = np.array([cfg.radius * math.cos(angle0), cfg.radius * math.sin(angle0)])\n",
        "    c1 = np.array([cfg.radius * math.cos(angle1), cfg.radius * math.sin(angle1)])\n",
        "\n",
        "    x0 = np.random.randn(half, 2) * cfg.spread + c0\n",
        "    x1 = np.random.randn(rem, 2) * cfg.spread + c1\n",
        "\n",
        "    x = np.concatenate([x0, x1], axis=0)\n",
        "    if cfg.dim > 2:\n",
        "        extra = np.random.randn(batch_size, cfg.dim - 2) * (0.5 * cfg.spread)\n",
        "        x = np.hstack([x, extra])\n",
        "\n",
        "    y = np.concatenate([np.zeros(half, dtype=np.int64), np.ones(rem, dtype=np.int64)], axis=0)\n",
        "\n",
        "    # Shuffle\n",
        "    idx = np.random.permutation(batch_size)\n",
        "    x = x[idx]\n",
        "    y = y[idx]\n",
        "\n",
        "    x = torch.tensor(x, dtype=torch.float32, device=device)\n",
        "    y = torch.tensor(y, dtype=torch.long, device=device)\n",
        "    return x, y\n",
        "\n",
        "\n",
        "def augment_noise(x: torch.Tensor, sigma: float = 0.2) -> torch.Tensor:\n",
        "    return x + sigma * torch.randn_like(x)\n",
        "\n",
        "\n",
        "# ----------------------------\n",
        "# Model\n",
        "# ----------------------------\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, in_dim: int, n_classes: int, hidden: int = 64):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(in_dim, hidden),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(hidden, hidden),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(hidden, n_classes),\n",
        "        )\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        return self.net(x)\n",
        "\n",
        "\n",
        "# ----------------------------\n",
        "# Loss helpers\n",
        "# ----------------------------\n",
        "\n",
        "def entropy_from_logits(logits: torch.Tensor) -> torch.Tensor:\n",
        "    probs = logits.softmax(dim=-1)\n",
        "    logp = (probs + 1e-12).log()\n",
        "    ent = -(probs * logp).sum(dim=-1)\n",
        "    return ent  # shape [B]\n",
        "\n",
        "\n",
        "def cross_entropy_soft_targets(logits: torch.Tensor, target_probs: torch.Tensor) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Cross-entropy with soft targets:\n",
        "      L = - sum_k target_probs[k] * log_softmax(logits)[k]\n",
        "    \"\"\"\n",
        "    logp = logits.log_softmax(dim=-1)\n",
        "    loss = -(target_probs * logp).sum(dim=-1).mean()\n",
        "    return loss\n",
        "\n",
        "\n",
        "def one_hot(num_classes: int, labels: torch.Tensor) -> torch.Tensor:\n",
        "    y = F.one_hot(labels, num_classes=num_classes).float()\n",
        "    return y\n",
        "\n",
        "\n",
        "def apply_label_smoothing(target_one_hot: torch.Tensor, smoothing: float) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Mix one-hot with uniform distribution. smoothing in [0, 1).\n",
        "      target = (1 - smoothing) * one_hot + smoothing * uniform\n",
        "    \"\"\"\n",
        "    if smoothing <= 0.0:\n",
        "        return target_one_hot\n",
        "    num_classes = target_one_hot.shape[-1]\n",
        "    uniform = torch.full_like(target_one_hot, 1.0 / num_classes)\n",
        "    return (1.0 - smoothing) * target_one_hot + smoothing * uniform\n",
        "\n",
        "\n",
        "def consistency_kl(logits_a: torch.Tensor, logits_b: torch.Tensor) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Symmetric KL between predictions of two augmentations.\n",
        "    \"\"\"\n",
        "    pa = logits_a.softmax(dim=-1).clamp_min(1e-8)\n",
        "    pb = logits_b.softmax(dim=-1).clamp_min(1e-8)\n",
        "    log_pa = pa.log()\n",
        "    log_pb = pb.log()\n",
        "    kl_ab = (pa * (log_pa - log_pb)).sum(dim=-1)\n",
        "    kl_ba = (pb * (log_pb - log_pa)).sum(dim=-1)\n",
        "    return 0.5 * (kl_ab + kl_ba).mean()\n",
        "\n",
        "\n",
        "# ----------------------------\n",
        "# Training\n",
        "# ----------------------------\n",
        "\n",
        "@dataclass\n",
        "class TrainConfig:\n",
        "    mode: str\n",
        "    steps: int\n",
        "    batch_size: int\n",
        "    lr: float\n",
        "    wd: float\n",
        "    entropy_bonus: float\n",
        "    label_sharpen: float\n",
        "    seed: int\n",
        "    outdir: str\n",
        "    log_every: int\n",
        "    cycle: str\n",
        "    dim: int = 2\n",
        "    n_classes: int = 2\n",
        "    device: str = \"cuda_if_available\"\n",
        "\n",
        "    def device_obj(self) -> torch.device:\n",
        "        if self.device == \"cuda_if_available\":\n",
        "            return torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        return torch.device(self.device)\n",
        "\n",
        "\n",
        "def train(cfg: TrainConfig) -> None:\n",
        "    set_seed(cfg.seed)\n",
        "\n",
        "    device = cfg.device_obj()\n",
        "    data_cfg = DataConfig(dim=cfg.dim, n_classes=cfg.n_classes)\n",
        "\n",
        "    # Output directory: runs/<cycle>/<timestamp>_<mode>_seed<seed>\n",
        "    base = os.path.join(cfg.outdir, f\"cycle-{cfg.cycle}\")\n",
        "    run_dir = os.path.join(base, f\"{timestamp()}_{cfg.mode}_seed{cfg.seed}\")\n",
        "    ensure_dir(run_dir)\n",
        "\n",
        "    # Save config\n",
        "    with open(os.path.join(run_dir, \"config.json\"), \"w\") as f:\n",
        "        json.dump(asdict(cfg), f, indent=2)\n",
        "\n",
        "    model = MLP(in_dim=cfg.dim, n_classes=cfg.n_classes).to(device)\n",
        "    opt = torch.optim.AdamW(model.parameters(), lr=cfg.lr, weight_decay=cfg.wd)\n",
        "\n",
        "    # Logging setup\n",
        "    csv_path = os.path.join(run_dir, \"metrics.csv\")\n",
        "    with open(csv_path, \"w\", newline=\"\") as f:\n",
        "        writer = csv.writer(f)\n",
        "        writer.writerow([\n",
        "            \"step\",\n",
        "            \"mode\",\n",
        "            \"loss_total\",\n",
        "            \"loss_sup\",\n",
        "            \"loss_ssl\",\n",
        "            \"entropy\",\n",
        "            \"lr\",\n",
        "        ])\n",
        "\n",
        "    loss_hist = []\n",
        "    sup_hist = []\n",
        "    ssl_hist = []\n",
        "    ent_hist = []\n",
        "\n",
        "    for step in range(1, cfg.steps + 1):\n",
        "        model.train()\n",
        "        x, y = sample_blobs(cfg.batch_size, data_cfg, device)\n",
        "\n",
        "        logits = model(x)\n",
        "\n",
        "        loss_sup = torch.tensor(0.0, device=device)\n",
        "        loss_ssl = torch.tensor(0.0, device=device)\n",
        "\n",
        "        # Supervised branch\n",
        "        if cfg.mode in (\"supervised\", \"hybrid\"):\n",
        "            y_1h = one_hot(cfg.n_classes, y)\n",
        "            y_soft = apply_label_smoothing(y_1h, cfg.label_sharpen)\n",
        "            loss_sup = cross_entropy_soft_targets(logits, y_soft)\n",
        "\n",
        "        # Self-supervised branch (consistency across two noisy views)\n",
        "        if cfg.mode in (\"self\", \"hybrid\"):\n",
        "            x_a = augment_noise(x, 0.25)\n",
        "            x_b = augment_noise(x, 0.25)\n",
        "            logits_a = model(x_a)\n",
        "            logits_b = model(x_b)\n",
        "            loss_ssl = consistency_kl(logits_a, logits_b)\n",
        "\n",
        "        # Entropy bonus (encourage exploration by increasing output entropy)\n",
        "        ent = entropy_from_logits(logits).mean()\n",
        "        total = loss_sup + loss_ssl - cfg.entropy_bonus * ent\n",
        "\n",
        "        opt.zero_grad(set_to_none=True)\n",
        "        total.backward()\n",
        "        opt.step()\n",
        "\n",
        "        # Log\n",
        "        if (step % cfg.log_every) == 0 or step == 1 or step == cfg.steps:\n",
        "            lr_cur = opt.param_groups[0][\"lr\"]\n",
        "            loss_hist.append(float(total.detach().cpu()))\n",
        "            sup_hist.append(float(loss_sup.detach().cpu()))\n",
        "            ssl_hist.append(float(loss_ssl.detach().cpu()))\n",
        "            ent_hist.append(float(ent.detach().cpu()))\n",
        "\n",
        "            with open(csv_path, \"a\", newline=\"\") as f:\n",
        "                writer = csv.writer(f)\n",
        "                writer.writerow([\n",
        "                    step, cfg.mode,\n",
        "                    f\"{loss_hist[-1]:.6f}\",\n",
        "                    f\"{sup_hist[-1]:.6f}\",\n",
        "                    f\"{ssl_hist[-1]:.6f}\",\n",
        "                    f\"{ent_hist[-1]:.6f}\",\n",
        "                    f\"{lr_cur:.8f}\",\n",
        "                ])\n",
        "\n",
        "            print(f\"[{step:5d}/{cfg.steps}] \"\n",
        "                  f\"mode={cfg.mode} \"\n",
        "                  f\"loss={loss_hist[-1]:.4f} \"\n",
        "                  f\"sup={sup_hist[-1]:.4f} \"\n",
        "                  f\"ssl={ssl_hist[-1]:.4f} \"\n",
        "                  f\"ent={ent_hist[-1]:.4f} \"\n",
        "                  f\"lr={lr_cur:.2e}\")\n",
        "\n",
        "    # Save model\n",
        "    ckpt_path = os.path.join(run_dir, \"model.pt\")\n",
        "    torch.save({\"model_state\": model.state_dict(), \"config\": asdict(cfg)}, ckpt_path)\n",
        "\n",
        "    # Plot metrics\n",
        "    if _HAS_MPL and len(loss_hist) > 0:\n",
        "        fig, ax = plt.subplots(1, 1, figsize=(7, 4))\n",
        "        xs = list(range(1, len(loss_hist) + 1))\n",
        "        ax.plot(xs, loss_hist, label=\"total\")\n",
        "        ax.plot(xs, sup_hist, label=\"supervised\")\n",
        "        ax.plot(xs, ssl_hist, label=\"self\")\n",
        "        ax.plot(xs, ent_hist, label=\"entropy\")\n",
        "        ax.set_title(f\"Training - {cfg.mode}\")\n",
        "        ax.set_xlabel(f\"Logged steps (every {cfg.log_every})\")\n",
        "        ax.set_ylabel(\"Value\")\n",
        "        ax.grid(True, alpha=0.3)\n",
        "        ax.legend()\n",
        "        fig.tight_layout()\n",
        "        fig_path = os.path.join(run_dir, \"metrics.png\")\n",
        "        fig.savefig(fig_path, dpi=140)\n",
        "        plt.close(fig)\n",
        "\n",
        "    print(f\"Done. Outputs saved to: {run_dir}\")\n",
        "\n",
        "\n",
        "# ----------------------------\n",
        "# Entry points\n",
        "# ----------------------------\n",
        "\n",
        "def main(argv: Optional[List[str]] = None) -> None:\n",
        "    args = parse_args(argv)\n",
        "    cfg = TrainConfig(\n",
        "        mode=args.mode,\n",
        "        steps=args.steps,\n",
        "        batch_size=args.batch_size,\n",
        "        lr=args.lr,\n",
        "        wd=args.wd,\n",
        "        entropy_bonus=args.entropy_bonus,\n",
        "        label_sharpen=args.label_sharpen,\n",
        "        seed=args.seed,\n",
        "        outdir=args.outdir,\n",
        "        log_every=args.log_every,\n",
        "        cycle=str(args.cycle),\n",
        "    )\n",
        "    train(cfg)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # This is safe in notebooks too; sanitize_argv prevents -f kernel.json crashes.\n",
        "    try:\n",
        "        main()\n",
        "    except SystemExit as e:\n",
        "        # Mirror IPython friendliness\n",
        "        print(\"Use 'exit', 'quit', or Ctrl-D to exit.\", file=sys.stderr)\n",
        "        raise"
      ]
    }
  ]
}