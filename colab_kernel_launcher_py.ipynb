{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyPypgGMpob4jgow/1XpbIGX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OneFineStarstuff/Cosmic-Brilliance/blob/main/colab_kernel_launcher_py.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kxgGp-begFA6"
      },
      "outputs": [],
      "source": [
        "#!/usr/bin/env python3\n",
        "# filename: colab_kernel_launcher.py\n",
        "\n",
        "import argparse\n",
        "import os\n",
        "import json\n",
        "import math\n",
        "import random\n",
        "from typing import Tuple\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "\n",
        "def str2bool(v: str) -> bool:\n",
        "    if isinstance(v, bool):\n",
        "        return v\n",
        "    v = v.strip().lower()\n",
        "    if v in (\"yes\", \"true\", \"t\", \"1\", \"y\"):\n",
        "        return True\n",
        "    if v in (\"no\", \"false\", \"f\", \"0\", \"n\"):\n",
        "        return False\n",
        "    raise argparse.ArgumentTypeError(f\"Boolean value expected for --save_model, got: {v}\")\n",
        "\n",
        "\n",
        "def set_seed(seed: int = 42) -> None:\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)  # safe even if no cuda\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "\n",
        "def make_synthetic_classification(\n",
        "    n_samples: int,\n",
        "    n_features: int = 3,\n",
        "    random_state: int = 42,\n",
        "    noise: float = 0.5,\n",
        ") -> Tuple[np.ndarray, np.ndarray]:\n",
        "    \"\"\"\n",
        "    Generate a simple, linearly-separable-ish binary classification dataset\n",
        "    without external dependencies.\n",
        "    \"\"\"\n",
        "    rng = np.random.default_rng(random_state)\n",
        "    X = rng.normal(loc=0.0, scale=1.0, size=(n_samples, n_features))\n",
        "\n",
        "    # Create a random linear boundary\n",
        "    w = rng.normal(size=(n_features,))\n",
        "    b = rng.normal()\n",
        "\n",
        "    logits = X @ w + b + rng.normal(scale=noise, size=n_samples)\n",
        "    y = (logits > 0).astype(np.float32)\n",
        "    return X.astype(np.float32), y\n",
        "\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, input_dim: int = 3, hidden_dim: int = 16):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(input_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, 1),  # binary logit\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x).squeeze(-1)  # shape: (batch,)\n",
        "\n",
        "\n",
        "def train(\n",
        "    model: nn.Module,\n",
        "    loader: DataLoader,\n",
        "    epochs: int,\n",
        "    device: torch.device,\n",
        "    lr: float = 1e-3,\n",
        ") -> list:\n",
        "    model.to(device)\n",
        "    optim = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "    criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "    history = []\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        for xb, yb in loader:\n",
        "            xb = xb.to(device)\n",
        "            yb = yb.to(device)\n",
        "            optim.zero_grad()\n",
        "            logits = model(xb)\n",
        "            loss = criterion(logits, yb)\n",
        "            loss.backward()\n",
        "            optim.step()\n",
        "            running_loss += loss.item() * xb.size(0)\n",
        "\n",
        "        epoch_loss = running_loss / len(loader.dataset)\n",
        "        history.append(epoch_loss)\n",
        "        print(f\"Epoch {epoch:03d}/{epochs} - loss: {epoch_loss:.4f}\")\n",
        "    return history\n",
        "\n",
        "\n",
        "def evaluate_accuracy(model: nn.Module, loader: DataLoader, device: torch.device) -> float:\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for xb, yb in loader:\n",
        "            xb = xb.to(device)\n",
        "            yb = yb.to(device)\n",
        "            logits = model(xb)\n",
        "            probs = torch.sigmoid(logits)\n",
        "            preds = (probs >= 0.5).float()\n",
        "            correct += (preds == yb).sum().item()\n",
        "            total += yb.numel()\n",
        "    return correct / total if total else math.nan\n",
        "\n",
        "\n",
        "def ensure_outdir(path: str) -> None:\n",
        "    os.makedirs(path, exist_ok=True)\n",
        "\n",
        "\n",
        "def save_artifacts(\n",
        "    outdir: str,\n",
        "    model: nn.Module,\n",
        "    config: dict,\n",
        "    history: list,\n",
        "    metrics: dict,\n",
        ") -> None:\n",
        "    ensure_outdir(outdir)\n",
        "    torch.save(model.state_dict(), os.path.join(outdir, \"model.pt\"))\n",
        "    with open(os.path.join(outdir, \"run_summary.json\"), \"w\") as f:\n",
        "        json.dump(\n",
        "            {\n",
        "                \"config\": config,\n",
        "                \"history\": history,\n",
        "                \"metrics\": metrics,\n",
        "            },\n",
        "            f,\n",
        "            indent=2,\n",
        "        )\n",
        "    print(f\"Saved model and summary to: {outdir}\")\n",
        "\n",
        "\n",
        "def parse_args():\n",
        "    parser = argparse.ArgumentParser(\n",
        "        description=\"End-to-end demo script that trains a tiny classifier and works in Colab/Jupyter.\",\n",
        "        formatter_class=argparse.ArgumentDefaultsHelpFormatter,\n",
        "    )\n",
        "\n",
        "    parser.add_argument(\"--n_samples\", type=int, default=1000, help=\"Number of synthetic samples to generate.\")\n",
        "    parser.add_argument(\"--epochs\", type=int, default=20, help=\"Number of training epochs.\")\n",
        "    parser.add_argument(\"--batch_size\", type=int, default=32, help=\"Mini-batch size.\")\n",
        "    parser.add_argument(\"--save_model\", type=str, default=\"true\", help=\"Whether to save model artifacts (true/false).\")\n",
        "    parser.add_argument(\n",
        "        \"--test_input\",\n",
        "        type=float,\n",
        "        nargs=3,\n",
        "        metavar=(\"X1\", \"X2\", \"X3\"),\n",
        "        help=\"Three feature values to run a single prediction after training.\",\n",
        "    )\n",
        "    parser.add_argument(\"--outdir\", type=str, default=\"outputs\", help=\"Directory to save artifacts.\")\n",
        "    parser.add_argument(\"--random_state\", type=int, default=42, help=\"Random seed for reproducibility.\")\n",
        "    parser.add_argument(\"--lr\", type=float, default=1e-3, help=\"Learning rate.\")\n",
        "    parser.add_argument(\"--hidden_dim\", type=int, default=16, help=\"Hidden layer size for the MLP.\")\n",
        "\n",
        "    # Important for Jupyter/Colab: ignore unknown args like \"-f <kernel.json>\"\n",
        "    args, _ = parser.parse_known_args()\n",
        "    # Normalize boolean\n",
        "    args.save_model = str2bool(args.save_model)\n",
        "    return args\n",
        "\n",
        "\n",
        "def main():\n",
        "    args = parse_args()\n",
        "    set_seed(args.random_state)\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    n_features = 3  # fixed to align with --test_input of length 3\n",
        "    X, y = make_synthetic_classification(\n",
        "        n_samples=args.n_samples,\n",
        "        n_features=n_features,\n",
        "        random_state=args.random_state,\n",
        "        noise=0.5,\n",
        "    )\n",
        "\n",
        "    # Train/val split (80/20)\n",
        "    n_train = int(0.8 * len(X))\n",
        "    X_train, y_train = X[:n_train], y[:n_train]\n",
        "    X_val, y_val = X[n_train:], y[n_train:]\n",
        "\n",
        "    train_ds = TensorDataset(torch.from_numpy(X_train), torch.from_numpy(y_train))\n",
        "    val_ds = TensorDataset(torch.from_numpy(X_val), torch.from_numpy(y_val))\n",
        "\n",
        "    train_loader = DataLoader(train_ds, batch_size=args.batch_size, shuffle=True, drop_last=False)\n",
        "    val_loader = DataLoader(val_ds, batch_size=args.batch_size, shuffle=False, drop_last=False)\n",
        "\n",
        "    model = MLP(input_dim=n_features, hidden_dim=args.hidden_dim)\n",
        "    history = train(model, train_loader, epochs=args.epochs, device=device, lr=args.lr)\n",
        "    val_acc = evaluate_accuracy(model, val_loader, device=device)\n",
        "\n",
        "    print(f\"Validation accuracy: {val_acc:.4f}\")\n",
        "\n",
        "    # Optional single prediction from --test_input\n",
        "    test_pred = None\n",
        "    if args.test_input is not None:\n",
        "        test_vec = torch.tensor(args.test_input, dtype=torch.float32, device=device).unsqueeze(0)\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            logit = model(test_vec)\n",
        "            prob = torch.sigmoid(logit).item()\n",
        "            label = 1 if prob >= 0.5 else 0\n",
        "            test_pred = {\"input\": list(map(float, args.test_input)), \"probability\": float(prob), \"predicted_label\": int(label)}\n",
        "        print(f\"Test input: {test_pred['input']} -> prob={test_pred['probability']:.4f}, label={test_pred['predicted_label']}\")\n",
        "\n",
        "    config = {\n",
        "        \"n_samples\": args.n_samples,\n",
        "        \"epochs\": args.epochs,\n",
        "        \"batch_size\": args.batch_size,\n",
        "        \"save_model\": args.save_model,\n",
        "        \"outdir\": args.outdir,\n",
        "        \"random_state\": args.random_state,\n",
        "        \"lr\": args.lr,\n",
        "        \"hidden_dim\": args.hidden_dim,\n",
        "        \"device\": str(device),\n",
        "    }\n",
        "    metrics = {\"val_accuracy\": float(val_acc), \"final_loss\": float(history[-1]) if history else float(\"nan\")}\n",
        "    if test_pred is not None:\n",
        "        metrics[\"test_prediction\"] = test_pred\n",
        "\n",
        "    if args.save_model:\n",
        "        save_artifacts(args.outdir, model, config=config, history=history, metrics=metrics)\n",
        "    else:\n",
        "        print(\"Skipping artifact save (save_model=false).\")\n",
        "\n",
        "    print(\"Done.\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ]
}