{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyPjVgqEVGBi2Alh0+GuM7Ln",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OneFineStarstuff/Cosmic-Brilliance/blob/main/colab_kernel_launcher_py.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U1hqm393iemJ"
      },
      "outputs": [],
      "source": [
        "#!/usr/bin/env python3\n",
        "# colab_kernel_launcher.py\n",
        "# MetaIntelligence end-to-end toolkit (notebook/Colab-safe)\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import json\n",
        "import math\n",
        "import time\n",
        "import copy\n",
        "import argparse\n",
        "import random\n",
        "import csv\n",
        "from typing import Tuple, Dict, Any, List, Optional\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# Optional heavy deps guarded to keep import-time light in notebooks\n",
        "try:\n",
        "    import torch\n",
        "    import torch.nn as nn\n",
        "    import torch.nn.functional as F\n",
        "    from torch.utils.data import DataLoader, TensorDataset\n",
        "except Exception as e:\n",
        "    raise RuntimeError(\"PyTorch is required. Please install torch before running this script.\") from e\n",
        "\n",
        "# sklearn only for data generation and metrics\n",
        "try:\n",
        "    from sklearn.datasets import make_blobs, make_moons, make_circles, make_classification\n",
        "    from sklearn.metrics import confusion_matrix, accuracy_score\n",
        "except Exception as e:\n",
        "    raise RuntimeError(\"scikit-learn is required. Please install scikit-learn before running this script.\") from e\n",
        "\n",
        "# Matplotlib is optional: plotting will be skipped if unavailable\n",
        "try:\n",
        "    import matplotlib.pyplot as plt\n",
        "    HAS_MPL = True\n",
        "except Exception:\n",
        "    HAS_MPL = False\n",
        "\n",
        "\n",
        "# ----------------------------\n",
        "# Utilities\n",
        "# ----------------------------\n",
        "def set_seed(seed: int) -> None:\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "\n",
        "def ensure_dir(path: str) -> None:\n",
        "    if path and not os.path.exists(path):\n",
        "        os.makedirs(path, exist_ok=True)\n",
        "\n",
        "\n",
        "def save_json(obj: Dict[str, Any], path: str) -> None:\n",
        "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(obj, f, indent=2, sort_keys=True)\n",
        "\n",
        "\n",
        "def load_json(path: str) -> Dict[str, Any]:\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "        return json.load(f)\n",
        "\n",
        "\n",
        "def save_csv(rows: List[Dict[str, Any]], path: str) -> None:\n",
        "    if not rows:\n",
        "        return\n",
        "    ensure_dir(os.path.dirname(path))\n",
        "    fieldnames = list(rows[0].keys())\n",
        "    with open(path, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
        "        writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
        "        writer.writeheader()\n",
        "        for r in rows:\n",
        "            writer.writerow(r)\n",
        "\n",
        "\n",
        "class Standardizer:\n",
        "    def __init__(self):\n",
        "        self.mean_: Optional[np.ndarray] = None\n",
        "        self.scale_: Optional[np.ndarray] = None\n",
        "\n",
        "    def fit(self, X: np.ndarray):\n",
        "        self.mean_ = X.mean(axis=0)\n",
        "        self.scale_ = X.std(axis=0)\n",
        "        self.scale_[self.scale_ == 0.0] = 1.0\n",
        "        return self\n",
        "\n",
        "    def transform(self, X: np.ndarray) -> np.ndarray:\n",
        "        if self.mean_ is None or self.scale_ is None:\n",
        "            raise ValueError(\"Standardizer not fitted\")\n",
        "        return (X - self.mean_) / self.scale_\n",
        "\n",
        "    def to_dict(self) -> Dict[str, Any]:\n",
        "        return {\n",
        "            \"mean\": self.mean_.tolist() if self.mean_ is not None else None,\n",
        "            \"scale\": self.scale_.tolist() if self.scale_ is not None else None,\n",
        "        }\n",
        "\n",
        "    @staticmethod\n",
        "    def from_dict(d: Dict[str, Any]) -> \"Standardizer\":\n",
        "        s = Standardizer()\n",
        "        if d.get(\"mean\") is not None:\n",
        "            s.mean_ = np.array(d[\"mean\"], dtype=np.float32)\n",
        "        if d.get(\"scale\") is not None:\n",
        "            s.scale_ = np.array(d[\"scale\"], dtype=np.float32)\n",
        "        return s\n",
        "\n",
        "\n",
        "# ----------------------------\n",
        "# Data generation\n",
        "# ----------------------------\n",
        "def generate_synthetic(\n",
        "    dataset: str,\n",
        "    n_samples: int,\n",
        "    n_features: int,\n",
        "    noise: float,\n",
        "    n_classes: int,\n",
        "    seed: int,\n",
        ") -> Tuple[np.ndarray, np.ndarray]:\n",
        "    rng = np.random.RandomState(seed)\n",
        "    if dataset == \"blobs\":\n",
        "        centers = n_classes\n",
        "        X, y = make_blobs(n_samples=n_samples, centers=centers, n_features=n_features,\n",
        "                          cluster_std=max(0.05, noise), random_state=seed)\n",
        "    elif dataset == \"moons\":\n",
        "        if n_features != 2:\n",
        "            raise ValueError(\"moons supports only dims=2\")\n",
        "        X, y = make_moons(n_samples=n_samples, noise=noise, random_state=seed)\n",
        "    elif dataset == \"circles\":\n",
        "        if n_features != 2:\n",
        "            raise ValueError(\"circles supports only dims=2\")\n",
        "        X, y = make_circles(n_samples=n_samples, factor=0.5, noise=noise, random_state=seed)\n",
        "    elif dataset == \"classification\":\n",
        "        X, y = make_classification(\n",
        "            n_samples=n_samples, n_features=n_features,\n",
        "            n_informative=max(2, min(n_features, n_features - 2)),\n",
        "            n_redundant=0, n_repeated=0,\n",
        "            n_classes=n_classes, n_clusters_per_class=1,\n",
        "            flip_y=min(0.25, noise), class_sep=max(0.5, 2 - 3*noise),\n",
        "            random_state=seed\n",
        "        )\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown dataset: {dataset}\")\n",
        "    X = X.astype(np.float32)\n",
        "    y = y.astype(np.int64)\n",
        "    # Shuffle\n",
        "    idx = np.arange(len(X))\n",
        "    rng.shuffle(idx)\n",
        "    return X[idx], y[idx]\n",
        "\n",
        "\n",
        "def train_val_split(X, y, val_frac=0.2, seed=42):\n",
        "    n = len(X)\n",
        "    n_val = int(n * val_frac)\n",
        "    rng = np.random.RandomState(seed)\n",
        "    idx = np.arange(n)\n",
        "    rng.shuffle(idx)\n",
        "    val_idx = idx[:n_val]\n",
        "    train_idx = idx[n_val:]\n",
        "    return (X[train_idx], y[train_idx]), (X[val_idx], y[val_idx])\n",
        "\n",
        "\n",
        "# ----------------------------\n",
        "# Model\n",
        "# ----------------------------\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, input_dim: int, hidden: List[int], num_classes: int, dropout: float = 0.0):\n",
        "        super().__init__()\n",
        "        layers = []\n",
        "        last = input_dim\n",
        "        for h in hidden:\n",
        "            layers += [nn.Linear(last, h), nn.ReLU(inplace=True)]\n",
        "            if dropout > 0:\n",
        "                layers.append(nn.Dropout(p=dropout))\n",
        "            last = h\n",
        "        layers.append(nn.Linear(last, num_classes))\n",
        "        self.net = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)  # logits\n",
        "\n",
        "\n",
        "def make_model(input_dim: int, num_classes: int, width: int, depth: int, dropout: float) -> MLP:\n",
        "    hidden = [width] * max(1, depth)\n",
        "    return MLP(input_dim, hidden, num_classes, dropout)\n",
        "\n",
        "\n",
        "# ----------------------------\n",
        "# Training helpers\n",
        "# ----------------------------\n",
        "def batch_iter(X: np.ndarray, y: Optional[np.ndarray], batch_size: int, shuffle: bool, seed: int):\n",
        "    n = len(X)\n",
        "    idx = np.arange(n)\n",
        "    if shuffle:\n",
        "        rng = np.random.RandomState(seed)\n",
        "        rng.shuffle(idx)\n",
        "    for start in range(0, n, batch_size):\n",
        "        sl = idx[start:start + batch_size]\n",
        "        if y is None:\n",
        "            yield X[sl], None\n",
        "        else:\n",
        "            yield X[sl], y[sl]\n",
        "\n",
        "\n",
        "def add_noise(x: torch.Tensor, sigma: float) -> torch.Tensor:\n",
        "    if sigma <= 0:\n",
        "        return x\n",
        "    noise = torch.randn_like(x) * sigma\n",
        "    return x + noise\n",
        "\n",
        "\n",
        "def train_supervised(\n",
        "    model: nn.Module,\n",
        "    optimizer: torch.optim.Optimizer,\n",
        "    Xtr: np.ndarray, ytr: np.ndarray,\n",
        "    Xva: np.ndarray, yva: np.ndarray,\n",
        "    device: str,\n",
        "    epochs: int,\n",
        "    batch_size: int,\n",
        "    aug_noise: float = 0.0,\n",
        "    seed: int = 42,\n",
        ") -> Dict[str, List[float]]:\n",
        "    crit = nn.CrossEntropyLoss()\n",
        "    metrics = {\"train_loss\": [], \"val_acc\": []}\n",
        "\n",
        "    Xtr_t = torch.from_numpy(Xtr).to(device)\n",
        "    ytr_t = torch.from_numpy(ytr).to(device)\n",
        "    Xva_t = torch.from_numpy(Xva).to(device)\n",
        "    yva_t = torch.from_numpy(yva).to(device)\n",
        "\n",
        "    for ep in range(1, epochs + 1):\n",
        "        model.train()\n",
        "        running = 0.0\n",
        "        for xb_np, yb_np in batch_iter(Xtr, ytr, batch_size, shuffle=True, seed=seed + ep):\n",
        "            xb = torch.from_numpy(xb_np).to(device)\n",
        "            yb = torch.from_numpy(yb_np).to(device)\n",
        "            xb = add_noise(xb, aug_noise)\n",
        "            logits = model(xb)\n",
        "            loss = crit(logits, yb)\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running += loss.item() * len(xb_np)\n",
        "        train_loss = running / len(Xtr)\n",
        "        metrics[\"train_loss\"].append(train_loss)\n",
        "\n",
        "        # val\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            logits = model(Xva_t)\n",
        "            preds = logits.argmax(dim=1)\n",
        "            acc = (preds == yva_t).float().mean().item()\n",
        "        metrics[\"val_acc\"].append(acc)\n",
        "        print(f\"[supervised] epoch {ep:03d} | train_loss={train_loss:.4f} | val_acc={acc:.4f}\")\n",
        "    return metrics\n",
        "\n",
        "\n",
        "def train_self_training(\n",
        "    model: nn.Module,\n",
        "    optimizer: torch.optim.Optimizer,\n",
        "    Xtr: np.ndarray, ytr: np.ndarray,\n",
        "    Xva: np.ndarray, yva: np.ndarray,\n",
        "    device: str,\n",
        "    epochs: int,\n",
        "    batch_size: int,\n",
        "    labeled_frac: float,\n",
        "    pseudo_thresh: float,\n",
        "    aug_noise: float,\n",
        "    warmup_epochs: int,\n",
        "    seed: int = 42,\n",
        ") -> Dict[str, List[float]]:\n",
        "    \"\"\"\n",
        "    Simple self-training:\n",
        "      1) Split train into labeled/unlabeled by labeled_frac\n",
        "      2) Warmup on labeled\n",
        "      3) Iteratively add high-confidence pseudo-labeled samples\n",
        "    \"\"\"\n",
        "    n = len(Xtr)\n",
        "    n_lab = max(1, int(n * labeled_frac))\n",
        "    rng = np.random.RandomState(seed)\n",
        "    idx = np.arange(n)\n",
        "    rng.shuffle(idx)\n",
        "    lab_idx = idx[:n_lab]\n",
        "    unlab_idx = idx[n_lab:]\n",
        "\n",
        "    X_lab, y_lab = Xtr[lab_idx], ytr[lab_idx]\n",
        "    X_unl = Xtr[unlab_idx]\n",
        "\n",
        "    # Warmup\n",
        "    print(f\"[selfsup] warmup on {len(X_lab)} labeled samples\")\n",
        "    _ = train_supervised(\n",
        "        model, optimizer, X_lab, y_lab, Xva, yva, device,\n",
        "        epochs=warmup_epochs, batch_size=batch_size, aug_noise=aug_noise, seed=seed\n",
        "    )\n",
        "\n",
        "    crit = nn.CrossEntropyLoss()\n",
        "    metrics = {\"train_loss\": [], \"val_acc\": [], \"pseudo_added\": []}\n",
        "\n",
        "    for ep in range(1, epochs + 1):\n",
        "        model.train()\n",
        "        running = 0.0\n",
        "\n",
        "        # Pseudo-label selection\n",
        "        if len(X_unl) > 0:\n",
        "            with torch.no_grad():\n",
        "                logits = model(torch.from_numpy(X_unl).to(device))\n",
        "                probs = F.softmax(logits, dim=1).cpu().numpy()\n",
        "                conf = probs.max(axis=1)\n",
        "                plabels = probs.argmax(axis=1)\n",
        "            keep = conf >= pseudo_thresh\n",
        "            X_keep = X_unl[keep]\n",
        "            y_keep = plabels[keep]\n",
        "            if len(X_keep) > 0:\n",
        "                # merge into labeled pool\n",
        "                X_lab = np.concatenate([X_lab, X_keep], axis=0)\n",
        "                y_lab = np.concatenate([y_lab, y_keep], axis=0)\n",
        "                X_unl = X_unl[~keep]\n",
        "            added = int(keep.sum())\n",
        "        else:\n",
        "            added = 0\n",
        "\n",
        "        metrics[\"pseudo_added\"].append(added)\n",
        "        print(f\"[selfsup] epoch {ep:03d} | added {added} pseudo-labeled samples | lab_pool={len(X_lab)} | unl={len(X_unl)}\")\n",
        "\n",
        "        # Train one epoch on current labeled pool\n",
        "        for xb_np, yb_np in batch_iter(X_lab, y_lab, batch_size, shuffle=True, seed=seed + ep):\n",
        "            xb = torch.from_numpy(xb_np).to(device)\n",
        "            yb = torch.from_numpy(yb_np).to(device)\n",
        "            xb = add_noise(xb, aug_noise)\n",
        "            logits = model(xb)\n",
        "            loss = crit(logits, yb)\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running += loss.item() * len(xb_np)\n",
        "        train_loss = running / max(1, len(X_lab))\n",
        "        metrics[\"train_loss\"].append(train_loss)\n",
        "\n",
        "        # val\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            logits = model(torch.from_numpy(Xva).to(device))\n",
        "            preds = logits.argmax(dim=1).cpu().numpy()\n",
        "            acc = (preds == yva).mean().item()\n",
        "        metrics[\"val_acc\"].append(acc)\n",
        "        print(f\"[selfsup] epoch {ep:03d} | train_loss={train_loss:.4f} | val_acc={acc:.4f}\")\n",
        "    return metrics\n",
        "\n",
        "\n",
        "def train_hybrid_consistency(\n",
        "    model: nn.Module,\n",
        "    optimizer: torch.optim.Optimizer,\n",
        "    Xtr: np.ndarray, ytr: np.ndarray,\n",
        "    Xva: np.ndarray, yva: np.ndarray,\n",
        "    device: str,\n",
        "    epochs: int,\n",
        "    batch_size: int,\n",
        "    aug_noise: float,\n",
        "    lambda_consistency: float,\n",
        "    seed: int = 42,\n",
        ") -> Dict[str, List[float]]:\n",
        "    \"\"\"\n",
        "    Hybrid = supervised CE + consistency regularization between two noisy views.\n",
        "    \"\"\"\n",
        "    ce = nn.CrossEntropyLoss()\n",
        "    metrics = {\"train_loss\": [], \"val_acc\": []}\n",
        "    Xtr_t = torch.from_numpy(Xtr).to(device)\n",
        "    ytr_t = torch.from_numpy(ytr).to(device)\n",
        "    Xva_t = torch.from_numpy(Xva).to(device)\n",
        "    yva_t = torch.from_numpy(yva).to(device)\n",
        "\n",
        "    for ep in range(1, epochs + 1):\n",
        "        model.train()\n",
        "        running = 0.0\n",
        "        for xb_np, yb_np in batch_iter(Xtr, ytr, batch_size, shuffle=True, seed=seed + ep):\n",
        "            xb = torch.from_numpy(xb_np).to(device)\n",
        "            yb = torch.from_numpy(yb_np).to(device)\n",
        "\n",
        "            x1 = add_noise(xb, aug_noise)\n",
        "            x2 = add_noise(xb, aug_noise)\n",
        "\n",
        "            logits1 = model(x1)\n",
        "            logits2 = model(x2)\n",
        "\n",
        "            loss_ce = ce(logits1, yb)\n",
        "            p1 = F.log_softmax(logits1, dim=1)\n",
        "            p2 = F.softmax(logits2, dim=1)\n",
        "            # KL divergence between the two views (p2 as target)\n",
        "            loss_cons = F.kl_div(p1, p2, reduction=\"batchmean\")\n",
        "            loss = loss_ce + lambda_consistency * loss_cons\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running += loss.item() * len(xb_np)\n",
        "        train_loss = running / len(Xtr)\n",
        "        metrics[\"train_loss\"].append(train_loss)\n",
        "\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            logits = model(Xva_t)\n",
        "            preds = logits.argmax(dim=1)\n",
        "            acc = (preds == yva_t).float().mean().item()\n",
        "        metrics[\"val_acc\"].append(acc)\n",
        "        print(f\"[hybrid] epoch {ep:03d} | train_loss={train_loss:.4f} | val_acc={acc:.4f}\")\n",
        "    return metrics\n",
        "\n",
        "\n",
        "# ----------------------------\n",
        "# Plotting (2D only)\n",
        "# ----------------------------\n",
        "def maybe_plot_2d_decision_boundary(\n",
        "    path: str,\n",
        "    model: nn.Module,\n",
        "    scaler: Standardizer,\n",
        "    X: np.ndarray,\n",
        "    y: np.ndarray,\n",
        "    title: str = \"\",\n",
        "    device: str = \"cpu\",\n",
        "    grid_steps: int = 300\n",
        "):\n",
        "    if not HAS_MPL:\n",
        "        print(\"Matplotlib not available; skipping plot.\")\n",
        "        return\n",
        "    if X.shape[1] != 2:\n",
        "        print(\"Plotting only for 2D; skipping plot.\")\n",
        "        return\n",
        "\n",
        "    x_min, x_max = X[:, 0].min() - 0.6, X[:, 0].max() + 0.6\n",
        "    y_min, y_max = X[:, 1].min() - 0.6, X[:, 1].max() + 0.6\n",
        "    xx, yy = np.meshgrid(\n",
        "        np.linspace(x_min, x_max, grid_steps, dtype=np.float32),\n",
        "        np.linspace(y_min, y_max, grid_steps, dtype=np.float32)\n",
        "    )\n",
        "    grid = np.c_[xx.ravel(), yy.ravel()].astype(np.float32)\n",
        "    grid_std = scaler.transform(grid)\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        logits = model(torch.from_numpy(grid_std).to(device))\n",
        "        preds = logits.argmax(dim=1).cpu().numpy().reshape(xx.shape)\n",
        "\n",
        "    plt.figure(figsize=(6.0, 5.5), dpi=140)\n",
        "    plt.contourf(xx, yy, preds, alpha=0.25, levels=np.arange(preds.max()+2)-0.5, cmap=\"coolwarm\")\n",
        "    scatter = plt.scatter(X[:, 0], X[:, 1], c=y, s=15, cmap=\"coolwarm\", edgecolors=\"k\", linewidths=0.3)\n",
        "    plt.title(title)\n",
        "    plt.xlabel(\"x1\"); plt.ylabel(\"x2\")\n",
        "    plt.tight_layout()\n",
        "    ensure_dir(os.path.dirname(path))\n",
        "    plt.savefig(path)\n",
        "    plt.close()\n",
        "    print(f\"Saved plot: {path}\")\n",
        "\n",
        "\n",
        "# ----------------------------\n",
        "# Checkpoint I/O\n",
        "# ----------------------------\n",
        "def save_checkpoint(path: str, model: nn.Module, scaler: Standardizer, meta: Dict[str, Any]):\n",
        "    ckpt = {\n",
        "        \"model_state\": model.state_dict(),\n",
        "        \"meta\": meta,\n",
        "        \"scaler\": scaler.to_dict(),\n",
        "    }\n",
        "    ensure_dir(os.path.dirname(path))\n",
        "    torch.save(ckpt, path)\n",
        "    print(f\"Saved checkpoint: {path}\")\n",
        "\n",
        "\n",
        "def load_checkpoint(path: str, map_location: str = \"cpu\"):\n",
        "    ckpt = torch.load(path, map_location=map_location)\n",
        "    meta = ckpt[\"meta\"]\n",
        "    scaler = Standardizer.from_dict(ckpt[\"scaler\"])\n",
        "    model = make_model(\n",
        "        input_dim=meta[\"input_dim\"],\n",
        "        num_classes=meta[\"num_classes\"],\n",
        "        width=meta[\"model\"][\"width\"],\n",
        "        depth=meta[\"model\"][\"depth\"],\n",
        "        dropout=meta[\"model\"][\"dropout\"],\n",
        "    )\n",
        "    model.load_state_dict(ckpt[\"model_state\"])\n",
        "    model.to(map_location)\n",
        "    model.eval()\n",
        "    return model, scaler, meta\n",
        "\n",
        "\n",
        "# ----------------------------\n",
        "# Command implementations\n",
        "# ----------------------------\n",
        "def cmd_train(args):\n",
        "    set_seed(args.seed)\n",
        "    device = \"cuda\" if torch.cuda.is_available() and not args.cpu else \"cpu\"\n",
        "    ensure_dir(args.out)\n",
        "\n",
        "    # Data\n",
        "    X_all, y_all = generate_synthetic(\n",
        "        dataset=args.dataset,\n",
        "        n_samples=args.samples,\n",
        "        n_features=args.dims,\n",
        "        noise=args.noise,\n",
        "        n_classes=args.classes,\n",
        "        seed=args.seed\n",
        "    )\n",
        "    (Xtr, ytr), (Xva, yva) = train_val_split(X_all, y_all, val_frac=args.val_frac, seed=args.seed + 1)\n",
        "\n",
        "    # Scale\n",
        "    scaler = Standardizer().fit(Xtr)\n",
        "    Xtr_s = scaler.transform(Xtr)\n",
        "    Xva_s = scaler.transform(Xva)\n",
        "\n",
        "    # Model\n",
        "    model = make_model(input_dim=args.dims, num_classes=args.classes, width=args.width, depth=args.depth, dropout=args.dropout).to(device)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
        "\n",
        "    # Train\n",
        "    t0 = time.time()\n",
        "    if args.mode == \"supervised\":\n",
        "        hist = train_supervised(model, optimizer, Xtr_s, ytr, Xva_s, yva, device, args.epochs, args.batch_size, args.aug_noise, args.seed)\n",
        "    elif args.mode == \"selfsup\":\n",
        "        hist = train_self_training(\n",
        "            model, optimizer, Xtr_s, ytr, Xva_s, yva, device,\n",
        "            epochs=args.epochs, batch_size=args.batch_size,\n",
        "            labeled_frac=args.labeled_frac, pseudo_thresh=args.pseudo_thresh,\n",
        "            aug_noise=args.aug_noise, warmup_epochs=args.warmup_epochs, seed=args.seed\n",
        "        )\n",
        "    elif args.mode == \"hybrid\":\n",
        "        hist = train_hybrid_consistency(\n",
        "            model, optimizer, Xtr_s, ytr, Xva_s, yva, device,\n",
        "            epochs=args.epochs, batch_size=args.batch_size,\n",
        "            aug_noise=args.aug_noise, lambda_consistency=args.lambda_consistency, seed=args.seed\n",
        "        )\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown mode: {args.mode}\")\n",
        "    dt = time.time() - t0\n",
        "\n",
        "    # Final val accuracy\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        logits = model(torch.from_numpy(Xva_s).to(device))\n",
        "        acc = (logits.argmax(dim=1).cpu().numpy() == yva).mean().item()\n",
        "\n",
        "    # Save artifacts\n",
        "    meta = {\n",
        "        \"dataset\": args.dataset,\n",
        "        \"input_dim\": args.dims,\n",
        "        \"num_classes\": args.classes,\n",
        "        \"noise\": args.noise,\n",
        "        \"mode\": args.mode,\n",
        "        \"val_frac\": args.val_frac,\n",
        "        \"seed\": args.seed,\n",
        "        \"model\": {\"width\": args.width, \"depth\": args.depth, \"dropout\": args.dropout},\n",
        "        \"opt\": {\"lr\": args.lr, \"weight_decay\": args.weight_decay},\n",
        "        \"train\": {\n",
        "            \"epochs\": args.epochs, \"batch_size\": args.batch_size,\n",
        "            \"aug_noise\": args.aug_noise,\n",
        "            \"labeled_frac\": args.labeled_frac,\n",
        "            \"pseudo_thresh\": args.pseudo_thresh,\n",
        "            \"warmup_epochs\": args.warmup_epochs,\n",
        "            \"lambda_consistency\": args.lambda_consistency\n",
        "        },\n",
        "        \"val_acc\": acc,\n",
        "        \"elapsed_sec\": dt\n",
        "    }\n",
        "\n",
        "    save_json(meta, os.path.join(args.out, \"config.json\"))\n",
        "    save_checkpoint(os.path.join(args.out, \"checkpoint.pt\"), model, scaler, meta)\n",
        "\n",
        "    # Plot 2D boundary\n",
        "    if args.plot and Xtr.shape[1] == 2:\n",
        "        maybe_plot_2d_decision_boundary(\n",
        "            path=os.path.join(args.out, \"decision_boundary.png\"),\n",
        "            model=model, scaler=scaler, X=Xtr, y=ytr, title=f\"{args.dataset} ({args.mode})\", device=device\n",
        "        )\n",
        "\n",
        "    # Metrics CSV (history)\n",
        "    rows = []\n",
        "    epochs_hist = len(next(iter(hist.values()))) if hist else 0\n",
        "    for i in range(epochs_hist):\n",
        "        row = {\"epoch\": i + 1}\n",
        "        for k, v in hist.items():\n",
        "            row[k] = v[i] if i < len(v) else \"\"\n",
        "        rows.append(row)\n",
        "    save_csv(rows, os.path.join(args.out, \"history.csv\"))\n",
        "\n",
        "    print(f\"Training done. Val acc={acc:.4f}. Artifacts in: {args.out}\")\n",
        "\n",
        "\n",
        "def cmd_eval(args):\n",
        "    device = \"cuda\" if torch.cuda.is_available() and not args.cpu else \"cpu\"\n",
        "    model, scaler, meta = load_checkpoint(args.ckpt, map_location=device)\n",
        "\n",
        "    # Generate a fresh test set with same dataset/config\n",
        "    Xte, yte = generate_synthetic(\n",
        "        dataset=meta[\"dataset\"],\n",
        "        n_samples=args.samples,\n",
        "        n_features=meta[\"input_dim\"],\n",
        "        noise=meta[\"noise\"],\n",
        "        n_classes=meta[\"num_classes\"],\n",
        "        seed=meta[\"seed\"] + 999  # distinct\n",
        "    )\n",
        "    Xte_s = scaler.transform(Xte)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        logits = model(torch.from_numpy(Xte_s).to(device))\n",
        "        preds = logits.argmax(dim=1).cpu().numpy()\n",
        "    acc = accuracy_score(yte, preds)\n",
        "    cm = confusion_matrix(yte, preds)\n",
        "\n",
        "    print(f\"Eval accuracy: {acc:.4f}\")\n",
        "    out_dir = args.out or os.path.dirname(args.ckpt)\n",
        "    ensuredir(outdir)\n",
        "\n",
        "    # Save metrics CSV\n",
        "    rows = [{\"metric\": \"accuracy\", \"value\": acc}]\n",
        "    savecsv(rows, os.path.join(outdir, \"eval_metrics.csv\"))\n",
        "\n",
        "    # Save confusion matrix CSV\n",
        "    cm_rows = []\n",
        "    for i in range(cm.shape[0]):\n",
        "        for j in range(cm.shape[1]):\n",
        "            cm_rows.append({\"row\": i, \"col\": j, \"count\": int(cm[i, j])})\n",
        "    savecsv(cmrows, os.path.join(outdir, \"confusionmatrix.csv\"))\n",
        "\n",
        "    # Optional plot (2D only)\n",
        "    if args.plot and meta[\"input_dim\"] == 2:\n",
        "        maybeplot2ddecisionboundary(\n",
        "            path=os.path.join(outdir, \"evaldecision_boundary.png\"),\n",
        "            model=model, scaler=scaler, X=Xte, y=yte,\n",
        "            title=f\"{meta['dataset']} (eval)\", device=device\n",
        "        )\n",
        "\n",
        "\n",
        "def loadcsv_points(path: str) -> np.ndarray:\n",
        "    # Assumes numerical CSV with no header or any header; we try to parse gracefully.\n",
        "    data = []\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "        reader = csv.reader(f)\n",
        "        for row in reader:\n",
        "            # skip empty lines or non-numeric headers\n",
        "            try:\n",
        "                vals = [float(x) for x in row if x.strip() != \"\"]\n",
        "                if vals:\n",
        "                    data.append(vals)\n",
        "            except ValueError:\n",
        "                continue\n",
        "    if not data:\n",
        "        raise ValueError(f\"No numeric rows found in CSV: {path}\")\n",
        "    # Ensure consistent dims\n",
        "    dims = len(data[0])\n",
        "    for r in data:\n",
        "        if len(r) != dims:\n",
        "            raise ValueError(\"Inconsistent number of columns in CSV rows\")\n",
        "    return np.array(data, dtype=np.float32)\n",
        "\n",
        "\n",
        "def cmd_predict(args):\n",
        "    device = \"cuda\" if torch.cuda.is_available() and not args.cpu else \"cpu\"\n",
        "    model, scaler, meta = loadcheckpoint(args.ckpt, maplocation=device)\n",
        "    d = meta[\"input_dim\"]\n",
        "\n",
        "    X: Optional[np.ndarray] = None\n",
        "    if args.points:\n",
        "        pts = []\n",
        "        for pair in args.points.split(\";\"):\n",
        "            pair = pair.strip()\n",
        "            if not pair:\n",
        "                continue\n",
        "            nums = [float(x.strip()) for x in pair.split(\",\")]\n",
        "            pts.append(nums)\n",
        "        X = np.array(pts, dtype=np.float32)\n",
        "    elif args.csv:\n",
        "        X = loadcsv_points(args.csv)\n",
        "    else:\n",
        "        raise ValueError(\"Provide --points \\\"x1,x2; ...\\\" or --csv path.csv\")\n",
        "\n",
        "    if X.shape[1] != d:\n",
        "        raise ValueError(f\"Input dims mismatch: model expects {d}, got {X.shape[1]}\")\n",
        "\n",
        "    Xs = scaler.transform(X)\n",
        "    with torch.no_grad():\n",
        "        logits = model(torch.from_numpy(Xs).to(device))\n",
        "        probs = F.softmax(logits, dim=1).cpu().numpy()\n",
        "        preds = probs.argmax(axis=1)\n",
        "\n",
        "    # Save predictions\n",
        "    out_dir = args.out or os.path.dirname(args.ckpt)\n",
        "    ensuredir(outdir)\n",
        "    rows = []\n",
        "    for i, (p, pr) in enumerate(zip(preds, probs)):\n",
        "        row = {\"index\": i, \"pred\": int(p)}\n",
        "        for k, v in enumerate(pr):\n",
        "            row[f\"prob_{k}\"] = float(v)\n",
        "        rows.append(row)\n",
        "    savecsv(rows, os.path.join(outdir, \"predictions.csv\"))\n",
        "    print(f\"Saved predictions to {os.path.join(out_dir, 'predictions.csv')}\")\n",
        "\n",
        "    # Optional visualization if 2D\n",
        "    if args.plot and d == 2:\n",
        "        # Build a small plot with the points overlaid on decision boundary\n",
        "        # For boundary, generate a synthetic cloud matching config just to get bounds\n",
        "        Xref, yref = generate_synthetic(\n",
        "            dataset=meta[\"dataset\"],\n",
        "            n_samples=500,\n",
        "            n_features=d,\n",
        "            noise=meta[\"noise\"],\n",
        "            nclasses=meta[\"numclasses\"],\n",
        "            seed=meta[\"seed\"] + 202\n",
        "        )\n",
        "        maybeplot2ddecisionboundary(\n",
        "            path=os.path.join(outdir, \"predictboundary.png\"),\n",
        "            model=model, scaler=scaler, X=Xref, y=yref,\n",
        "            title=f\"{meta['dataset']} (predict overlay)\", device=device\n",
        "        )\n",
        "        # also scatter user points\n",
        "        if HAS_MPL:\n",
        "            plt.figure(figsize=(5.5, 4.8), dpi=140)\n",
        "            plt.scatter(X[:, 0], X[:, 1], c=preds, s=50, cmap=\"coolwarm\", edgecolors=\"k\")\n",
        "            plt.title(\"Predicted points\")\n",
        "            plt.tight_layout()\n",
        "            plt.savefig(os.path.join(outdir, \"predictpoints.png\"))\n",
        "            plt.close()\n",
        "            print(f\"Saved plot: {os.path.join(outdir, 'predictpoints.png')}\")\n",
        "\n",
        "\n",
        "def cmd_benchmark(args):\n",
        "    set_seed(args.seed)\n",
        "    device = \"cuda\" if torch.cuda.is_available() and not args.cpu else \"cpu\"\n",
        "    ensure_dir(args.out)\n",
        "\n",
        "    datasets = [x.strip() for x in args.datasets.split(\",\")]\n",
        "    modes = [x.strip() for x in args.modes.split(\",\")]\n",
        "    seeds = list(range(args.seed, args.seed + args.seeds))\n",
        "\n",
        "    results = []\n",
        "    total = len(datasets) * len(modes) * len(seeds)\n",
        "    k = 0\n",
        "\n",
        "    for ds in datasets:\n",
        "        for mode in modes:\n",
        "            for sd in seeds:\n",
        "                k += 1\n",
        "                tag = f\"{ds}-{mode}-d{args.dims}-s{sd}\"\n",
        "                out_dir = os.path.join(args.out, tag)\n",
        "                print(f\"[{k}/{total}] Running {tag}\")\n",
        "                # Data\n",
        "                Xall, yall = generatesynthetic(ds, nsamples=args.samples, n_features=args.dims,\n",
        "                                                  noise=args.noise, n_classes=args.classes, seed=sd)\n",
        "                (Xtr, ytr), (Xva, yva) = trainvalsplit(Xall, yall, valfrac=args.valfrac, seed=sd + 1)\n",
        "                scaler = Standardizer().fit(Xtr)\n",
        "                Xtr_s = scaler.transform(Xtr)\n",
        "                Xva_s = scaler.transform(Xva)\n",
        "\n",
        "                model = make_model(args.dims, args.classes, args.width, args.depth, args.dropout).to(device)\n",
        "                opt = torch.optim.Adam(model.parameters(), lr=args.lr, weightdecay=args.weightdecay)\n",
        "\n",
        "                if mode == \"supervised\":\n",
        "                    trainsupervised(model, opt, Xtrs, ytr, Xvas, yva, device, args.epochs, args.batchsize, args.aug_noise, sd)\n",
        "                elif mode == \"selfsup\":\n",
        "                    trainselftraining(model, opt, Xtrs, ytr, Xvas, yva, device, args.epochs, args.batch_size,\n",
        "                                        labeledfrac=args.labeledfrac, pseudothresh=args.pseudothresh,\n",
        "                                        augnoise=args.augnoise, warmupepochs=args.warmupepochs, seed=sd)\n",
        "                elif mode == \"hybrid\":\n",
        "                    trainhybridconsistency(model, opt, Xtrs, ytr, Xvas, yva, device, args.epochs, args.batch_size,\n",
        "                                             augnoise=args.augnoise, lambdaconsistency=args.lambdaconsistency, seed=sd)\n",
        "                else:\n",
        "                    raise ValueError(f\"Unknown mode: {mode}\")\n",
        "\n",
        "                with torch.no_grad():\n",
        "                    logits = model(torch.fromnumpy(Xvas).to(device))\n",
        "                    preds = logits.argmax(dim=1).cpu().numpy()\n",
        "                acc = accuracy_score(yva, preds)\n",
        "\n",
        "                results.append({\n",
        "                    \"dataset\": ds, \"mode\": mode, \"dims\": args.dims, \"seed\": sd,\n",
        "                    \"val_acc\": acc\n",
        "                })\n",
        "\n",
        "                # Save minimal artifacts per run\n",
        "                ensuredir(outdir)\n",
        "                savecsv([results[-1]], os.path.join(outdir, \"val_acc.csv\"))\n",
        "\n",
        "    # Summarize\n",
        "    savecsv(results, os.path.join(args.out, \"benchmarkresults.csv\"))\n",
        "    # Aggregate by (dataset, mode)\n",
        "    agg = {}\n",
        "    for r in results:\n",
        "        key = (r[\"dataset\"], r[\"mode\"])\n",
        "        agg.setdefault(key, []).append(r[\"val_acc\"])\n",
        "    summary_rows = []\n",
        "    for (ds, mode), vals in agg.items():\n",
        "        arr = np.array(vals, dtype=np.float32)\n",
        "        summary_rows.append({\n",
        "            \"dataset\": ds,\n",
        "            \"mode\": mode,\n",
        "            \"dims\": args.dims,\n",
        "            \"runs\": len(vals),\n",
        "            \"acc_mean\": float(arr.mean()),\n",
        "            \"acc_std\": float(arr.std(ddof=1)) if len(vals) > 1 else 0.0,\n",
        "            \"acc_min\": float(arr.min()),\n",
        "            \"acc_max\": float(arr.max()),\n",
        "        })\n",
        "    savecsv(summaryrows, os.path.join(args.out, \"benchmark_summary.csv\"))\n",
        "    print(f\"Benchmark complete. Summary saved to {os.path.join(args.out, 'benchmark_summary.csv')}\")\n",
        "\n",
        "\n",
        "def cmdexportonnx(args):\n",
        "    try:\n",
        "        import onnx  # noqa: F401\n",
        "    except Exception:\n",
        "        print(\"onnx package not found. Please install onnx to export. Skipping.\")\n",
        "        return\n",
        "\n",
        "    device = \"cuda\" if torch.cuda.is_available() and not args.cpu else \"cpu\"\n",
        "    model, scaler, meta = loadcheckpoint(args.ckpt, maplocation=device)\n",
        "    model.eval()\n",
        "\n",
        "    dummy = torch.randn(1, meta[\"input_dim\"], device=device)\n",
        "    dynamic_axes = {\"input\": {0: \"batch\"}, \"logits\": {0: \"batch\"}}\n",
        "    ensure_dir(os.path.dirname(args.out))\n",
        "    torch.onnx.export(\n",
        "        model, dummy, args.out,\n",
        "        inputnames=[\"input\"], outputnames=[\"logits\"],\n",
        "        opsetversion=13, dynamicaxes=dynamic_axes\n",
        "    )\n",
        "    print(f\"Exported ONNX model to: {args.out}\")\n",
        "\n",
        "    # Save scaler alongside as JSON (needed to standardize inputs pre-ONNX)\n",
        "    savejson({\"standardizer\": scaler.todict(), \"meta\": meta}, os.path.splitext(args.out)[0] + \".preproc.json\")\n",
        "\n",
        "\n",
        "# ----------------------------\n",
        "# Argparse\n",
        "# ----------------------------\n",
        "import argparse\n",
        "from typing import Optional, List\n",
        "\n",
        "def build_parser():\n",
        "    p = argparse.ArgumentParser(\n",
        "        description=\"MetaIntelligence end-to-end toolkit (notebook/Colab-safe)\"\n",
        "    )\n",
        "    sub = p.add_subparsers(dest=\"cmd\", required=True)\n",
        "\n",
        "    # Train\n",
        "    t = sub.add_parser(\"train\", help=\"Train a model (self/supervised/hybrid)\")\n",
        "    t.add_argument(\"--dataset\", type=str, default=\"moons\",\n",
        "                   choices=[\"moons\", \"circles\", \"blobs\", \"classification\"])\n",
        "    t.add_argument(\"--dims\", type=int, default=2)\n",
        "    t.add_argument(\"--classes\", type=int, default=2)\n",
        "    t.add_argument(\"--samples\", type=int, default=2000)\n",
        "    t.add_argument(\"--noise\", type=float, default=0.2)\n",
        "    t.add_argument(\"--val-frac\", dest=\"valfrac\", type=float, default=0.2)\n",
        "\n",
        "    t.add_argument(\"--mode\", type=str, default=\"supervised\",\n",
        "                   choices=[\"supervised\", \"selfsup\", \"hybrid\"])\n",
        "    t.add_argument(\"--epochs\", type=int, default=50)\n",
        "    t.add_argument(\"--batch-size\", type=int, default=128)\n",
        "    t.add_argument(\"--lr\", type=float, default=3e-3)\n",
        "    t.add_argument(\"--weight-decay\", type=float, default=0.0)\n",
        "    t.add_argument(\"--width\", type=int, default=64)\n",
        "    t.add_argument(\"--depth\", type=int, default=2)\n",
        "    t.add_argument(\"--dropout\", type=float, default=0.0)\n",
        "    t.add_argument(\"--aug-noise\", type=float, default=0.05,\n",
        "                   help=\"Gaussian feature noise for augmentation\")\n",
        "\n",
        "    # self-training/hybrid knobs\n",
        "    t.add_argument(\"--labeled-frac\", type=float, default=0.1)\n",
        "    t.add_argument(\"--pseudo-thresh\", type=float, default=0.9)\n",
        "    t.add_argument(\"--warmup-epochs\", type=int, default=5)\n",
        "    t.add_argument(\"--lambda-consistency\", type=float, default=0.5)\n",
        "\n",
        "    t.add_argument(\"--plot\", action=\"store_true\")\n",
        "    t.add_argument(\"--seed\", type=int, default=42)\n",
        "    t.add_argument(\"--cpu\", action=\"store_true\")\n",
        "    t.add_argument(\"--out\", type=str, required=True)\n",
        "\n",
        "    # Eval\n",
        "    e = sub.add_parser(\"eval\", help=\"Evaluate a saved checkpoint on a fresh synthetic test set\")\n",
        "    e.add_argument(\"--ckpt\", type=str, required=True)\n",
        "    e.add_argument(\"--samples\", type=int, default=2000)\n",
        "    e.add_argument(\"--plot\", action=\"store_true\")\n",
        "    e.add_argument(\"--cpu\", action=\"store_true\")\n",
        "    e.add_argument(\"--out\", type=str, default=\"\")\n",
        "\n",
        "    # Predict\n",
        "    pr = sub.add_parser(\"predict\", help=\"Predict for given points\")\n",
        "    pr.add_argument(\"--ckpt\", type=str, required=True)\n",
        "    pr.add_argument(\"--points\", type=str, default=\"\", help='Inline points: \"x1,x2; y1,y2; ...\"')\n",
        "    pr.add_argument(\"--csv\", type=str, default=\"\", help=\"CSV file with samples (rows) and features (columns)\")\n",
        "    pr.add_argument(\"--plot\", action=\"store_true\")\n",
        "    pr.add_argument(\"--cpu\", action=\"store_true\")\n",
        "    pr.add_argument(\"--out\", type=str, default=\"\")\n",
        "\n",
        "    # Benchmark\n",
        "    b = sub.add_parser(\"benchmark\", help=\"Run grid of modes/datasets/dims across seeds and summarize\")\n",
        "    b.add_argument(\"--datasets\", type=str, default=\"moons,blobs\")\n",
        "    b.add_argument(\"--modes\", type=str, default=\"supervised,selfsup,hybrid\")\n",
        "    b.add_argument(\"--dims\", type=int, default=2)\n",
        "    b.add_argument(\"--classes\", type=int, default=2)\n",
        "    b.add_argument(\"--samples\", type=int, default=2000)\n",
        "    b.add_argument(\"--noise\", type=float, default=0.2)\n",
        "    b.add_argument(\"--val-frac\", dest=\"valfrac\", type=float, default=0.2)\n",
        "    b.add_argument(\"--epochs\", type=int, default=40)\n",
        "    b.add_argument(\"--batch-size\", type=int, default=128)\n",
        "    b.add_argument(\"--lr\", type=float, default=3e-3)\n",
        "    b.add_argument(\"--weight-decay\", type=float, default=0.0)\n",
        "    b.add_argument(\"--width\", type=int, default=64)\n",
        "    b.add_argument(\"--depth\", type=int, default=2)\n",
        "    b.add_argument(\"--dropout\", type=float, default=0.0)\n",
        "    b.add_argument(\"--aug-noise\", type=float, default=0.05)\n",
        "    b.add_argument(\"--labeled-frac\", type=float, default=0.1)\n",
        "    b.add_argument(\"--pseudo-thresh\", type=float, default=0.9)\n",
        "    b.add_argument(\"--warmup-epochs\", type=int, default=5)\n",
        "    b.add_argument(\"--lambda-consistency\", type=float, default=0.5)\n",
        "    b.add_argument(\"--seeds\", type=int, default=3, help=\"number of seeds to run starting from --seed\")\n",
        "    b.add_argument(\"--seed\", type=int, default=42)\n",
        "    b.add_argument(\"--cpu\", action=\"store_true\")\n",
        "    b.add_argument(\"--out\", type=str, required=True)\n",
        "\n",
        "    # Export ONNX\n",
        "    x = sub.add_parser(\"export-onnx\", help=\"Export a trained checkpoint to ONNX\")\n",
        "    x.add_argument(\"--ckpt\", type=str, required=True)\n",
        "    x.add_argument(\"--out\", type=str, required=True)\n",
        "    x.add_argument(\"--cpu\", action=\"store_true\")\n",
        "\n",
        "    return p\n",
        "\n",
        "\n",
        "def main(argv: Optional[List[str]] = None):\n",
        "    parser = build_parser()\n",
        "\n",
        "    # Notebook/Colab-safe: running with no args just prints help and returns\n",
        "    if argv is None:\n",
        "        argv = []\n",
        "    if len(argv) == 0:\n",
        "        parser.print_help()\n",
        "        return\n",
        "\n",
        "    args = parser.parse_args(argv)\n",
        "\n",
        "    # Basic sanitization for file paths\n",
        "    for attr in [\"ckpt\", \"csv\", \"out\"]:\n",
        "        if hasattr(args, attr):\n",
        "            val = getattr(args, attr)\n",
        "            if isinstance(val, str) and any(ch in val for ch in [\"..\", \"|\", \";\", \"`\"]):\n",
        "                raise ValueError(f\"Unsafe characters in path argument: --{attr}\")\n",
        "\n",
        "    # Dispatch after all handlers are defined/imported\n",
        "    dispatch = {\n",
        "        \"train\": cmdtrain,\n",
        "        \"eval\": cmdeval,          # Make sure this exists\n",
        "        \"predict\": cmdpredict,\n",
        "        \"benchmark\": cmdbenchmark,\n",
        "        \"export-onnx\": cmdexport_onnx,\n",
        "    }\n",
        "\n",
        "    handler = dispatch.get(args.cmd)\n",
        "    if handler is None:\n",
        "        parser.error(f\"Unknown command: {args.cmd}\")\n",
        "    return handler(args)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Safe for notebooks and scripts\n",
        "    main()"
      ]
    }
  ]
}