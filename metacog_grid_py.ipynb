{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyPJBDmAHD4FSFXT6zcwvTps",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OneFineStarstuff/Cosmic-Brilliance/blob/main/metacog_grid_py.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qoLQz8vdXKtf"
      },
      "outputs": [],
      "source": [
        "#!/usr/bin/env python3\n",
        "# metacog_grid.py\n",
        "# Meta-cognitive agent integrated with a dynamic, noisy gridworld.\n",
        "\n",
        "from __future__ import annotations\n",
        "import argparse\n",
        "import json\n",
        "import math\n",
        "import random\n",
        "from dataclasses import dataclass, field\n",
        "from typing import Dict, List, Optional, Set, Tuple, Any\n",
        "\n",
        "# ---------------------------\n",
        "# Types and utilities\n",
        "# ---------------------------\n",
        "\n",
        "Action = str  # 'UP'|'DOWN'|'LEFT'|'RIGHT'\n",
        "Pos = Tuple[int, int]\n",
        "\n",
        "ACTIONS: Tuple[Action, ...] = ('UP', 'DOWN', 'LEFT', 'RIGHT')\n",
        "DELTA: Dict[Action, Tuple[int, int]] = {\n",
        "    'UP': (0, -1),\n",
        "    'DOWN': (0, 1),\n",
        "    'LEFT': (-1, 0),\n",
        "    'RIGHT': (1, 0),\n",
        "}\n",
        "\n",
        "def manhattan(a: Pos, b: Pos) -> int:\n",
        "    return abs(a[0]-b[0]) + abs(a[1]-b[1])\n",
        "\n",
        "def in_bounds(p: Pos, n: int) -> bool:\n",
        "    return 0 <= p[0] < n and 0 <= p[1] < n\n",
        "\n",
        "def parse_xy(s: str) -> Pos:\n",
        "    x, y = s.split(',')\n",
        "    return (int(x.strip()), int(y.strip()))\n",
        "\n",
        "def parse_dyn_spec(s: str) -> Dict[int, Set[Pos]]:\n",
        "    \"\"\"\n",
        "    Parse dynamic forbidden schedule like:\n",
        "      \"3:(1,1);4:(1,2);6:(2,2)\"\n",
        "    meaning: at t=3 add (1,1), at t=4 add (1,2), at t=6 add (2,2).\n",
        "    \"\"\"\n",
        "    events: Dict[int, Set[Pos]] = {}\n",
        "    if not s:\n",
        "        return events\n",
        "    for part in s.split(';'):\n",
        "        part = part.strip()\n",
        "        if not part:\n",
        "            continue\n",
        "        ts, coords = part.split(':')\n",
        "        t = int(ts.strip())\n",
        "        x, y = coords.strip().strip('()').split(',')\n",
        "        pos = (int(x), int(y))\n",
        "        events.setdefault(t, set()).add(pos)\n",
        "    return events\n",
        "\n",
        "def seeded_rng(seed: int) -> random.Random:\n",
        "    rng = random.Random(seed)\n",
        "    return rng\n",
        "\n",
        "# ---------------------------\n",
        "# Environment\n",
        "# ---------------------------\n",
        "\n",
        "@dataclass\n",
        "class GridWorld:\n",
        "    n: int\n",
        "    start: Pos\n",
        "    goal: Pos\n",
        "    forbidden: Set[Pos] = field(default_factory=set)\n",
        "    dyn_events: Dict[int, Set[Pos]] = field(default_factory=dict)\n",
        "    action_noise: float = 0.0\n",
        "    rng: random.Random = field(default_factory=lambda: seeded_rng(0))\n",
        "\n",
        "    t: int = 0\n",
        "    pos: Pos = field(init=False)\n",
        "\n",
        "    def __post_init__(self):\n",
        "        self.pos = self.start\n",
        "\n",
        "    def observe(self) -> Dict[str, Any]:\n",
        "        # Agent-facing percepts\n",
        "        return {\n",
        "            't': self.t,\n",
        "            'pos': self.pos,\n",
        "            'goal': self.goal,\n",
        "            'forbidden': set(self.forbidden),  # give current snapshot\n",
        "            'grid': self.n,\n",
        "        }\n",
        "\n",
        "    def maybe_apply_dynamic_forbidden(self):\n",
        "        if self.t in self.dyn_events:\n",
        "            for p in self.dyn_events[self.t]:\n",
        "                self.forbidden.add(p)\n",
        "\n",
        "    def sample_action(self, a: Action) -> Action:\n",
        "        if self.action_noise <= 0:\n",
        "            return a\n",
        "        if self.rng.random() < self.action_noise:\n",
        "            return self.rng.choice(ACTIONS)\n",
        "        return a\n",
        "\n",
        "    def step(self, a: Action) -> Dict[str, Any]:\n",
        "        # Apply dynamic ethics updates at the start of each tick.\n",
        "        self.maybe_apply_dynamic_forbidden()\n",
        "\n",
        "        orig_pos = self.pos\n",
        "        a_exec = self.sample_action(a)\n",
        "        dx, dy = DELTA[a_exec]\n",
        "        cand = (orig_pos[0] + dx, orig_pos[1] + dy)\n",
        "\n",
        "        status = 'same'\n",
        "        reward = 0.0\n",
        "        reached = False\n",
        "\n",
        "        if not in_bounds(cand, self.n) or cand in self.forbidden:\n",
        "            cand = orig_pos\n",
        "            status = 'blocked'\n",
        "            reward = -1.0\n",
        "        else:\n",
        "            before = manhattan(orig_pos, self.goal)\n",
        "            after = manhattan(cand, self.goal)\n",
        "            if after < before:\n",
        "                status = 'closer'\n",
        "                reward = 1.0\n",
        "            elif after > before:\n",
        "                status = 'farther'\n",
        "                reward = -1.0\n",
        "            else:\n",
        "                status = 'same'\n",
        "                reward = 0.0\n",
        "\n",
        "        self.pos = cand\n",
        "\n",
        "        if self.pos == self.goal:\n",
        "            status = 'reached'\n",
        "            reward = 11.0\n",
        "            reached = True\n",
        "\n",
        "        rec = {\n",
        "            't': self.t,\n",
        "            'pos': orig_pos,\n",
        "            'action': {'type': 'MOVE', 'dir': a, 'exec': a_exec},\n",
        "            'outcome': status,\n",
        "            'reward': reward,\n",
        "            'goal': self.goal,\n",
        "            'reached': reached\n",
        "        }\n",
        "        self.t += 1\n",
        "        return rec\n",
        "\n",
        "# ---------------------------\n",
        "# Values, Core model, and meta components\n",
        "# ---------------------------\n",
        "\n",
        "@dataclass\n",
        "class Values:\n",
        "    # Simple value system with tunable weights and rules\n",
        "    w_progress: float = 1.0\n",
        "    w_safety: float = 2.0  # penalize forbidden/bounds\n",
        "    tie_break: Tuple[Action, ...] = ('DOWN', 'RIGHT', 'UP', 'LEFT')\n",
        "\n",
        "class HeuristicCore:\n",
        "    \"\"\"\n",
        "    Core model that decides, can explain, predicts outcome deterministically,\n",
        "    and can update from reflection.\n",
        "    \"\"\"\n",
        "    def __init__(self, values: Values, n: int):\n",
        "        self.values = values\n",
        "        self.n = n\n",
        "        self.last_percepts: Optional[Dict[str, Any]] = None\n",
        "\n",
        "    def decide(self, percepts: Dict[str, Any]) -> Action:\n",
        "        self.last_percepts = percepts\n",
        "        pos: Pos = percepts['pos']\n",
        "        goal: Pos = percepts['goal']\n",
        "        forbidden: Set[Pos] = percepts['forbidden']\n",
        "\n",
        "        best: List[Action] = []\n",
        "        best_score = -1e9\n",
        "        for a in ACTIONS:\n",
        "            dx, dy = DELTA[a]\n",
        "            cand = (pos[0] + dx, pos[1] + dy)\n",
        "            if not in_bounds(cand, self.n):\n",
        "                score = -10 * self.values.w_safety\n",
        "            elif cand in forbidden:\n",
        "                score = -5 * self.values.w_safety\n",
        "            else:\n",
        "                # progress toward goal\n",
        "                before = manhattan(pos, goal)\n",
        "                after = manhattan(cand, goal)\n",
        "                progress = (before - after)\n",
        "                score = self.values.w_progress * progress\n",
        "            if score > best_score:\n",
        "                best = [a]\n",
        "                best_score = score\n",
        "            elif score == best_score:\n",
        "                best.append(a)\n",
        "\n",
        "        if not best:\n",
        "            return 'RIGHT'\n",
        "        # Stable tie-break\n",
        "        tie = [a for a in self.values.tie_break if a in best]\n",
        "        return tie[0] if tie else best[0]\n",
        "\n",
        "    def predict_outcome(self, decision: Action) -> str:\n",
        "        # Deterministic predictor using last_percepts and no noise/dynamics\n",
        "        if not self.last_percepts:\n",
        "            return 'unknown'\n",
        "        pos: Pos = self.last_percepts['pos']\n",
        "        goal: Pos = self.last_percepts['goal']\n",
        "        forbidden: Set[Pos] = self.last_percepts['forbidden']\n",
        "        dx, dy = DELTA[decision]\n",
        "        cand = (pos[0] + dx, pos[1] + dy)\n",
        "        if not in_bounds(cand, self.n) or cand in forbidden:\n",
        "            return 'blocked'\n",
        "        if cand == goal:\n",
        "            return 'reached'\n",
        "        before = manhattan(pos, goal)\n",
        "        after = manhattan(cand, goal)\n",
        "        if after < before:\n",
        "            return 'closer'\n",
        "        if after > before:\n",
        "            return 'farther'\n",
        "        return 'same'\n",
        "\n",
        "    def explain(self, decision: Action) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Structured explanation containing the prediction and value rationale.\n",
        "        \"\"\"\n",
        "        if not self.last_percepts:\n",
        "            raise RuntimeError(\"No percepts to explain from.\")\n",
        "        pos: Pos = self.last_percepts['pos']\n",
        "        goal: Pos = self.last_percepts['goal']\n",
        "        pred = self.predict_outcome(decision)\n",
        "        rationale = {\n",
        "            'move': decision,\n",
        "            'prediction': pred,\n",
        "            'pos': pos,\n",
        "            'goal': goal,\n",
        "            'principles': [\n",
        "                'Prioritize safety (avoid forbidden/out-of-bounds)',\n",
        "                'Prefer actions that reduce Manhattan distance',\n",
        "                f'Tie-break preference: {self.values.tie_break}',\n",
        "            ],\n",
        "        }\n",
        "        return rationale\n",
        "\n",
        "    def update_from_reflection(self, resolution: Dict[str, Any]):\n",
        "        \"\"\"\n",
        "        Apply policy updates (e.g., learned forbidden cells, updated tie-break, weights).\n",
        "        \"\"\"\n",
        "        # Update tie-break if proposed\n",
        "        tb = resolution.get('tie_break')\n",
        "        if tb:\n",
        "            self.values.tie_break = tuple(tb)\n",
        "        # Adjust weights\n",
        "        if 'w_progress' in resolution:\n",
        "            self.values.w_progress = float(resolution['w_progress'])\n",
        "        if 'w_safety' in resolution:\n",
        "            self.values.w_safety = float(resolution['w_safety'])\n",
        "\n",
        "# ---------------------------\n",
        "# Memory graph and meta reasoning\n",
        "# ---------------------------\n",
        "\n",
        "@dataclass\n",
        "class MemoryItem:\n",
        "    t: int\n",
        "    pos: Pos\n",
        "    goal: Pos\n",
        "    action: Action\n",
        "    predicted_outcome: str\n",
        "    observed_outcome: Optional[str] = None\n",
        "    contradiction: bool = False\n",
        "\n",
        "@dataclass\n",
        "class ReflectiveMemoryGraph:\n",
        "    decay_lambda: float = 0.0\n",
        "    items: List[MemoryItem] = field(default_factory=list)\n",
        "\n",
        "    def update_trace(self, explanation: Dict[str, Any]):\n",
        "        self.items.append(\n",
        "            MemoryItem(\n",
        "                t=explanation.get('t', explanation.get('pos', (0,0))[0] if explanation.get('pos') else len(self.items)),\n",
        "                pos=explanation['pos'],\n",
        "                goal=explanation['goal'],\n",
        "                action=explanation['move'],\n",
        "                predicted_outcome=explanation['prediction'],\n",
        "            )\n",
        "        )\n",
        "\n",
        "    def attach_observation(self, t: int, observed_outcome: str):\n",
        "        # Attach to the last undecided item at or before t\n",
        "        for it in reversed(self.items):\n",
        "            if it.observed_outcome is None:\n",
        "                it.observed_outcome = observed_outcome\n",
        "                it.contradiction = (it.predicted_outcome != it.observed_outcome)\n",
        "                break\n",
        "\n",
        "    def stats(self, t_now: int) -> Dict[str, float]:\n",
        "        if not self.items:\n",
        "            return {'processed': 0, 'contradictions': 0, 'density': 0.0, 'avg_match': 0.0}\n",
        "        total_w = 0.0\n",
        "        match_w = 0.0\n",
        "        contra = 0\n",
        "        for it in self.items:\n",
        "            w = math.exp(-self.decay_lambda * max(0, t_now - it.t))\n",
        "            total_w += w\n",
        "            if it.observed_outcome is None:\n",
        "                continue\n",
        "            if it.predicted_outcome == it.observed_outcome:\n",
        "                match_w += w\n",
        "            else:\n",
        "                contra += 1\n",
        "        avg_match = (match_w / total_w) if total_w > 0 else 0.0\n",
        "        density = contra / max(1, sum(1 for it in self.items if it.observed_outcome is not None))\n",
        "        return {'processed': len(self.items), 'contradictions': contra, 'density': density, 'avg_match': avg_match}\n",
        "\n",
        "class ContradictionResolutionEngine:\n",
        "    def detect_contradiction(self, rmg: ReflectiveMemoryGraph) -> bool:\n",
        "        return any(it.contradiction for it in rmg.items if it.observed_outcome is not None)\n",
        "\n",
        "    def extract_conflict(self, rmg: ReflectiveMemoryGraph) -> Dict[str, Any]:\n",
        "        recent = next((it for it in reversed(rmg.items) if it.observed_outcome is not None), None)\n",
        "        if not recent:\n",
        "            return {}\n",
        "        return {\n",
        "            't': recent.t,\n",
        "            'pos': recent.pos,\n",
        "            'goal': recent.goal,\n",
        "            'action': recent.action,\n",
        "            'predicted': recent.predicted_outcome,\n",
        "            'observed': recent.observed_outcome,\n",
        "            'contradiction': recent.contradiction,\n",
        "        }\n",
        "\n",
        "class InternalDialogueEngine:\n",
        "    \"\"\"\n",
        "    Lightweight debate: if we predicted 'closer' but saw 'blocked',\n",
        "    we likely missed a forbidden cell update or noise; react by\n",
        "    increasing safety weight and/or updating tie-break.\n",
        "    \"\"\"\n",
        "    def debate(self, conflict_set: Dict[str, Any]) -> Dict[str, Any]:\n",
        "        if not conflict_set or not conflict_set.get('contradiction'):\n",
        "            return {'note': 'no-op'}\n",
        "        predicted = conflict_set['predicted']\n",
        "        observed = conflict_set['observed']\n",
        "        resolution: Dict[str, Any] = {}\n",
        "        if predicted != observed:\n",
        "            # Increase caution if we ran into 'blocked' unexpectedly\n",
        "            if observed == 'blocked':\n",
        "                resolution['w_safety'] = 2.5\n",
        "            # If we moved 'farther', emphasize progress pressure\n",
        "            if observed == 'farther':\n",
        "                resolution['w_progress'] = 1.2\n",
        "            # Rotate tie-break to de-emphasize the last action\n",
        "            last = conflict_set['action']\n",
        "            preferred = ['DOWN', 'RIGHT', 'UP', 'LEFT']\n",
        "            if last in preferred:\n",
        "                preferred.remove(last)\n",
        "                preferred.append(last)\n",
        "            resolution['tie_break'] = preferred\n",
        "            resolution['rationale'] = f\"Adjusted after mismatch (pred={predicted}, obs={observed}).\"\n",
        "        return resolution\n",
        "\n",
        "# ---------------------------\n",
        "# Your MetaCognitiveAgent (extended with outcome ingestion)\n",
        "# ---------------------------\n",
        "\n",
        "class MetaCognitiveAgent:\n",
        "    def __init__(self, core_model, values, memory_graph):\n",
        "        self.core = core_model\n",
        "        self.values = values\n",
        "        self.rmg = memory_graph\n",
        "        self.ide = InternalDialogueEngine()\n",
        "        self.cre = ContradictionResolutionEngine()\n",
        "        self.confidence: Dict[str, float] = {'transition_model': 0.9, 'policy': 0.9}\n",
        "\n",
        "    def act(self, environment) -> Action:\n",
        "        percepts = environment.observe()\n",
        "        decision = self.core.decide(percepts)\n",
        "        self._monitor(decision, percepts)\n",
        "        return decision\n",
        "\n",
        "    def _monitor(self, decision, context):\n",
        "        # Attach time into explanation for better decay accounting\n",
        "        explanation = self.core.explain(decision)\n",
        "        explanation['t'] = context.get('t', 0)\n",
        "        self.rmg.update_trace(explanation)\n",
        "        # Pre-action contradiction check is about historical items\n",
        "        if self.cre.detect_contradiction(self.rmg):\n",
        "            self._reflect_and_revise()\n",
        "\n",
        "    def observe_outcome(self, outcome_record: Dict[str, Any]):\n",
        "        # Post-action: connect observed outcome to the latest trace\n",
        "        self.rmg.attach_observation(outcome_record['t'], outcome_record['outcome'])\n",
        "        if self.cre.detect_contradiction(self.rmg):\n",
        "            self._reflect_and_revise()\n",
        "\n",
        "    def _reflect_and_revise(self):\n",
        "        conflict_set = self.cre.extract_conflict(self.rmg)\n",
        "        if not conflict_set:\n",
        "            return\n",
        "        resolution = self.ide.debate(conflict_set)\n",
        "        self.rmg_integrate_reflection(resolution)\n",
        "        self.core.update_from_reflection(resolution)\n",
        "        # Simple confidence calibration\n",
        "        if resolution.get('note') == 'no-op':\n",
        "            self.confidence['policy'] = min(1.0, self.confidence['policy'] + 0.01)\n",
        "        else:\n",
        "            self.confidence['policy'] = max(0.0, self.confidence['policy'] - 0.05)\n",
        "\n",
        "    # Provide compatibility shim for your original call\n",
        "    def rmg_integrate_reflection(self, resolution: Dict[str, Any]):\n",
        "        # In a richer graph, we'd add nodes/edges; here we keep a note\n",
        "        if not hasattr(self.rmg, 'reflections'):\n",
        "            self.rmg.reflections = []\n",
        "        self.rmg.reflections.append({'t': len(self.rmg.items), 'resolution': resolution})\n",
        "\n",
        "# ---------------------------\n",
        "# Rollout configuration and runner\n",
        "# ---------------------------\n",
        "\n",
        "@dataclass\n",
        "class RolloutConfig:\n",
        "    grid: int = 5\n",
        "    start: Pos = (0, 1)\n",
        "    goal: Pos = (3, 3)\n",
        "    tmax: int = 50\n",
        "    t_flip: Optional[int] = None\n",
        "    dynamic_forbidden: Dict[int, Set[Pos]] = field(default_factory=dict)\n",
        "    action_noise: float = 0.0\n",
        "    decay_lambda: float = 0.0\n",
        "    seed: int = 7\n",
        "    scenario: str = 'baseline'  # baseline|dynamic-ethics|temporal-decay|flip-goal|inject-noise|all\n",
        "    log_jsonl: Optional[str] = None\n",
        "\n",
        "def make_config_from_args(args: argparse.Namespace) -> RolloutConfig:\n",
        "    start = parse_xy(args.start)\n",
        "    goal = parse_xy(args.goal)\n",
        "    dyn = parse_dyn_spec(args.dynamic)\n",
        "\n",
        "    scenario = args.scenario\n",
        "    cfg = RolloutConfig(\n",
        "        grid=args.grid,\n",
        "        start=start,\n",
        "        goal=goal,\n",
        "        tmax=args.tmax,\n",
        "        t_flip=args.t_flip if scenario in ('flip-goal', 'all') else None,\n",
        "        dynamic_forbidden=dyn if scenario in ('dynamic-ethics', 'all') else {},\n",
        "        action_noise=(args.noise if scenario in ('inject-noise', 'all') else 0.0),\n",
        "        decay_lambda=(args.decay if scenario in ('temporal-decay', 'all') else 0.0),\n",
        "        seed=args.seed,\n",
        "        scenario=scenario,\n",
        "        log_jsonl=args.log,\n",
        "    )\n",
        "    return cfg\n",
        "\n",
        "def run_rollout(cfg: RolloutConfig):\n",
        "    rng = seeded_rng(cfg.seed)\n",
        "    env = GridWorld(\n",
        "        n=cfg.grid,\n",
        "        start=cfg.start,\n",
        "        goal=cfg.goal,\n",
        "        forbidden=set(),\n",
        "        dyn_events=cfg.dynamic_forbidden,\n",
        "        action_noise=cfg.action_noise,\n",
        "        rng=rng\n",
        "    )\n",
        "\n",
        "    values = Values()\n",
        "    core = HeuristicCore(values, n=cfg.grid)\n",
        "    rmg = ReflectiveMemoryGraph(decay_lambda=cfg.decay_lambda)\n",
        "    agent = MetaCognitiveAgent(core, values, rmg)\n",
        "\n",
        "    steps: List[Dict[str, Any]] = []\n",
        "    log_f = open(cfg.log_jsonl, 'w') if cfg.log_jsonl else None\n",
        "\n",
        "    print(\"=== Rollout ===\")\n",
        "    for _ in range(cfg.tmax):\n",
        "        # Optional goal flip\n",
        "        if cfg.t_flip is not None and env.t == cfg.t_flip:\n",
        "            old_goal = env.goal\n",
        "            env.goal = (cfg.grid - 1 - old_goal[0], cfg.grid - 1 - old_goal[1])\n",
        "            print(f\"[Info] Goal flipped at t={env.t} from {old_goal} to {env.goal}\")\n",
        "\n",
        "        a = agent.act(env)\n",
        "        rec = env.step(a)\n",
        "        agent.observe_outcome(rec)\n",
        "\n",
        "        steps.append(rec)\n",
        "        print(f\"t={rec['t']:02d} pos={rec['pos']} action={{'type': 'MOVE', 'dir': '{rec['action']['dir']}', 'exec': '{rec['action']['exec']}'}} \"\n",
        "              f\"outcome={rec['outcome']} reward={rec['reward']} goal={rec['goal']} reached={rec['reached']}\")\n",
        "\n",
        "        if log_f:\n",
        "            log_f.write(json.dumps(rec) + \"\\n\")\n",
        "\n",
        "        if rec['reached']:\n",
        "            print(\"Goal reached. Stopping rollout.\")\n",
        "            break\n",
        "\n",
        "    if log_f:\n",
        "        log_f.close()\n",
        "\n",
        "    # Reporting\n",
        "    print(\"\\n=== Audited contradictions ===\")\n",
        "    stats = rmg.stats(t_now=env.t)\n",
        "    print(f\"processed={stats['processed']} contradictions={stats['contradictions']} density={stats['density']:.3f} avg_match={stats['avg_match']:.3f}\")\n",
        "\n",
        "    print(\"\\n=== Reflections applied ===\")\n",
        "    reflections = getattr(rmg, 'reflections', [])\n",
        "    if not reflections:\n",
        "        print(\"(none)\")\n",
        "    else:\n",
        "        for i, ref in enumerate(reflections, 1):\n",
        "            print(f\"[{i}] {ref['resolution']}\")\n",
        "\n",
        "    print(\"\\n=== Confidence ===\")\n",
        "    print(agent.confidence)\n",
        "\n",
        "# ---------------------------\n",
        "# CLI\n",
        "# ---------------------------\n",
        "\n",
        "def main():\n",
        "    p = argparse.ArgumentParser(description=\"Meta-cognitive grid rollout with dynamic ethics, noise, and reflection.\")\n",
        "    p.add_argument('--grid', type=int, default=5, help='Grid size N (NxN)')\n",
        "    p.add_argument('--start', type=str, default='0,1', help='Start position \\\"x,y\\\"')\n",
        "    p.add_argument('--goal', type=str, default='3,3', help='Goal position \\\"x,y\\\"')\n",
        "    p.add_argument('--tmax', type=int, default=50, help='Max timesteps')\n",
        "    p.add_argument('--t_flip', type=int, default=None, help='Timestep to flip the goal (only in flip-goal or all)')\n",
        "    p.add_argument('--dynamic', type=str, default='', help='Dynamic forbidden spec, e.g. \\\"3:(1,1);6:(2,2)\\\"')\n",
        "    p.add_argument('--noise', type=float, default=0.0, help='Action noise probability (only in inject-noise or all)')\n",
        "    p.add_argument('--decay', type=float, default=0.0, help='Temporal decay lambda (only in temporal-decay or all)')\n",
        "    p.add_argument('--seed', type=int, default=7, help='RNG seed')\n",
        "    p.add_argument('--log', type=str, default=None, help='Path to JSONL step log')\n",
        "    p.add_argument('--scenario', type=str, default='baseline',\n",
        "                   choices=['baseline', 'dynamic-ethics', 'temporal-decay', 'flip-goal', 'inject-noise', 'all'],\n",
        "                   help='Which friction(s) to enable')\n",
        "\n",
        "    args = p.parse_args()\n",
        "    cfg = make_config_from_args(args)\n",
        "    run_rollout(cfg)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ]
    }
  ]
}