{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyMkjX8aPRbNZ21O51Xd5tfD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OneFineStarstuff/Cosmic-Brilliance/blob/main/train_tachyon_pinn_dropout_py.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z6dcpp6ErKxC"
      },
      "outputs": [],
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "train_tachyon_pinn_dropout.py\n",
        "\n",
        "End‐to‐end script for a physics‐informed, dropout‐based TachyonAI:\n",
        "\n",
        "  • Synthetic data generator for (E, p, f) → (f_next, rate)\n",
        "  • PINN loss enforcing df/dt = p − E⋅f\n",
        "  • Model with residual skip, layer norm, dropout\n",
        "  • MC Dropout for uncertainty quantification\n",
        "  • Animated GIF of field evolution\n",
        "\n",
        "Usage:\n",
        "  pip install torch matplotlib pillow\n",
        "  python train_tachyon_pinn_dropout.py\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.animation as animation\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 1. Synthetic Data Generator\n",
        "# ------------------------------------------------------------------------------\n",
        "def generate_synthetic_tachyon_data(n_samples=12000, dt=0.1, seed=42):\n",
        "    np.random.seed(seed)\n",
        "    E = np.random.uniform(-1, 1, size=(n_samples,1)).astype(np.float32)\n",
        "    p = np.random.uniform(-1, 1, size=(n_samples,1)).astype(np.float32)\n",
        "    f = np.random.uniform(-1, 1, size=(n_samples,1)).astype(np.float32)\n",
        "    X = np.hstack([E, p, f])  # (n_samples,3)\n",
        "\n",
        "    f_next = f + dt * (p - E*f)\n",
        "    rate   = p*f - E\n",
        "    Y = np.hstack([f_next, rate]).astype(np.float32)\n",
        "    return X, Y\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 2. Physics‐Informed Neural Net with Dropout & Residual Skip\n",
        "# ------------------------------------------------------------------------------\n",
        "class TachyonPINN(nn.Module):\n",
        "    def __init__(self, input_dim=3, hidden_dim=64, output_dim=2, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.fc1      = nn.Linear(input_dim, hidden_dim)\n",
        "        self.norm1    = nn.LayerNorm(hidden_dim)\n",
        "        self.relu     = nn.ReLU()\n",
        "        self.drop     = nn.Dropout(dropout)\n",
        "        self.fc2      = nn.Linear(hidden_dim, output_dim)\n",
        "        self.skip     = nn.Linear(input_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h = self.fc1(x)\n",
        "        h = self.norm1(h)\n",
        "        h = self.relu(h)\n",
        "        h = self.drop(h)\n",
        "        out = self.fc2(h)\n",
        "        # residual skip from input\n",
        "        out = out + self.skip(x)\n",
        "        return out\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 3. MC Dropout Prediction for Uncertainty\n",
        "# ------------------------------------------------------------------------------\n",
        "def mc_dropout_predict(model, x, n_samples=50):\n",
        "    model.train()  # keep dropout on\n",
        "    preds = []\n",
        "    with torch.no_grad():\n",
        "        for _ in range(n_samples):\n",
        "            preds.append(model(x).cpu().numpy())\n",
        "    preds = np.stack(preds, axis=0)  # (n_samples, batch, 2)\n",
        "    mean = preds.mean(axis=0)\n",
        "    std  = preds.std(axis=0)\n",
        "    return mean, std\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 4. Training & Validation Setup\n",
        "# ------------------------------------------------------------------------------\n",
        "def prepare_dataloaders(batch_size=128, val_frac=0.2):\n",
        "    X, Y = generate_synthetic_tachyon_data()\n",
        "    X, Y = torch.from_numpy(X), torch.from_numpy(Y)\n",
        "    ds   = TensorDataset(X, Y)\n",
        "    n_val= int(len(ds)*val_frac)\n",
        "    n_trn= len(ds)-n_val\n",
        "    trn, val = random_split(ds, [n_trn, n_val])\n",
        "    trn_loader = DataLoader(trn, batch_size=batch_size, shuffle=True)\n",
        "    val_loader = DataLoader(val, batch_size=batch_size, shuffle=False)\n",
        "    return trn_loader, val_loader\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 5. Loss Function: Data + Physics Constraint\n",
        "# ------------------------------------------------------------------------------\n",
        "def pinn_loss(pred, true, inputs, dt=0.1, λ_phys=1.0):\n",
        "    # supervised MSE\n",
        "    mse = nn.MSELoss()(pred, true)\n",
        "    # physics: pred[:,0] ≈ f + dt*(p - E*f)\n",
        "    f    = inputs[:,2]\n",
        "    p    = inputs[:,1]\n",
        "    E    = inputs[:,0]\n",
        "    f_phys_target = f + dt*(p - E*f)\n",
        "    f_pred         = pred[:,0]\n",
        "    phys_loss      = nn.MSELoss()(f_pred, f_phys_target)\n",
        "    return mse + λ_phys * phys_loss, mse, phys_loss\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 6. Training Loop with Early Stopping & LR Scheduler\n",
        "# ------------------------------------------------------------------------------\n",
        "def train_model(model, trn_loader, val_loader,\n",
        "                epochs=200, lr=1e-3, weight_decay=1e-5,\n",
        "                λ_phys=1.0, max_patience=15, device='cpu'):\n",
        "\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr,\n",
        "                           weight_decay=weight_decay)\n",
        "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
        "        optimizer, mode='min', factor=0.5, patience=5)\n",
        "\n",
        "    best_val = float('inf')\n",
        "    patience  = 0\n",
        "    history = {'train':[], 'val':[]}\n",
        "\n",
        "    for ep in range(1, epochs+1):\n",
        "        # Training\n",
        "        model.train()\n",
        "        total_loss = total_mse = total_phys = 0\n",
        "        for xb, yb in trn_loader:\n",
        "            xb, yb = xb.to(device), yb.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            pred = model(xb)\n",
        "            loss, mse_l, phys_l = pinn_loss(pred, yb, xb, λ_phys=λ_phys)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()*xb.size(0)\n",
        "            total_mse  += mse_l.item()*xb.size(0)\n",
        "            total_phys+= phys_l.item()*xb.size(0)\n",
        "\n",
        "        train_loss = total_loss/len(trn_loader.dataset)\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        val_loss = 0\n",
        "        with torch.no_grad():\n",
        "            for xb, yb in val_loader:\n",
        "                xb, yb = xb.to(device), yb.to(device)\n",
        "                pred = model(xb)\n",
        "                loss, _, _ = pinn_loss(pred, yb, xb, λ_phys=λ_phys)\n",
        "                val_loss += loss.item()*xb.size(0)\n",
        "        val_loss /= len(val_loader.dataset)\n",
        "\n",
        "        scheduler.step(val_loss)\n",
        "        history['train'].append(train_loss)\n",
        "        history['val'].append(val_loss)\n",
        "\n",
        "        # Early stopping\n",
        "        if val_loss < best_val:\n",
        "            best_val = val_loss\n",
        "            patience  = 0\n",
        "            torch.save(model.state_dict(), 'best_model.pt')\n",
        "        else:\n",
        "            patience += 1\n",
        "            if patience >= max_patience:\n",
        "                print(f'Early stopping at epoch {ep}')\n",
        "                break\n",
        "\n",
        "        if ep % 10 == 0 or ep==1:\n",
        "            print(f'Epoch {ep:03d} | Train Loss {train_loss:.4e} | Val Loss {val_loss:.4e}')\n",
        "\n",
        "    print(f'Best Val Loss: {best_val:.4e}')\n",
        "    # load best\n",
        "    model.load_state_dict(torch.load('best_model.pt'))\n",
        "    return model, history\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 7. Animate Field Evolution & Save GIF\n",
        "# ------------------------------------------------------------------------------\n",
        "def animate_evolution(model, n_steps=50, n_curves=5, dt=0.1,\n",
        "                      save_path='tachyon_evolution.gif', device='cpu'):\n",
        "    # sample random initial conditions\n",
        "    X0 = np.random.uniform(-1,1,(n_curves,3)).astype(np.float32)\n",
        "    xs = torch.from_numpy(X0).to(device)\n",
        "    paths = np.zeros((n_curves, n_steps+1), dtype=np.float32)\n",
        "    paths[:,0] = X0[:,2]\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        x = xs\n",
        "        for t in range(1, n_steps+1):\n",
        "            pred = model(x)\n",
        "            f_next = pred[:,0].cpu().numpy()\n",
        "            paths[:,t] = f_next\n",
        "            # prepare next input: keep same E,p; update f\n",
        "            x = torch.stack([x[:,0], x[:,1], torch.from_numpy(f_next).to(device)], dim=1)\n",
        "\n",
        "    # Plot and animate\n",
        "    fig, ax = plt.subplots()\n",
        "    lines = [ax.plot([],[], lw=2)[0] for _ in range(n_curves)]\n",
        "    ax.set_xlim(0, n_steps*dt)\n",
        "    ax.set_ylim(paths.min()-0.1, paths.max()+0.1)\n",
        "    ax.set_xlabel('t')\n",
        "    ax.set_ylabel('f(t)')\n",
        "    ax.set_title('Tachyonic Field Evolution')\n",
        "\n",
        "    def init():\n",
        "        for line in lines:\n",
        "            line.set_data([],[])\n",
        "        return lines\n",
        "\n",
        "    def update(frame):\n",
        "        t = np.linspace(0, frame*dt, frame+1)\n",
        "        for i, line in enumerate(lines):\n",
        "            line.set_data(t, paths[i,:frame+1])\n",
        "        return lines\n",
        "\n",
        "    anim = animation.FuncAnimation(fig, update, frames=n_steps+1,\n",
        "                                   init_func=init, blit=True)\n",
        "    anim.save(save_path, writer='pillow', fps=10)\n",
        "    plt.close(fig)\n",
        "    print(f'Animation saved to {save_path}')\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 8. Main Execution\n",
        "# ------------------------------------------------------------------------------\n",
        "if __name__ == '__main__':\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    # data\n",
        "    train_loader, val_loader = prepare_dataloaders()\n",
        "\n",
        "    # model\n",
        "    model = TachyonPINN().to(device)\n",
        "\n",
        "    # train\n",
        "    model, history = train_model(model, train_loader, val_loader,\n",
        "                                 epochs=200, λ_phys=1.0, device=device)\n",
        "\n",
        "    # plot losses\n",
        "    plt.figure()\n",
        "    plt.plot(history['train'], label='Train Loss')\n",
        "    plt.plot(history['val'],   label='Val Loss')\n",
        "    plt.xlabel('Epoch'); plt.ylabel('Loss')\n",
        "    plt.legend(); plt.title('Training History')\n",
        "    plt.show()\n",
        "\n",
        "    # MC Dropout on validation set\n",
        "    xb, yb = next(iter(val_loader))\n",
        "    xb = xb.to(device)\n",
        "    mean, std = mc_dropout_predict(model, xb, n_samples=100)\n",
        "    print('MC Dropout—first 5 samples:')\n",
        "    for i in range(5):\n",
        "        print(f'Input: {xb[i].cpu().numpy()}')\n",
        "        print(f' Pred Mean: {mean[i]}  Std: {std[i]}')\n",
        "\n",
        "    # animation\n",
        "    animate_evolution(model, n_steps=80, n_curves=4, save_path='tachyon_evolution.gif', device=device)"
      ]
    }
  ]
}