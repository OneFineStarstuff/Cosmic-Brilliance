{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyP1azgWbhjdIMqIgSStsXh9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OneFineStarstuff/Cosmic-Brilliance/blob/main/unified_stack_py.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "# unified_stack.py — Metasystem + Dreamstack with real physics frames (single-file, notebook-safe)\n",
        "\n",
        "from __future__ import annotations\n",
        "\n",
        "import argparse\n",
        "import csv\n",
        "import json\n",
        "import logging\n",
        "import math\n",
        "import random\n",
        "import re\n",
        "import shutil\n",
        "import sys\n",
        "import time\n",
        "from dataclasses import dataclass, asdict, field\n",
        "from pathlib import Path\n",
        "from typing import Any, Dict, Iterable, List, Optional, Sequence, Tuple\n",
        "\n",
        "# Hard deps for physics + images\n",
        "import numpy as np\n",
        "import matplotlib\n",
        "matplotlib.use(\"Agg\")\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Optional: networkx for causal graph (metasystem)\n",
        "try:\n",
        "    import networkx as nx\n",
        "except Exception:\n",
        "    nx = None\n",
        "\n",
        "\n",
        "# ======================\n",
        "# Notebook-safe argv\n",
        "# ======================\n",
        "def sanitize_argv(argv: Sequence[str]) -> List[str]:\n",
        "    cleaned: List[str] = []\n",
        "    skip_next = False\n",
        "    for a in argv:\n",
        "        if skip_next:\n",
        "            skip_next = False\n",
        "            continue\n",
        "        if a == \"-f\":\n",
        "            skip_next = True\n",
        "            continue\n",
        "        if a.endswith(\".json\") and \"kernel\" in a:\n",
        "            continue\n",
        "        if \"ipykernel\" in a or \"jupyter\" in a:\n",
        "            continue\n",
        "        cleaned.append(a)\n",
        "    return cleaned\n",
        "\n",
        "def setup_logging(verbosity: int = 0) -> None:\n",
        "    lvl = logging.WARNING if verbosity <= 0 else logging.INFO if verbosity == 1 else logging.DEBUG\n",
        "    logging.basicConfig(level=lvl, format=\"%(levelname)s | %(message)s\")\n",
        "\n",
        "\n",
        "def ensure_dir(p: Path) -> None:\n",
        "    p.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "def write_json(path: Path, obj: Any) -> None:\n",
        "    ensure_dir(path.parent)\n",
        "    path.write_text(json.dumps(obj, indent=2), encoding=\"utf-8\")\n",
        "\n",
        "def write_csv(path: Path, headers: Sequence[str], rows: Iterable[Sequence[Any]]) -> None:\n",
        "    ensure_dir(path.parent)\n",
        "    with path.open(\"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
        "        w = csv.writer(f)\n",
        "        w.writerow(list(headers))\n",
        "        for r in rows:\n",
        "            w.writerow(list(r))\n",
        "\n",
        "\n",
        "# ======================\n",
        "# Metasystem foundations\n",
        "# ======================\n",
        "@dataclass\n",
        "class Reality:\n",
        "    space: str = \"3D Euclidean\"\n",
        "    time: str = \"linear\"\n",
        "    objects: List[str] = field(default_factory=lambda: [\"wave\",\"particle\",\"observer\"])\n",
        "    logic: str = \"quantum-decoherent\"\n",
        "    ethics: str = \"CEAS level-3\"\n",
        "    def mutate(self, *, logic: Optional[str]=None, add_objects: Optional[List[str]]=None, ethics: Optional[str]=None):\n",
        "        if logic: self.logic = logic\n",
        "        if add_objects:\n",
        "            for o in add_objects:\n",
        "                if o not in self.objects:\n",
        "                    self.objects.append(o)\n",
        "        if ethics: self.ethics = ethics\n",
        "    def validate(self) -> List[str]:\n",
        "        issues = []\n",
        "        if self.time not in (\"linear\",\"cyclic\",\"branching\"):\n",
        "            issues.append(\"unsupported time form\")\n",
        "        if self.logic not in (\"quantum-decoherent\",\"paraconsistent\",\"hybrid_consistent\"):\n",
        "            issues.append(\"unsupported logic\")\n",
        "        if not self.ethics.startswith(\"CEAS level-\"):\n",
        "            issues.append(\"ethics not CEAS-tiered\")\n",
        "        return issues\n",
        "\n",
        "@dataclass\n",
        "class ESystem:\n",
        "    A: Any\n",
        "    L: Any\n",
        "    R: Dict[str, Any]\n",
        "    D: Any\n",
        "    Sigma: Dict[str, Any]\n",
        "\n",
        "def MetaTransform(E: ESystem) -> ESystem:\n",
        "    R2 = dict(E.R)\n",
        "    if R2.get(\"logic\") == \"quantum-decoherent\":\n",
        "        R2[\"logic\"] = \"hybrid_consistent\"\n",
        "    Sigma2 = dict(E.Sigma); Sigma2[\"version\"] = Sigma2.get(\"version\", 0) + 1\n",
        "    return ESystem(A=E.A, L=E.L, R=R2, D=E.D, Sigma=Sigma2)\n",
        "\n",
        "@dataclass\n",
        "class HSystem:\n",
        "    I: Any\n",
        "    C: Dict[str, Any]\n",
        "    E: ESystem\n",
        "    Omega: Dict[str, Any]\n",
        "\n",
        "def Psi(H: HSystem, Sigma_t: Dict[str, Any]) -> HSystem:\n",
        "    C2 = dict(H.C); C2[\"policy_tag\"] = Sigma_t.get(\"policy\",\"default\")\n",
        "    return HSystem(I=H.I, C=C2, E=H.E, Omega={\"audit\":True,\"Σ\":Sigma_t})\n",
        "\n",
        "@dataclass\n",
        "class SSystem:\n",
        "    O: Dict[str, Any]\n",
        "    L: Any\n",
        "    R: Dict[str, Any]\n",
        "    E: ESystem\n",
        "\n",
        "def Phi(S: SSystem, Psi_t: HSystem) -> SSystem:\n",
        "    O2 = dict(S.O); O2[\"tick\"] = O2.get(\"tick\", 0) + 1\n",
        "    return SSystem(O=O2, L=Psi_t.C.get(\"policy_tag\",\"default\"), R=S.R, E=S.E)\n",
        "\n",
        "\n",
        "# ======================\n",
        "# Symbolic engine\n",
        "# ======================\n",
        "SYMBOL_TABLE = {\n",
        "    '∴✶': 'sacred',\n",
        "    'Æ': 'divine_agent',\n",
        "    'Δ⇌': 'translate_ontology',\n",
        "    '⊘∨': 'conditional_divinity',\n",
        "    '⊕†': 'atonement_merge',\n",
        "    '♲⊚': 'recursion_loop'\n",
        "}\n",
        "TOKEN_PATTERN = r'([∴✶ÆΔ⇌⊘∨⊕†♲⊚\\(\\),])|([A-Za-z_][A-Za-z0-9_\\-]*)|([^\\s])'\n",
        "\n",
        "def tokenize(text: str) -> List[str]:\n",
        "    ts = re.findall(TOKEN_PATTERN, text)\n",
        "    return [a or b or c for a,b,c in ts if (a or b or c)]\n",
        "\n",
        "def parse(tokens: List[str]) -> Dict[str, Any]:\n",
        "    expr: Dict[str, Any] = {}\n",
        "    i = 0\n",
        "    while i < len(tokens):\n",
        "        t = tokens[i]\n",
        "        if t in SYMBOL_TABLE:\n",
        "            expr[\"operation\"] = SYMBOL_TABLE[t]\n",
        "            if i+1 < len(tokens) and tokens[i+1] == '(':\n",
        "                i += 2\n",
        "                args, cur, depth = [], [], 1\n",
        "                while i < len(tokens) and depth > 0:\n",
        "                    tok = tokens[i]\n",
        "                    if tok == '(':\n",
        "                        depth += 1; cur.append(tok)\n",
        "                    elif tok == ')':\n",
        "                        depth -= 1\n",
        "                        if depth == 0:\n",
        "                            if cur: args.append(''.join(cur).strip()); break\n",
        "                            else: break\n",
        "                        cur.append(tok)\n",
        "                    elif tok == ',' and depth == 1:\n",
        "                        args.append(''.join(cur).strip()); cur=[]\n",
        "                    else:\n",
        "                        cur.append(tok)\n",
        "                    i += 1\n",
        "                expr[\"arguments\"] = args\n",
        "        i += 1\n",
        "    return expr\n",
        "\n",
        "def interpret(expr: Dict[str, Any]) -> str:\n",
        "    op = expr.get(\"operation\"); args = expr.get(\"arguments\", [])\n",
        "    if op == \"sacred\":\n",
        "        subj = args[0] if args else \"Unknown\"\n",
        "        return f\"[Declaration] '{subj}' marked as sacred.\"\n",
        "    if op == \"atonement_merge\":\n",
        "        a,b = (args+[\"X\",\"Y\"])[:2]; return f\"[Ethical Merge] Attempting reconciliation between '{a}' and '{b}'.\"\n",
        "    if op == \"translate_ontology\":\n",
        "        a,b = (args+[\"A\",\"B\"])[:2]; return f\"[Translation] Initiating ontology shift: {a} ↔ {b}\"\n",
        "    if op == \"conditional_divinity\":\n",
        "        subj = args[0] if args else \"proposition\"\n",
        "        return f\"[Conditional Action] Executing divine function if '{subj}' is ethically coherent.\"\n",
        "    if op == \"recursion_loop\":\n",
        "        return \"[Recursion] Loop authorized.\"\n",
        "    return \"[Unknown Operation]\"\n",
        "\n",
        "\n",
        "# ======================\n",
        "# Physics: 2D φ⁴ field\n",
        "# ======================\n",
        "@dataclass\n",
        "class PhysConfig:\n",
        "    nx: int = 128\n",
        "    ny: int = 128\n",
        "    dx: float = 1.0\n",
        "    dt: float = 0.05\n",
        "    c: float = 1.0\n",
        "    lam: float = 1.0\n",
        "    v: float = 1.0\n",
        "    eta: float = 0.01\n",
        "    init: str = \"random\"  # random|blob|bias\n",
        "    random_amp: float = 0.05\n",
        "    blob_sigma: float = 18.0\n",
        "    bias_value: float = 0.9\n",
        "    cmap: str = \"seismic\"\n",
        "\n",
        "def laplacian_5pt(a: np.ndarray, dx: float) -> np.ndarray:\n",
        "    return (np.roll(a,1,0)+np.roll(a,-1,0)+np.roll(a,1,1)+np.roll(a,-1,1)-4*a)/(dx*dx)\n",
        "\n",
        "def dV_dphi(phi: np.ndarray, lam: float, v: float) -> np.ndarray:\n",
        "    return lam * phi * (phi*phi - v*v)\n",
        "\n",
        "def gradients_central(a: np.ndarray, dx: float):\n",
        "    gx = (np.roll(a,-1,0) - np.roll(a,1,0))/(2*dx)\n",
        "    gy = (np.roll(a,-1,1) - np.roll(a,1,1))/(2*dx)\n",
        "    return gx, gy\n",
        "\n",
        "def energy_density(phi: np.ndarray, phi_prev: np.ndarray, cfg: PhysConfig) -> np.ndarray:\n",
        "    vel = (phi - phi_prev) / cfg.dt\n",
        "    Ek = 0.5 * (vel*vel)\n",
        "    gx, gy = gradients_central(phi, cfg.dx)\n",
        "    Eg = 0.5 * (cfg.c*cfg.c) * (gx*gx + gy*gy)\n",
        "    Ep = 0.25 * cfg.lam * (phi*phi - cfg.v*cfg.v)**2\n",
        "    return Ek + Eg + Ep\n",
        "\n",
        "def total_energy(phi: np.ndarray, phi_prev: np.ndarray, cfg: PhysConfig) -> float:\n",
        "    return float(energy_density(phi, phi_prev, cfg).sum() * cfg.dx * cfg.dx)\n",
        "\n",
        "def init_field(cfg: PhysConfig, rng: np.random.Generator, shape: Tuple[int,int]) -> np.ndarray:\n",
        "    nx, ny = shape\n",
        "    if cfg.init == \"random\":\n",
        "        return cfg.random_amp * (rng.random((nx, ny)) - 0.5)\n",
        "    elif cfg.init == \"blob\":\n",
        "        i = np.arange(nx); j = np.arange(ny)\n",
        "        ii, jj = np.meshgrid(i, j, indexing=\"ij\")\n",
        "        r2 = (ii - (nx-1)/2.0)**2 + (jj - (ny-1)/2.0)**2\n",
        "        return np.exp(-r2/(2.0*cfg.blob_sigma*cfg.blob_sigma))\n",
        "    elif cfg.init == \"bias\":\n",
        "        return np.full((nx, ny), cfg.bias_value, dtype=np.float32)\n",
        "    else:\n",
        "        return cfg.random_amp * (rng.random((nx, ny)) - 0.5)\n",
        "\n",
        "def simulate_phi4_frames(out_dir: Path, frames: int, phys: PhysConfig, seed: int, save_png: bool=True) -> Dict[str, Any]:\n",
        "    log = logging.getLogger(\"physics\")\n",
        "    rng = np.random.default_rng(seed)\n",
        "    nx, ny = phys.nx, phys.ny\n",
        "    phi = init_field(phys, rng, (nx, ny)).astype(np.float32)\n",
        "    phi_prev = phi.copy()\n",
        "\n",
        "    # Stability hint\n",
        "    s = phys.c * phys.dt / phys.dx\n",
        "    if s > 1.0 / math.sqrt(2.0):\n",
        "        log.warning(\"CFL advisory: c*dt/dx=%.3f > 1/sqrt(2). Consider smaller dt.\", s)\n",
        "\n",
        "    frame_dir = out_dir\n",
        "    ensure_dir(frame_dir)\n",
        "\n",
        "    energies = []\n",
        "    for t in range(frames):\n",
        "        lap = laplacian_5pt(phi, phys.dx)\n",
        "        force = (phys.c*phys.c)*lap - dV_dphi(phi, phys.lam, phys.v)\n",
        "        damp = phys.eta * phys.dt\n",
        "        phi_new = (2.0 - damp) * phi - (1.0 - damp) * phi_prev + (phys.dt*phys.dt) * force\n",
        "\n",
        "        # Small stochastic kick for diversity\n",
        "        phi_new += 0.001 * rng.standard_normal(phi.shape).astype(np.float32)\n",
        "\n",
        "        # rotate buffers\n",
        "        phi_prev, phi = phi, phi_new\n",
        "\n",
        "        E = total_energy(phi, phi_prev, phys)\n",
        "        energies.append(E)\n",
        "\n",
        "        if save_png:\n",
        "            fig = plt.figure(figsize=(4,4), dpi=120)\n",
        "            ax = fig.add_axes([0,0,1,1])\n",
        "            im = ax.imshow(phi.T, cmap=phys.cmap, origin=\"lower\", interpolation=\"bilinear\")\n",
        "            ax.set_axis_off()\n",
        "            fig.savefig(frame_dir / f\"frame_{t:04d}.png\", bbox_inches=\"tight\", pad_inches=0)\n",
        "            plt.close(fig)\n",
        "\n",
        "    # Fitness: low energy variance + structure variance\n",
        "    arrE = np.array(energies, dtype=np.float64)\n",
        "    fitness = float(-arrE.std() + 0.001*arrE.mean())\n",
        "    return {\"frames\": frames, \"fitness\": fitness, \"E_mean\": float(arrE.mean()), \"E_std\": float(arrE.std())}\n",
        "\n",
        "\n",
        "# ======================\n",
        "# Feature extraction + k-means\n",
        "# ======================\n",
        "def radial_power_spectrum(img: np.ndarray, nbins: int=32) -> np.ndarray:\n",
        "    # img: 2D float array\n",
        "    F = np.fft.fftn(img)\n",
        "    P = np.abs(F)**2\n",
        "    h, w = P.shape\n",
        "    y, x = np.indices((h, w))\n",
        "    cy, cx = (h-1)/2.0, (w-1)/2.0\n",
        "    r = np.sqrt((y-cy)**2 + (x-cx)**2)\n",
        "    r /= (r.max() + 1e-9)\n",
        "    bins = np.linspace(0, 1.0, nbins+1)\n",
        "    feat = np.zeros(nbins, dtype=np.float64)\n",
        "    for i in range(nbins):\n",
        "        mask = (r >= bins[i]) & (r < bins[i+1])\n",
        "        feat[i] = P[mask].mean() if np.any(mask) else 0.0\n",
        "    feat = np.log1p(feat)\n",
        "    feat /= (np.linalg.norm(feat) + 1e-12)\n",
        "    return feat\n",
        "\n",
        "def load_png_as_gray(path: Path) -> np.ndarray:\n",
        "    img = matplotlib.image.imread(path)\n",
        "    if img.ndim == 3:\n",
        "        img = img[..., :3].mean(axis=-1)\n",
        "    img = img.astype(np.float32)\n",
        "    if img.max() > 1.0:\n",
        "        img /= 255.0\n",
        "    return img\n",
        "\n",
        "def kmeans(X: np.ndarray, k: int, iters: int=10, seed: int=42) -> Tuple[np.ndarray, np.ndarray]:\n",
        "    rng = np.random.default_rng(seed)\n",
        "    n, d = X.shape\n",
        "    k = max(1, min(k, n))\n",
        "    idx = rng.choice(n, size=k, replace=False)\n",
        "    C = X[idx].copy()\n",
        "    labels = np.zeros(n, dtype=np.int32)\n",
        "    for _ in range(iters):\n",
        "        # assign\n",
        "        dists = np.linalg.norm(X[:, None, :] - C[None, :, :], axis=2)\n",
        "        labels = np.argmin(dists, axis=1)\n",
        "        # update\n",
        "        for j in range(k):\n",
        "            mask = (labels == j)\n",
        "            if mask.any():\n",
        "                C[j] = X[mask].mean(axis=0)\n",
        "    return C, labels\n",
        "\n",
        "\n",
        "# ======================\n",
        "# Dreamstack-ish pipeline (with real physics PNG frames)\n",
        "# ======================\n",
        "def ds_run(out_dir: Path, steps: int, survivors: int, frames_per: int, seed: Optional[int],\n",
        "           phys_cfg: Optional[PhysConfig]=None) -> Dict[str, Any]:\n",
        "    log = logging.getLogger(\"ds.run\")\n",
        "    ensure_dir(out_dir / \"runs\")\n",
        "\n",
        "    # Metasystem: initial reality and transforms recorded with the run\n",
        "    R = Reality()\n",
        "    issues_before = R.validate()\n",
        "    R.mutate(logic=\"paraconsistent\", add_objects=[\"meta-observer\"], ethics=\"CEAS level-4\")\n",
        "    issues_after = R.validate()\n",
        "\n",
        "    E0 = ESystem(A=\"A0\", L=\"L0\", R=asdict(R), D=\"D0\", Sigma={\"policy\":\"hybrid_consistent\",\"version\":0})\n",
        "    E1 = MetaTransform(E0)\n",
        "    H0 = HSystem(I=\"I0\", C={\"controller\":\"Ψ0\"}, E=E1, Omega={})\n",
        "    H1 = Psi(H0, E1.Sigma)\n",
        "    S0 = SSystem(O={\"tick\":0}, L=\"L0\", R=asdict(R), E=E1)\n",
        "    S1 = Phi(S0, H1)\n",
        "\n",
        "    write_json(out_dir / \"runs\" / \"metasystem.json\", {\n",
        "        \"issues_before\": issues_before, \"issues_after\": issues_after,\n",
        "        \"E0\": asdict(E0), \"E1\": asdict(E1),\n",
        "        \"H0\": {\"I\":H0.I,\"C\":H0.C,\"E\":asdict(H0.E),\"Omega\":H0.Omega},\n",
        "        \"H1\": {\"I\":H1.I,\"C\":H1.C,\"E\":asdict(H1.E),\"Omega\":H1.Omega},\n",
        "        \"S0\": {\"O\":S0.O,\"L\":S0.L,\"R\":S0.R},\n",
        "        \"S1\": {\"O\":S1.O,\"L\":S1.L,\"R\":S1.R},\n",
        "    })\n",
        "\n",
        "    rng = random.Random(seed)\n",
        "    phys = phys_cfg or PhysConfig()\n",
        "\n",
        "    survivors_meta = []\n",
        "    for s in range(survivors):\n",
        "        sid = f\"survivor_{s:03d}\"\n",
        "        sdir = out_dir / \"runs\" / sid / \"frames\"\n",
        "        ensure_dir(sdir)\n",
        "\n",
        "        # Slightly vary physics per survivor for diversity\n",
        "        ph = PhysConfig(\n",
        "            nx=phys.nx, ny=phys.ny, dx=phys.dx, dt=phys.dt,\n",
        "            c=phys.c * (0.95 + 0.1*rng.random()),\n",
        "            lam=phys.lam * (0.8 + 0.4*rng.random()),\n",
        "            v=phys.v,\n",
        "            eta=phys.eta,\n",
        "            init=random.choice([\"random\",\"blob\",\"bias\"]),\n",
        "            random_amp=phys.random_amp,\n",
        "            blob_sigma=phys.blob_sigma,\n",
        "            bias_value=phys.bias_value,\n",
        "            cmap=phys.cmap\n",
        "        )\n",
        "\n",
        "        stats = simulate_phi4_frames(sdir, frames_per, ph, seed=rng.randrange(10**9), save_png=True)\n",
        "        survivors_meta.append({\"id\": sid, \"fitness\": stats[\"fitness\"], \"frames\": frames_per,\n",
        "                               \"E_mean\": stats[\"E_mean\"], \"E_std\": stats[\"E_std\"]})\n",
        "\n",
        "    write_json(out_dir / \"runs\" / \"survivors.json\", survivors_meta)\n",
        "    log.info(\"Run: %d survivors × %d frames (%dx%d grid)\", survivors, frames_per, phys.nx, phys.ny)\n",
        "    return {\"count\": len(survivors_meta)}\n",
        "\n",
        "def ds_visualize(out_dir: Path, top_n: int) -> Dict[str, Any]:\n",
        "    meta_path = out_dir / \"runs\" / \"survivors.json\"\n",
        "    data = json.loads(meta_path.read_text(encoding=\"utf-8\"))\n",
        "    data.sort(key=lambda x: x[\"fitness\"], reverse=True)\n",
        "    rows = [[i, d[\"id\"], d[\"fitness\"], d[\"frames\"], d[\"E_mean\"], d[\"E_std\"]] for i, d in enumerate(data)]\n",
        "    write_csv(out_dir / \"visualize\" / \"summary.csv\", [\"rank\",\"id\",\"fitness\",\"frames\",\"E_mean\",\"E_std\"], rows)\n",
        "    write_json(out_dir / \"visualize\" / \"top.json\", data[:max(0, top_n)])\n",
        "    return {\"summarized\": len(rows), \"top\": min(top_n, len(rows))}\n",
        "\n",
        "def ds_compress(out_dir: Path, k: int, scope: str, top_n: int, seed: Optional[int]) -> Dict[str, Any]:\n",
        "    top = json.loads((out_dir / \"visualize\" / \"top.json\").read_text(encoding=\"utf-8\"))\n",
        "    considered = top[:max(0, top_n)] if scope == \"local\" else top\n",
        "\n",
        "    # Collect features from first N frames of each considered survivor\n",
        "    feats: List[np.ndarray] = []\n",
        "    idx_map: List[Tuple[str, int]] = []\n",
        "    for entry in considered:\n",
        "        sid = entry[\"id\"]\n",
        "        fdir = out_dir / \"runs\" / sid / \"frames\"\n",
        "        files = sorted(fdir.glob(\"frame_*.png\"))[:16]  # sample up to 16 frames\n",
        "        for i, fp in enumerate(files):\n",
        "            img = load_png_as_gray(fp)\n",
        "            feat = radial_power_spectrum(img, nbins=48)\n",
        "            feats.append(feat)\n",
        "            idx_map.append((sid, i))\n",
        "    if not feats:\n",
        "        write_json(out_dir / \"compress\" / \"motifs.json\", {\"k\": k, \"counts\": [], \"centroids\": [], \"assignments\": []})\n",
        "        return {\"k\": k, \"considered\": 0}\n",
        "\n",
        "    X = np.vstack(feats)\n",
        "    C, labels = kmeans(X, k=k, iters=10, seed=seed or 42)\n",
        "\n",
        "    counts = [int((labels == j).sum()) for j in range(C.shape[0])]\n",
        "    obj = {\n",
        "        \"k\": int(C.shape[0]),\n",
        "        \"counts\": counts,\n",
        "        \"assignments\": [{\"sid\": sid, \"frame_idx\": int(fi), \"label\": int(l)} for (sid, fi), l in zip(idx_map, labels)],\n",
        "        \"centroids\": C.tolist(),\n",
        "    }\n",
        "    write_json(out_dir / \"compress\" / \"motifs.json\", obj)\n",
        "    return {\"k\": int(C.shape[0]), \"considered\": len(considered)}\n",
        "\n",
        "def ds_replay(out_dir: Path, top_n: int, copy_assets: bool) -> Dict[str, Any]:\n",
        "    top = json.loads((out_dir / \"visualize\" / \"top.json\").read_text(encoding=\"utf-8\"))\n",
        "    sel = top[:max(0, top_n)]\n",
        "    rdir = out_dir / \"replay\"; ensure_dir(rdir)\n",
        "    total_png = 0\n",
        "    for m in sel:\n",
        "        sid = m[\"id\"]\n",
        "        src = out_dir / \"runs\" / sid / \"frames\"\n",
        "        dst = rdir / sid; ensure_dir(dst)\n",
        "        write_json(dst / \"replay.json\", {\"id\": sid, \"source\": str(src)})\n",
        "        if copy_assets and src.exists():\n",
        "            dframes = dst / \"frames\"; ensure_dir(dframes)\n",
        "            for fp in sorted(src.glob(\"frame_*.png\")):\n",
        "                shutil.copy2(fp, dframes / fp.name)\n",
        "                total_png += 1\n",
        "    return {\"replays\": len(sel), \"copied_png\": total_png}\n",
        "\n",
        "def ds_fuse(out_dir: Path, top_n: int, strategy: str) -> Dict[str, Any]:\n",
        "    top = json.loads((out_dir / \"visualize\" / \"top.json\").read_text(encoding=\"utf-8\"))\n",
        "    motifs = json.loads((out_dir / \"compress\" / \"motifs.json\").read_text(encoding=\"utf-8\"))\n",
        "    sel = top[:max(0, top_n)]\n",
        "    if strategy == \"average\":\n",
        "        score_hint = float(np.mean([s[\"fitness\"] for s in sel])) if sel else 0.0\n",
        "    elif strategy == \"best\":\n",
        "        score_hint = float(max([s[\"fitness\"] for s in sel])) if sel else 0.0\n",
        "    else:  # mix\n",
        "        scores = [s[\"fitness\"] for s in sel]\n",
        "        score_hint = float(np.median(scores)) if scores else 0.0\n",
        "    fused = {\n",
        "        \"strategy\": strategy,\n",
        "        \"survivors\": [m[\"id\"] for m in sel],\n",
        "        \"motifs\": motifs.get(\"motifs\", []),\n",
        "        \"score_hint\": score_hint,\n",
        "        \"created_at\": time.time(),\n",
        "    }\n",
        "    write_json(out_dir / \"fuse\" / \"fused_config.json\", fused)\n",
        "    return {\"fused\": len(sel)}\n",
        "\n",
        "# ======================\n",
        "# High-level pipeline (+ metasystem extras)\n",
        "# ======================\n",
        "def pipeline(out_dir: Path, steps: int, survivors: int, frames_per: int,\n",
        "             top_n: int, k: int, scope: str, copy_assets: bool,\n",
        "             strategy: str, seed: Optional[int]) -> int:\n",
        "    log = logging.getLogger(\"pipeline\")\n",
        "    log.info(\"Pipeline start\")\n",
        "    ds_run(out_dir, steps, survivors, frames_per, seed)\n",
        "    ds_visualize(out_dir, top_n)\n",
        "    ds_compress(out_dir, k, scope, top_n, seed)\n",
        "    ds_replay(out_dir, top_n, copy_assets)\n",
        "    ds_fuse(out_dir, top_n, strategy)\n",
        "\n",
        "    # Metasystem artifacts: causal graph, ethics gate, symbolic evals\n",
        "    if nx is not None:\n",
        "        G = nx.DiGraph()\n",
        "        G.add_edges_from([(\"E1\",\"E2\"),(\"E2\",\"E3\"),(\"E1\",\"E4\")])\n",
        "        G.nodes[\"E1\"][\"state\"] = {\"object\":\"observer\",\"property\":\"init\"}\n",
        "        G.nodes[\"E2\"][\"state\"] = {\"object\":\"field\",\"interaction\":\"gravity\"}\n",
        "        past = sorted(nx.ancestors(G, \"E3\"))\n",
        "        write_json(out_dir / \"metasystem\" / \"causal_past_E3.json\", {\"past\": past})\n",
        "    else:\n",
        "        write_json(out_dir / \"metasystem\" / \"causal_past_E3.json\", {\"past\": [], \"note\": \"networkx not installed\"})\n",
        "\n",
        "    gate = \"Action Permitted under Δ⇌ logic\"\n",
        "    write_json(out_dir / \"metasystem\" / \"ethics_gate.json\", {\"decision\": gate})\n",
        "\n",
        "    texts = [\"⊘∨(paradox)\", \"⊕†(clarity, loss)\", \"∴✶(Emergence)\", \"Δ⇌(Paradox_Tolerance, Coherence)\"]\n",
        "    evals = []\n",
        "    for t in texts:\n",
        "        expr = parse(tokenize(t)); evals.append({\"text\": t, \"result\": interpret(expr)})\n",
        "    write_json(out_dir / \"metasystem\" / \"symbols.json\", {\"evaluations\": evals})\n",
        "\n",
        "    log.info(\"Pipeline complete\")\n",
        "    return 0\n",
        "\n",
        "\n",
        "# ======================\n",
        "# CLI\n",
        "# ======================\n",
        "def build_parser() -> argparse.ArgumentParser:\n",
        "    p = argparse.ArgumentParser(\n",
        "        prog=\"unified_stack\",\n",
        "        description=\"Metasystem + Dreamstack unified pipeline (notebook-safe)\"\n",
        "    )\n",
        "    p.add_argument(\"-v\", \"--verbose\", action=\"count\", default=0,\n",
        "                   help=\"Increase verbosity (-v, -vv)\")\n",
        "\n",
        "    sub = p.add_subparsers(dest=\"cmd\")  # not required; main handles help on no cmd\n",
        "\n",
        "    # run\n",
        "    pr = sub.add_parser(\"run\", help=\"Run dreamstack with real physics frames (PNG)\")\n",
        "    pr.add_argument(\"--out-dir\", type=Path, default=Path(\"out_unified\"), dest=\"out_dir\")\n",
        "    pr.add_argument(\"--steps\", type=int, default=200)\n",
        "    pr.add_argument(\"--survivors\", type=int, default=5)\n",
        "    pr.add_argument(\"--frames-per\", type=int, default=32, dest=\"frames_per\")\n",
        "    pr.add_argument(\"--seed\", type=int, default=None)\n",
        "    pr.add_argument(\"--nx\", type=int, default=128)\n",
        "    pr.add_argument(\"--ny\", type=int, default=128)\n",
        "    pr.add_argument(\"--dt\", type=float, default=0.05)\n",
        "    pr.add_argument(\"--c\", type=float, default=1.0)\n",
        "    pr.add_argument(\"--lam\", type=float, default=1.0)\n",
        "    pr.add_argument(\"--v\", type=float, default=1.0)\n",
        "    pr.add_argument(\"--eta\", type=float, default=0.01)\n",
        "    pr.add_argument(\"--init\", choices=[\"random\",\"blob\",\"bias\"], default=\"random\")\n",
        "    pr.add_argument(\"--cmap\", type=str, default=\"seismic\")\n",
        "    pr.set_defaults(func=lambda ns: ds_run(\n",
        "        ns.out_dir, ns.steps, ns.survivors, ns.frames_per, ns.seed,\n",
        "        PhysConfig(nx=ns.nx, ny=ns.ny, dt=ns.dt, c=ns.c, lam=ns.lam, v=ns.v, eta=ns.eta, init=ns.init, cmap=ns.cmap)\n",
        "    ) or 0)\n",
        "\n",
        "    # visualize\n",
        "    pv = sub.add_parser(\"visualize\", help=\"Summarize survivors\")\n",
        "    pv.add_argument(\"--out-dir\", type=Path, default=Path(\"out_unified\"), dest=\"out_dir\")\n",
        "    pv.add_argument(\"--top-n\", type=int, default=3, dest=\"top_n\")\n",
        "    pv.set_defaults(func=lambda ns: ds_visualize(ns.out_dir, ns.top_n) or 0)\n",
        "\n",
        "    # compress\n",
        "    pc = sub.add_parser(\"compress\", help=\"Compress frame PNGs into spectral motifs\")\n",
        "    pc.add_argument(\"--out-dir\", type=Path, default=Path(\"out_unified\"), dest=\"out_dir\")\n",
        "    pc.add_argument(\"--k\", type=int, default=8)\n",
        "    pc.add_argument(\"--scope\", choices=(\"local\",\"global\"), default=\"local\")\n",
        "    pc.add_argument(\"--top-n\", type=int, default=3, dest=\"top_n\")\n",
        "    pc.add_argument(\"--seed\", type=int, default=None)\n",
        "    pc.set_defaults(func=lambda ns: ds_compress(ns.out_dir, ns.k, ns.scope, ns.top_n, ns.seed) or 0)\n",
        "\n",
        "    # replay\n",
        "    prp = sub.add_parser(\"replay\", help=\"Prepare replays (optionally copy PNGs)\")\n",
        "    prp.add_argument(\"--out-dir\", type=Path, default=Path(\"out_unified\"), dest=\"out_dir\")\n",
        "    prp.add_argument(\"--top-n\", type=int, default=3, dest=\"top_n\")\n",
        "    prp.add_argument(\"--copy\", action=\"store_true\")\n",
        "    prp.set_defaults(func=lambda ns: ds_replay(ns.out_dir, ns.top_n, ns.copy) or 0)\n",
        "\n",
        "    # fuse\n",
        "    pf = sub.add_parser(\"fuse\", help=\"Fuse configs\")\n",
        "    pf.add_argument(\"--out-dir\", type=Path, default=Path(\"out_unified\"), dest=\"out_dir\")\n",
        "    pf.add_argument(\"--top-n\", type=int, default=3, dest=\"top_n\")\n",
        "    pf.add_argument(\"--strategy\", choices=(\"average\",\"best\",\"mix\"), default=\"mix\")\n",
        "    pf.set_defaults(func=lambda ns: ds_fuse(ns.out_dir, ns.top_n, ns.strategy) or 0)\n",
        "\n",
        "    # pipeline\n",
        "    pp = sub.add_parser(\"pipeline\", help=\"Run full pipeline + metasystem artifacts\")\n",
        "    pp.add_argument(\"--out-dir\", type=Path, default=Path(\"out_unified\"), dest=\"out_dir\")\n",
        "    pp.add_argument(\"--steps\", type=int, default=200)\n",
        "    pp.add_argument(\"--survivors\", type=int, default=4)\n",
        "    pp.add_argument(\"--frames-per\", type=int, default=32, dest=\"frames_per\")\n",
        "    pp.add_argument(\"--top-n\", type=int, default=2, dest=\"top_n\")\n",
        "    pp.add_argument(\"--k\", type=int, default=6)\n",
        "    pp.add_argument(\"--scope\", choices=(\"local\",\"global\"), default=\"local\")\n",
        "    pp.add_argument(\"--copy\", action=\"store_true\")\n",
        "    pp.add_argument(\"--strategy\", choices=(\"average\",\"best\",\"mix\"), default=\"mix\")\n",
        "    pp.add_argument(\"--seed\", type=int, default=None)\n",
        "    pp.set_defaults(func=lambda ns: pipeline(\n",
        "        ns.out_dir, ns.steps, ns.survivors, ns.frames_per,\n",
        "        ns.top_n, ns.k, ns.scope, ns.copy, ns.strategy, ns.seed\n",
        "    ))\n",
        "\n",
        "    return p\n",
        "\n",
        "def main(argv: Optional[List[str]] = None, *, exit: bool = True) -> int:\n",
        "    raw = sys.argv[1:] if argv is None else argv\n",
        "    argv_clean = sanitize_argv(raw)\n",
        "    parser = build_parser()\n",
        "\n",
        "    # Graceful help on no subcommand\n",
        "    if not argv_clean or (argv_clean and argv_clean[0].startswith(\"-\") and len(argv_clean) <= 2):\n",
        "        try:\n",
        "            ns, _ = parser.parse_known_args(argv_clean)\n",
        "            setup_logging(getattr(ns, \"verbose\", 0))\n",
        "        except SystemExit:\n",
        "            setup_logging(0)\n",
        "        parser.print_help()\n",
        "        if exit:\n",
        "            sys.exit(0)\n",
        "        return 0\n",
        "\n",
        "    try:\n",
        "        ns = parser.parse_args(argv_clean)\n",
        "    except SystemExit as e:\n",
        "        if exit:\n",
        "            raise\n",
        "        return int(getattr(e, \"code\", 2) or 0)\n",
        "\n",
        "    setup_logging(getattr(ns, \"verbose\", 0))\n",
        "    code = int(ns.func(ns))\n",
        "    if exit:\n",
        "        sys.exit(code)\n",
        "    return code\n",
        "\n",
        "def cli(command: str, /, **kwargs) -> int:\n",
        "    args: List[str] = [str(command)]\n",
        "    v = kwargs.pop(\"verbose\", None)\n",
        "    if isinstance(v, int) and v > 0:\n",
        "        args.extend([\"-\" + \"v\"*min(v,3)])\n",
        "    for k, val in kwargs.items():\n",
        "        flag = \"--\" + k.replace(\"_\",\"-\")\n",
        "        if isinstance(val, bool):\n",
        "            if val:\n",
        "                args.append(flag)\n",
        "        elif isinstance(val, (list, tuple)):\n",
        "            for item in val:\n",
        "                args.extend([flag, str(item)])\n",
        "        elif val is not None:\n",
        "            args.extend([flag, str(val)])\n",
        "    return main(args, exit=False)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "s94Z4w9o9y1h"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}