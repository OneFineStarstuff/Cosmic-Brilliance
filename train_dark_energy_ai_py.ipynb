{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyM5Ovv5YlJd35Ba1qvH0McW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OneFineStarstuff/Cosmic-Brilliance/blob/main/train_dark_energy_ai_py.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "train_dark_energy_ai.py\n",
        "\n",
        "Physics-informed neural network for dark energy optimization,\n",
        "all in double precision with normalized physics residual\n",
        "and corrected plot_uncertainty implementation.\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 1. Double-Precision Synthetic Dataset\n",
        "# ------------------------------------------------------------------------------\n",
        "class DarkEnergyDataset(Dataset):\n",
        "    def __init__(self, n_samples=6000, seed=0):\n",
        "        np.random.seed(seed)\n",
        "        vd = np.random.uniform(1e-10, 1e-8,  (n_samples,1)).astype(np.float64)\n",
        "        cc = np.random.uniform(1e-52, 1e-50,  (n_samples,1)).astype(np.float64)\n",
        "        qf = np.random.uniform(1e-5,   1e-3,   (n_samples,1)).astype(np.float64)\n",
        "        R  = np.random.uniform(-1e-35, 1e-35,  (n_samples,1)).astype(np.float64)\n",
        "        md = np.random.uniform(1e-27,  1e-25,  (n_samples,1)).astype(np.float64)\n",
        "        rd = np.random.uniform(1e-30,  1e-28,  (n_samples,1)).astype(np.float64)\n",
        "\n",
        "        X_raw = np.hstack([vd, cc, qf, R, md, rd])\n",
        "\n",
        "        sf = vd / (cc + 1e-60)\n",
        "        ce = 1.0 / (1.0 + qf * vd * 1e10)\n",
        "        ei = (md + rd) / (vd + 1e-60)\n",
        "        Y_raw = np.hstack([sf, ce, ei])\n",
        "\n",
        "        # add small noise\n",
        "        Y_raw += 0.02 * Y_raw.std(axis=0) * np.random.randn(*Y_raw.shape)\n",
        "\n",
        "        # compute normalization stats\n",
        "        self.X_mean = X_raw.mean(axis=0)\n",
        "        self.X_std  = X_raw.std(axis=0) + 1e-12\n",
        "        self.Y_mean = Y_raw.mean(axis=0)\n",
        "        self.Y_std  = Y_raw.std(axis=0) + 1e-12\n",
        "\n",
        "        X_norm = (X_raw - self.X_mean) / self.X_std\n",
        "        Y_norm = (Y_raw - self.Y_mean) / self.Y_std\n",
        "\n",
        "        self.X = torch.from_numpy(X_norm).double()\n",
        "        self.Y = torch.from_numpy(Y_norm).double()\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx], self.Y[idx]\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 2. Double-Precision Model Definition\n",
        "# ------------------------------------------------------------------------------\n",
        "class DarkEnergyAI(nn.Module):\n",
        "    def __init__(self, input_dim=6, hidden_dims=(128, 64), output_dim=3, p_drop=0.1):\n",
        "        super().__init__()\n",
        "        layers, dim = [], input_dim\n",
        "        for h in hidden_dims:\n",
        "            layers += [\n",
        "                nn.Linear(dim, h),\n",
        "                nn.LayerNorm(h),\n",
        "                nn.ReLU(),\n",
        "                nn.Dropout(p_drop),\n",
        "            ]\n",
        "            dim = h\n",
        "        layers.append(nn.Linear(dim, output_dim))\n",
        "        self.net = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 3. Normalized Physics-Informed Residual\n",
        "# ------------------------------------------------------------------------------\n",
        "def physics_residual(pred, X, stats):\n",
        "    # denormalize inputs\n",
        "    X_den = X * stats['X_std'] + stats['X_mean']\n",
        "    vd, cc = X_den[:, 0], X_den[:, 1]\n",
        "\n",
        "    # true stability factor\n",
        "    sf_true = vd / (cc + 1e-60)\n",
        "    # normalize sf_true\n",
        "    sf_true_norm = (sf_true - stats['Y_mean'][0]) / stats['Y_std'][0]\n",
        "\n",
        "    # compare normalized predictions to normalized target\n",
        "    sf_pred_norm = pred[:, 0]\n",
        "    return torch.mean((sf_pred_norm - sf_true_norm) ** 2)\n",
        "\n",
        "\n",
        "def total_loss(pred, true, X, stats, lam=1.0):\n",
        "    mse_loss = nn.MSELoss()(pred, true)\n",
        "    phys_loss = physics_residual(pred, X, stats)\n",
        "    return mse_loss + lam * phys_loss, mse_loss, phys_loss\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 4. MC-Dropout for Uncertainty Quantification\n",
        "# ------------------------------------------------------------------------------\n",
        "def mc_dropout_predict(model, X, T=50):\n",
        "    model.train()\n",
        "    preds = []\n",
        "    with torch.no_grad():\n",
        "        for _ in range(T):\n",
        "            preds.append(model(X))\n",
        "    arr = torch.stack(preds, dim=0)\n",
        "    return arr.mean(0), arr.std(0)\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 5. Training Loop with Early Stopping & Scheduler\n",
        "# ------------------------------------------------------------------------------\n",
        "def train(model, train_loader, val_loader, stats, device,\n",
        "          lr=1e-3, wd=1e-5, lam=1.0, epochs=100, patience=10):\n",
        "    model.to(device)\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=wd)\n",
        "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
        "        optimizer, mode='min', factor=0.5, patience=5\n",
        "    )\n",
        "\n",
        "    best_val, wait = float('inf'), 0\n",
        "    history = {'train': [], 'val': []}\n",
        "\n",
        "    for ep in range(1, epochs + 1):\n",
        "        model.train()\n",
        "        t_loss = 0.0\n",
        "        for Xb, Yb in train_loader:\n",
        "            Xb, Yb = Xb.to(device), Yb.to(device)\n",
        "            pred = model(Xb)\n",
        "            loss, _, _ = total_loss(pred, Yb, Xb, stats)\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            optimizer.step()\n",
        "            t_loss += loss.item() * Xb.size(0)\n",
        "        t_loss /= len(train_loader.dataset)\n",
        "\n",
        "        model.eval()\n",
        "        v_loss = 0.0\n",
        "        with torch.no_grad():\n",
        "            for Xb, Yb in val_loader:\n",
        "                Xb, Yb = Xb.to(device), Yb.to(device)\n",
        "                pred = model(Xb)\n",
        "                loss, _, _ = total_loss(pred, Yb, Xb, stats)\n",
        "                v_loss += loss.item() * Xb.size(0)\n",
        "        v_loss /= len(val_loader.dataset)\n",
        "\n",
        "        scheduler.step(v_loss)\n",
        "        history['train'].append(t_loss)\n",
        "        history['val'].append(v_loss)\n",
        "        print(f\"Epoch {ep:03d} | Train {t_loss:.4e} | Val {v_loss:.4e}\")\n",
        "\n",
        "        if v_loss + 1e-8 < best_val:\n",
        "            best_val, wait = v_loss, 0\n",
        "            torch.save(model.state_dict(), \"best_dark_energy_ai.pth\")\n",
        "        else:\n",
        "            wait += 1\n",
        "            if wait >= patience:\n",
        "                print(f\"Early stopping at epoch {ep}\")\n",
        "                break\n",
        "\n",
        "    model.load_state_dict(torch.load(\"best_dark_energy_ai.pth\"))\n",
        "    return history\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 6. Visualization Helpers\n",
        "# ------------------------------------------------------------------------------\n",
        "def plot_losses(hist):\n",
        "    plt.figure()\n",
        "    plt.plot(hist['train'], label='Train')\n",
        "    plt.plot(hist['val'],   label='Val')\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def plot_scatter(y_true, y_pred, title):\n",
        "    plt.figure()\n",
        "    plt.scatter(y_true, y_pred, s=5, alpha=0.6)\n",
        "    m, M = y_true.min(), y_true.max()\n",
        "    plt.plot([m, M], [m, M], 'r--')\n",
        "    plt.title(title)\n",
        "    plt.xlabel(\"True\")\n",
        "    plt.ylabel(\"Pred\")\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def plot_uncertainty(model, stats, device):\n",
        "    # 1) build vd-cc grid\n",
        "    vd_vals = torch.linspace(1e-10, 1e-8, 100,\n",
        "                             device=device, dtype=torch.double)\n",
        "    cc_vals = torch.linspace(1e-52, 1e-50, 100,\n",
        "                             device=device, dtype=torch.double)\n",
        "    VD, CC = torch.meshgrid(vd_vals, cc_vals, indexing='xy')\n",
        "    vd_flat, cc_flat = VD.reshape(-1), CC.reshape(-1)\n",
        "\n",
        "    # 2) other features at their mean\n",
        "    rest_mean = stats['X_mean'][2:]                               # shape [4]\n",
        "    rest = rest_mean.unsqueeze(0).repeat(vd_flat.shape[0], 1)     # shape [N,4]\n",
        "\n",
        "    # 3) assemble & standardize\n",
        "    Xg = torch.cat([vd_flat.unsqueeze(1),\n",
        "                    cc_flat.unsqueeze(1),\n",
        "                    rest], dim=1)\n",
        "    Xn = (Xg - stats['X_mean']) / stats['X_std']\n",
        "\n",
        "    # 4) MC-dropout uncertainty\n",
        "    _, std = mc_dropout_predict(model, Xn, T=100)\n",
        "    std_map = std[:, 0].reshape(VD.shape).cpu().numpy()\n",
        "\n",
        "    # 5) plot\n",
        "    plt.figure()\n",
        "    plt.pcolormesh(VD.cpu().numpy(),\n",
        "                   CC.cpu().numpy(),\n",
        "                   std_map,\n",
        "                   cmap='viridis',\n",
        "                   shading='auto')\n",
        "    plt.colorbar(label=\"Std of Stability Factor\")\n",
        "    plt.xlabel(\"Vacuum Density\")\n",
        "    plt.ylabel(\"Cosmological Constant\")\n",
        "    plt.title(\"Stability Uncertainty Heatmap\")\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 7. Main Execution\n",
        "# ------------------------------------------------------------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    ds = DarkEnergyDataset()\n",
        "    n_val = int(0.2 * len(ds))\n",
        "    train_ds, val_ds = random_split(ds, [len(ds) - n_val, n_val])\n",
        "    train_loader = DataLoader(train_ds, batch_size=128, shuffle=True)\n",
        "    val_loader   = DataLoader(val_ds,   batch_size=256)\n",
        "\n",
        "    stats = {\n",
        "        'X_mean': torch.tensor(ds.X_mean, dtype=torch.double, device=device),\n",
        "        'X_std':  torch.tensor(ds.X_std,  dtype=torch.double, device=device),\n",
        "        'Y_mean': torch.tensor(ds.Y_mean, dtype=torch.double, device=device),\n",
        "        'Y_std':  torch.tensor(ds.Y_std,  dtype=torch.double, device=device),\n",
        "    }\n",
        "\n",
        "    model = DarkEnergyAI().double().to(device)\n",
        "    history = train(model, train_loader, val_loader, stats, device,\n",
        "                    lr=1e-3, wd=1e-5, lam=1.0, epochs=100, patience=10)\n",
        "\n",
        "    plot_losses(history)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        X_all = ds.X.to(device)\n",
        "        Y_pred_norm = model(X_all).cpu().numpy()\n",
        "    Y_pred = Y_pred_norm * ds.Y_std + ds.Y_mean\n",
        "    Y_true = ds.Y.cpu().numpy() * ds.Y_std + ds.Y_mean\n",
        "\n",
        "    names = [\"Stability Factor\", \"Control Efficiency\", \"Expansion Index\"]\n",
        "    for i, nm in enumerate(names):\n",
        "        plot_scatter(Y_true[:, i], Y_pred[:, i], nm)\n",
        "\n",
        "    plot_uncertainty(model, stats, device)"
      ],
      "metadata": {
        "id": "Z1_oWQpyYk6k"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}