{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyPS5tHe+LF3h+0uZvvc71ju",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OneFineStarstuff/Cosmic-Brilliance/blob/main/exotic_ai_trainer_py.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NFnW3ox2wJzt"
      },
      "outputs": [],
      "source": [
        "#!/usr/bin/env python3\n",
        "# exotic_ai_trainer.py\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import json\n",
        "import time\n",
        "import argparse\n",
        "from dataclasses import dataclass, asdict\n",
        "from typing import Tuple, Optional\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# Reproducibility\n",
        "# -----------------------------\n",
        "def set_seeds(seed: int = 42):\n",
        "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    tf.random.set_seed(seed)\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# Data loaders\n",
        "# -----------------------------\n",
        "def load_mnist(kind: str = \"mnist\") -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, int, int]:\n",
        "    if kind == \"mnist\":\n",
        "        (x_train, y_train), (x_val, y_val) = tf.keras.datasets.mnist.load_data()\n",
        "    elif kind == \"fashion\":\n",
        "        (x_train, y_train), (x_val, y_val) = tf.keras.datasets.fashion_mnist.load_data()\n",
        "    else:\n",
        "        raise ValueError(\"Unsupported dataset. Use 'mnist' or 'fashion'.\")\n",
        "    x_train = x_train.reshape((-1, 784)).astype(np.float32) / 255.0\n",
        "    x_val = x_val.reshape((-1, 784)).astype(np.float32) / 255.0\n",
        "    return x_train, y_train, x_val, y_val, 784, 10\n",
        "\n",
        "\n",
        "def load_synthetic(\n",
        "    n_train=5000,\n",
        "    n_val=1000,\n",
        "    input_dim=30,\n",
        "    num_classes=10,\n",
        "    clusters=20,\n",
        "    seed=42\n",
        ") -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, int, int]:\n",
        "    rng = np.random.default_rng(seed)\n",
        "    centers = rng.normal(0.0, 3.0, size=(clusters, input_dim))\n",
        "    # Train\n",
        "    Xt, yt = [], []\n",
        "    for k in range(clusters):\n",
        "        n_k = n_train // clusters + (1 if k < n_train % clusters else 0)\n",
        "        cov = rng.uniform(0.3, 1.5)\n",
        "        Xk = centers[k] + rng.normal(0, cov, size=(n_k, input_dim))\n",
        "        yk = np.full(n_k, k % num_classes, dtype=np.int32)\n",
        "        Xt.append(Xk); yt.append(yk)\n",
        "    x_train = np.vstack(Xt).astype(np.float32)\n",
        "    y_train = np.concatenate(yt).astype(np.int32)\n",
        "    # Val\n",
        "    Xv, yv = [], []\n",
        "    for k in range(clusters):\n",
        "        n_k = n_val // clusters + (1 if k < n_val % clusters else 0)\n",
        "        cov = rng.uniform(0.3, 1.5)\n",
        "        Xk = centers[k] + rng.normal(0, cov, size=(n_k, input_dim))\n",
        "        yk = np.full(n_k, k % num_classes, dtype=np.int32)\n",
        "        Xv.append(Xk); yv.append(yk)\n",
        "    x_val = np.vstack(Xv).astype(np.float32)\n",
        "    y_val = np.concatenate(yv).astype(np.int32)\n",
        "    # Shuffle\n",
        "    p = rng.permutation(len(x_train)); x_train, y_train = x_train[p], y_train[p]\n",
        "    p = rng.permutation(len(x_val));   x_val, y_val   = x_val[p], y_val[p]\n",
        "    return x_train, y_train, x_val, y_val, input_dim, num_classes\n",
        "\n",
        "\n",
        "def make_tf_dataset(x, y, batch_size=128, shuffle=True, seed=42):\n",
        "    ds = tf.data.Dataset.from_tensor_slices((x, y))\n",
        "    if shuffle:\n",
        "        ds = ds.shuffle(min(len(x), 10000), seed=seed, reshuffle_each_iteration=True)\n",
        "    return ds.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# Model builder\n",
        "# -----------------------------\n",
        "def build_exotic_model(\n",
        "    input_dim: int,\n",
        "    num_classes: int = 10,\n",
        "    neg_units: int = 256,\n",
        "    hyper_units: int = 512,\n",
        "    negative_activation: str = \"tanh\",\n",
        "    hyper_activation: str = \"relu\",\n",
        "    dropout: float = 0.3,\n",
        "    batchnorm: bool = True,\n",
        "    l2_reg: float = 1e-4,\n",
        ") -> tf.keras.Model:\n",
        "    reg = tf.keras.regularizers.l2(l2_reg) if l2_reg and l2_reg > 0 else None\n",
        "\n",
        "    inputs = tf.keras.Input(shape=(input_dim,), name=\"inputs\")\n",
        "\n",
        "    # \"Negative-energy\" layer\n",
        "    x = tf.keras.layers.Dense(neg_units, use_bias=not batchnorm, kernel_regularizer=reg, name=\"neg_dense\")(inputs)\n",
        "    if batchnorm:\n",
        "        x = tf.keras.layers.BatchNormalization(name=\"neg_bn\")(x)\n",
        "    x = tf.keras.layers.Activation(negative_activation, name=\"neg_act\")(x)\n",
        "    if dropout and dropout > 0:\n",
        "        x = tf.keras.layers.Dropout(dropout, name=\"neg_dropout\")(x)\n",
        "\n",
        "    # \"Hyperdimensional\" layer\n",
        "    x = tf.keras.layers.Dense(hyper_units, use_bias=not batchnorm, kernel_regularizer=reg, name=\"hyper_dense\")(x)\n",
        "    if batchnorm:\n",
        "        x = tf.keras.layers.BatchNormalization(name=\"hyper_bn\")(x)\n",
        "    x = tf.keras.layers.Activation(hyper_activation, name=\"hyper_act\")(x)\n",
        "    if dropout and dropout > 0:\n",
        "        x = tf.keras.layers.Dropout(dropout, name=\"hyper_dropout\")(x)\n",
        "\n",
        "    outputs = tf.keras.layers.Dense(num_classes, activation=\"softmax\", kernel_regularizer=reg, name=\"logits\")(x)\n",
        "    return tf.keras.Model(inputs=inputs, outputs=outputs, name=\"ExoticAI\")\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# Config\n",
        "# -----------------------------\n",
        "@dataclass\n",
        "class Config:\n",
        "    dataset: str = \"synthetic\"        # synthetic | mnist | fashion\n",
        "    input_dim: int = 30               # used only for synthetic\n",
        "    num_classes: int = 10\n",
        "    epochs: int = 15\n",
        "    batch_size: int = 128\n",
        "    lr: float = 1e-3\n",
        "    dropout: float = 0.3\n",
        "    batchnorm: bool = True\n",
        "    l2_reg: float = 1e-4\n",
        "    neg_units: int = 256\n",
        "    hyper_units: int = 512\n",
        "    negative_activation: str = \"tanh\"\n",
        "    hyper_activation: str = \"relu\"\n",
        "    patience: int = 5\n",
        "    reduce_lr_patience: int = 3\n",
        "    seed: int = 42\n",
        "    results_dir: str = \"results\"\n",
        "    tag: Optional[str] = None\n",
        "    save_tflite: bool = False\n",
        "    save_onnx: bool = False\n",
        "\n",
        "\n",
        "def get_run_dir(cfg: Config) -> str:\n",
        "    ts = time.strftime(\"%Y%m%d-%H%M%S\")\n",
        "    tag = cfg.tag or cfg.dataset\n",
        "    run_dir = os.path.join(cfg.results_dir, f\"{ts}_{tag}\")\n",
        "    os.makedirs(run_dir, exist_ok=True)\n",
        "    return run_dir\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# Training\n",
        "# -----------------------------\n",
        "def train(cfg: Config):\n",
        "    set_seeds(cfg.seed)\n",
        "\n",
        "    # Data\n",
        "    if cfg.dataset in (\"mnist\", \"fashion\"):\n",
        "        x_train, y_train, x_val, y_val, input_dim, num_classes = load_mnist(cfg.dataset)\n",
        "    elif cfg.dataset == \"synthetic\":\n",
        "        x_train, y_train, x_val, y_val, input_dim, num_classes = load_synthetic(\n",
        "            n_train=5000, n_val=1000, input_dim=cfg.input_dim, num_classes=cfg.num_classes, seed=cfg.seed\n",
        "        )\n",
        "    else:\n",
        "        raise ValueError(\"dataset must be one of: synthetic | mnist | fashion\")\n",
        "\n",
        "    cfg.input_dim = input_dim\n",
        "    cfg.num_classes = num_classes\n",
        "\n",
        "    ds_train = make_tf_dataset(x_train, y_train, batch_size=cfg.batch_size, shuffle=True, seed=cfg.seed)\n",
        "    ds_val   = make_tf_dataset(x_val, y_val, batch_size=cfg.batch_size, shuffle=False)\n",
        "\n",
        "    # Model\n",
        "    model = build_exotic_model(\n",
        "        input_dim=input_dim,\n",
        "        num_classes=num_classes,\n",
        "        neg_units=cfg.neg_units,\n",
        "        hyper_units=cfg.hyper_units,\n",
        "        negative_activation=cfg.negative_activation,\n",
        "        hyper_activation=cfg.hyper_activation,\n",
        "        dropout=cfg.dropout,\n",
        "        batchnorm=cfg.batchnorm,\n",
        "        l2_reg=cfg.l2_reg,\n",
        "    )\n",
        "\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=cfg.lr)\n",
        "    model.compile(optimizer=optimizer, loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "\n",
        "    # Run directory and callbacks\n",
        "    run_dir = get_run_dir(cfg)\n",
        "    ckpt_path = os.path.join(run_dir, \"best_model.keras\")  # Keras 3 native format\n",
        "    callbacks = [\n",
        "        tf.keras.callbacks.EarlyStopping(patience=cfg.patience, restore_best_weights=True, monitor=\"val_loss\"),\n",
        "        tf.keras.callbacks.ReduceLROnPlateau(patience=cfg.reduce_lr_patience, factor=0.5, min_lr=1e-6),\n",
        "        tf.keras.callbacks.ModelCheckpoint(ckpt_path, monitor=\"val_accuracy\", save_best_only=True),\n",
        "        tf.keras.callbacks.CSVLogger(os.path.join(run_dir, \"training_log.csv\")),\n",
        "    ]\n",
        "\n",
        "    # Train\n",
        "    history = model.fit(ds_train, validation_data=ds_val, epochs=cfg.epochs, callbacks=callbacks, verbose=1)\n",
        "\n",
        "    # Save artifacts\n",
        "    pd.DataFrame(history.history).to_csv(os.path.join(run_dir, \"history.csv\"), index=False)\n",
        "    with open(os.path.join(run_dir, \"config.json\"), \"w\") as f:\n",
        "        json.dump(asdict(cfg), f, indent=2)\n",
        "\n",
        "    # Save final model (native Keras format)\n",
        "    final_model_path = os.path.join(run_dir, \"final_model.keras\")\n",
        "    model.save(final_model_path)\n",
        "\n",
        "    # Export SavedModel for serving/TFLite\n",
        "    savedmodel_dir = os.path.join(run_dir, \"savedmodel\")\n",
        "    model.export(savedmodel_dir)\n",
        "\n",
        "    # Optional: TFLite\n",
        "    if cfg.save_tflite:\n",
        "        try:\n",
        "            converter = tf.lite.TFLiteConverter.from_saved_model(savedmodel_dir)\n",
        "            tflite_model = converter.convert()\n",
        "            with open(os.path.join(run_dir, \"model.tflite\"), \"wb\") as f:\n",
        "                f.write(tflite_model)\n",
        "        except Exception as e:\n",
        "            print(f\"[WARN] TFLite export failed: {e}\")\n",
        "\n",
        "    # Optional: ONNX (requires tf2onnx)\n",
        "    if cfg.save_onnx:\n",
        "        try:\n",
        "            import tf2onnx\n",
        "            onnx_path = os.path.join(run_dir, \"model.onnx\")\n",
        "            spec = (tf.TensorSpec((None, input_dim), tf.float32, name=\"inputs\"),)\n",
        "            model_proto, _ = tf2onnx.convert.from_keras(model, input_signature=spec, output_path=onnx_path)\n",
        "        except Exception as e:\n",
        "            print(f\"[WARN] ONNX export failed (install tf2onnx?): {e}\")\n",
        "\n",
        "    # Final evaluate\n",
        "    val_metrics = model.evaluate(ds_val, verbose=0)\n",
        "    metrics = dict(zip(model.metrics_names, val_metrics))\n",
        "    with open(os.path.join(run_dir, \"val_metrics.json\"), \"w\") as f:\n",
        "        json.dump(metrics, f, indent=2)\n",
        "\n",
        "    print(f\"Run directory: {run_dir}\")\n",
        "    print(\"Validation metrics:\", metrics)\n",
        "    return model, history, run_dir\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# CLI (notebook-safe)\n",
        "# -----------------------------\n",
        "def build_arg_parser():\n",
        "    p = argparse.ArgumentParser(description=\"Train ExoticAI with regularization and robust exports.\")\n",
        "    p.add_argument(\"--dataset\", choices=[\"synthetic\", \"mnist\", \"fashion\"], default=\"synthetic\")\n",
        "    p.add_argument(\"--input_dim\", type=int, default=30)\n",
        "    p.add_argument(\"--num_classes\", type=int, default=10)\n",
        "    p.add_argument(\"--epochs\", type=int, default=15)\n",
        "    p.add_argument(\"--batch_size\", type=int, default=128)\n",
        "    p.add_argument(\"--lr\", type=float, default=1e-3)\n",
        "    p.add_argument(\"--dropout\", type=float, default=0.3)\n",
        "    p.add_argument(\"--batchnorm\", action=\"store_true\")\n",
        "    p.add_argument(\"--l2_reg\", type=float, default=1e-4)\n",
        "    p.add_argument(\"--neg_units\", type=int, default=256)\n",
        "    p.add_argument(\"--hyper_units\", type=int, default=512)\n",
        "    p.add_argument(\"--negative_activation\", type=str, default=\"tanh\")\n",
        "    p.add_argument(\"--hyper_activation\", type=str, default=\"relu\")\n",
        "    p.add_argument(\"--patience\", type=int, default=5)\n",
        "    p.add_argument(\"--reduce_lr_patience\", type=int, default=3)\n",
        "    p.add_argument(\"--seed\", type=int, default=42)\n",
        "    p.add_argument(\"--results_dir\", type=str, default=\"results\")\n",
        "    p.add_argument(\"--tag\", type=str, default=None)\n",
        "    p.add_argument(\"--save_tflite\", action=\"store_true\")\n",
        "    p.add_argument(\"--save_onnx\", action=\"store_true\")\n",
        "    return p\n",
        "\n",
        "\n",
        "def main():\n",
        "    parser = build_arg_parser()\n",
        "    # Notebook-safe: ignore stray Jupyter args like \"-f kernel.json\"\n",
        "    args, _unknown = parser.parse_known_args()\n",
        "    cfg = Config(**vars(args))\n",
        "    train(cfg)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ]
}