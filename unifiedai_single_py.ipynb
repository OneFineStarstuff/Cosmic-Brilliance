{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyMBXdskrQOabiKvQItKdrb3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OneFineStarstuff/Cosmic-Brilliance/blob/main/unifiedai_single_py.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g1XcWQbFDglc"
      },
      "outputs": [],
      "source": [
        "#!/usr/bin/env python3\n",
        "# unifiedai_single.py\n",
        "# One-file, ready-to-run: Entangled Episodic Memory + LineSeek env + Q-learning policy\n",
        "# + Reflection + Abstraction/Theory/Simulator/Meta-Validator + Curriculum tweaks.\n",
        "\n",
        "from __future__ import annotations\n",
        "import os, math, random, argparse\n",
        "from dataclasses import dataclass, asdict\n",
        "from typing import Optional, Tuple, Dict, Any, List\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# Utilities & Config\n",
        "# ============================================================\n",
        "\n",
        "def set_seed(seed: int):\n",
        "    random.seed(seed); np.random.seed(seed); torch.manual_seed(seed); torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "def device_or_cpu(name: str) -> torch.device:\n",
        "    if name == \"cuda\" and not torch.cuda.is_available():\n",
        "        return torch.device(\"cpu\")\n",
        "    return torch.device(name)\n",
        "\n",
        "@dataclass\n",
        "class Config:\n",
        "    # System\n",
        "    seed: int = 7\n",
        "    device: str = \"cpu\"\n",
        "    episodes: int = 10\n",
        "    steps_per_episode: int = 50\n",
        "    gamma: float = 0.95\n",
        "    verbose: bool = True\n",
        "\n",
        "    # Policy/Exploration\n",
        "    epsilon_start: float = 0.25\n",
        "    epsilon_min: float = 0.02\n",
        "    epsilon_decay: float = 0.98\n",
        "    lr: float = 1e-3\n",
        "    hidden_dim: int = 64\n",
        "\n",
        "    # Memory (EEM)\n",
        "    mem_slots: int = 256\n",
        "    key_dim: int = 32\n",
        "    value_dim: int = 16\n",
        "    mem_temperature: float = 0.2\n",
        "    mem_ema: float = 0.2\n",
        "    mem_householder_layers: int = 1\n",
        "\n",
        "    # Environment\n",
        "    line_length: int = 31\n",
        "    obs_noise_std: float = 0.1\n",
        "    success_reward: float = 10.0\n",
        "    step_penalty: float = -0.05\n",
        "\n",
        "    # Reflection/Curriculum\n",
        "    fail_recovery_eps_boost: float = 0.15\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# Entangled Episodic Memory (complex Hilbert space)\n",
        "# ============================================================\n",
        "\n",
        "def _as_complex(r: torch.Tensor, i: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
        "    if i is None: i = torch.zeros_like(r)\n",
        "    return torch.complex(r, i)\n",
        "\n",
        "def _norm_complex(z: torch.Tensor, eps: float = 1e-6, dim: int = -1) -> torch.Tensor:\n",
        "    mag = torch.sqrt((z.real**2 + z.imag**2).sum(dim=dim, keepdim=True) + eps)\n",
        "    return z / mag\n",
        "\n",
        "def _phase_unitary(z: torch.Tensor, theta: torch.Tensor) -> torch.Tensor:\n",
        "    # Element-wise complex phase: e^{iθ}\n",
        "    phase = torch.complex(torch.cos(theta), torch.sin(theta))\n",
        "    return z * phase\n",
        "\n",
        "def _stack_householder(U: torch.Tensor, v: torch.Tensor) -> torch.Tensor:\n",
        "    v = v / (v.norm() + 1e-6)\n",
        "    proj = torch.matmul(U, v)\n",
        "    return U - 2.0 * proj.unsqueeze(-1) * v\n",
        "\n",
        "class EntangledEpisodicMemory(nn.Module):\n",
        "    \"\"\"\n",
        "    Complex-key memory with measurement-based retrieval.\n",
        "    p(slot | q) ∝ |<U(q), k>|^2 / τ\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        slots: int = 256,\n",
        "        key_dim: int = 32,\n",
        "        value_dim: int = 16,\n",
        "        temperature: float = 0.2,\n",
        "        ema: float = 0.2,\n",
        "        trainable_memory: bool = False,\n",
        "        householder_layers: int = 1,\n",
        "        dtype: torch.dtype = torch.float32,\n",
        "        device: Optional[torch.device] = None,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.slots, self.key_dim, self.value_dim = slots, key_dim, value_dim\n",
        "        self.temperature, self.ema = temperature, ema\n",
        "        self.householder_layers = householder_layers\n",
        "\n",
        "        init_keys_r = F.normalize(torch.randn(slots, key_dim, dtype=dtype, device=device), dim=-1)\n",
        "        init_keys_i = F.normalize(torch.randn(slots, key_dim, dtype=dtype, device=device), dim=-1)\n",
        "        init_values = torch.zeros(slots, value_dim, dtype=dtype, device=device)\n",
        "\n",
        "        if trainable_memory:\n",
        "            self.keys_r = nn.Parameter(init_keys_r)\n",
        "            self.keys_i = nn.Parameter(init_keys_i)\n",
        "            self.values = nn.Parameter(init_values)\n",
        "        else:\n",
        "            self.register_buffer(\"keys_r\", init_keys_r)\n",
        "            self.register_buffer(\"keys_i\", init_keys_i)\n",
        "            self.register_buffer(\"values\", init_values)\n",
        "\n",
        "        self.theta = nn.Parameter(torch.zeros(key_dim, dtype=dtype, device=device))\n",
        "\n",
        "        if self.householder_layers > 0:\n",
        "            self.house_v = nn.ParameterList(\n",
        "                [nn.Parameter(F.normalize(torch.randn(2 * key_dim, dtype=dtype, device=device), dim=0))\n",
        "                 for _ in range(self.householder_layers)]\n",
        "            )\n",
        "\n",
        "        self.register_buffer(\"age\", torch.zeros(slots, dtype=torch.long, device=device))\n",
        "        self.register_buffer(\"ptr\", torch.zeros((), dtype=torch.long, device=device))\n",
        "\n",
        "    def _apply_unitary(self, z_c: torch.Tensor) -> torch.Tensor:\n",
        "        z_c = _phase_unitary(z_c, self.theta)\n",
        "        if self.householder_layers > 0:\n",
        "            re, im = z_c.real, z_c.imag\n",
        "            cat = torch.cat([re, im], dim=-1)\n",
        "            for v in self.house_v:\n",
        "                cat = _stack_householder(cat, v)\n",
        "            D = z_c.size(-1)\n",
        "            z_c = torch.complex(cat[..., :D], cat[..., D:])\n",
        "        return z_c\n",
        "\n",
        "    def _similarity(self, q_c: torch.Tensor, k_c: torch.Tensor) -> torch.Tensor:\n",
        "        qn = _norm_complex(q_c); kn = _norm_complex(k_c)\n",
        "        scores = torch.abs(qn.unsqueeze(1) @ torch.conj(kn).unsqueeze(0).transpose(-1, -2)) ** 2\n",
        "        return scores.squeeze(1)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def write(self, k: torch.Tensor, v: torch.Tensor, strategy: str = \"nearest\"):\n",
        "        single = k.dim() == 1\n",
        "        if single: k = k.unsqueeze(0); v = v.unsqueeze(0)\n",
        "        k_c = k if torch.is_complex(k) else _as_complex(k)\n",
        "        k_c = _norm_complex(self._apply_unitary(k_c))\n",
        "        mem_c = _as_complex(self.keys_r, self.keys_i)\n",
        "        if strategy == \"ring\":\n",
        "            for i in range(k_c.size(0)):\n",
        "                idx = int(self.ptr.item() % self.slots)\n",
        "                self.keys_r[idx] = k_c[i].real; self.keys_i[idx] = k_c[i].imag\n",
        "                self.values[idx] = v[i]; self.age[idx] = 0; self.ptr += 1\n",
        "        elif strategy == \"nearest\":\n",
        "            sims = self._similarity(k_c, mem_c)\n",
        "            idxs = sims.argmax(dim=-1)\n",
        "            for i, idx in enumerate(idxs.tolist()):\n",
        "                self.keys_r[idx] = F.normalize((1 - self.ema) * self.keys_r[idx] + self.ema * k_c[i].real, dim=-1)\n",
        "                self.keys_i[idx] = F.normalize((1 - self.ema) * self.keys_i[idx] + self.ema * k_c[i].imag, dim=-1)\n",
        "                self.values[idx] = (1 - self.ema) * self.values[idx] + self.ema * v[i]\n",
        "                self.age[idx] = 0\n",
        "        else:\n",
        "            raise ValueError(\"Unknown write strategy\")\n",
        "        self.age += 1\n",
        "\n",
        "    def read(self, q: torch.Tensor, topk: int = 0, return_weights: bool = False):\n",
        "        single = q.dim() == 1\n",
        "        if single: q = q.unsqueeze(0)\n",
        "        q_c = q if torch.is_complex(q) else _as_complex(q)\n",
        "        q_c = self._apply_unitary(q_c)\n",
        "        k_c = _as_complex(self.keys_r, self.keys_i)\n",
        "        scores = self._similarity(q_c, k_c) / max(self.temperature, 1e-6)\n",
        "        if topk and topk < self.slots:\n",
        "            vals, idxs = scores.topk(topk, dim=-1)\n",
        "            w = torch.softmax(vals, dim=-1)\n",
        "            gathered = self.values[idxs]\n",
        "            out = (w.unsqueeze(-1) * gathered).sum(dim=1); weights = w\n",
        "        else:\n",
        "            w = torch.softmax(scores, dim=-1)\n",
        "            out = w @ self.values; weights = w\n",
        "        if single:\n",
        "            out = out.squeeze(0)\n",
        "            if return_weights: weights = weights.squeeze(0)\n",
        "        return (out, weights) if return_weights else (out, None)\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# Environment: 1D LineSeek\n",
        "# ============================================================\n",
        "\n",
        "@dataclass\n",
        "class LineSeekConfig:\n",
        "    line_length: int\n",
        "    obs_noise_std: float\n",
        "    success_reward: float\n",
        "    step_penalty: float\n",
        "    steps_per_episode: int\n",
        "\n",
        "class LineSeekEnv:\n",
        "    def __init__(self, cfg: LineSeekConfig, device: torch.device):\n",
        "        assert cfg.line_length % 2 == 1, \"line_length must be odd\"\n",
        "        self.cfg = cfg; self.device = device\n",
        "        self.half = cfg.line_length // 2\n",
        "        self.pos = 0; self.target = 0; self.t = 0\n",
        "\n",
        "    def reset(self, seed: int | None = None):\n",
        "        if seed is not None:\n",
        "            random.seed(seed); torch.manual_seed(seed)\n",
        "        self.pos = 0\n",
        "        self.target = random.randint(-self.half, self.half)\n",
        "        self.t = 0\n",
        "        return self._obs()\n",
        "\n",
        "    def _obs(self):\n",
        "        dist = (self.target - self.pos) / max(1, self.half)\n",
        "        noise = torch.randn(1, device=self.device).item() * self.cfg.obs_noise_std\n",
        "        return torch.tensor([self.pos / self.half, dist + noise], device=self.device, dtype=torch.float32)\n",
        "\n",
        "    def step(self, action: int):\n",
        "        self.t += 1\n",
        "        self.pos = max(-self.half, min(self.half, self.pos + int(action)))\n",
        "        dist = self.target - self.pos\n",
        "        done = (self.pos == self.target) or (self.t >= self.cfg.steps_per_episode)\n",
        "        reward = self.cfg.step_penalty - abs(dist) / self.half\n",
        "        success = False\n",
        "        if self.pos == self.target:\n",
        "            reward += self.cfg.success_reward; success = True\n",
        "        obs = self._obs()\n",
        "        info = {\"success\": success, \"dist\": dist, \"t\": self.t}\n",
        "        return obs, reward, done, info\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# Policy: Q-learning\n",
        "# ============================================================\n",
        "\n",
        "class QNetwork(nn.Module):\n",
        "    def __init__(self, obs_dim: int, hidden: int, actions: int):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(obs_dim, hidden), nn.ReLU(),\n",
        "            nn.Linear(hidden, hidden), nn.ReLU(),\n",
        "            nn.Linear(hidden, actions)\n",
        "        )\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        return self.net(x)\n",
        "\n",
        "@dataclass\n",
        "class PolicyConfig:\n",
        "    obs_dim: int\n",
        "    hidden_dim: int\n",
        "    actions: int\n",
        "    lr: float\n",
        "    gamma: float\n",
        "    epsilon: float\n",
        "    epsilon_min: float\n",
        "    epsilon_decay: float\n",
        "\n",
        "class AgentPolicy:\n",
        "    def __init__(self, cfg: PolicyConfig, device: torch.device):\n",
        "        self.cfg = cfg; self.device = device\n",
        "        self.q = QNetwork(cfg.obs_dim, cfg.hidden_dim, cfg.actions).to(device)\n",
        "        self.opt = torch.optim.Adam(self.q.parameters(), lr=cfg.lr)\n",
        "        self.epsilon = cfg.epsilon\n",
        "\n",
        "    def select_action(self, obs: torch.Tensor) -> int:\n",
        "        if torch.rand(()) < self.epsilon:\n",
        "            return int(torch.randint(0, 3, (1,)).item() - 1)\n",
        "        with torch.no_grad():\n",
        "            q = self.q(obs.unsqueeze(0))\n",
        "            a_idx = int(q.argmax(dim=-1).item())\n",
        "            return a_idx - 1\n",
        "\n",
        "    def update(self, s: torch.Tensor, a: int, r: float, s2: torch.Tensor, done: bool):\n",
        "        a_idx = a + 1\n",
        "        q = self.q(s.unsqueeze(0))\n",
        "        q_sa = q[0, a_idx]\n",
        "        with torch.no_grad():\n",
        "            target = torch.tensor(r, device=self.device)\n",
        "            if not done:\n",
        "                q2 = self.q(s2.unsqueeze(0)).max(dim=-1).values[0]\n",
        "                target = target + self.cfg.gamma * q2\n",
        "        loss = F.smooth_l1_loss(q_sa, target)\n",
        "        self.opt.zero_grad(); loss.backward(); self.opt.step()\n",
        "        return float(loss.item())\n",
        "\n",
        "    def decay_epsilon(self):\n",
        "        self.epsilon = max(self.cfg.epsilon_min, self.epsilon * self.cfg.epsilon_decay)\n",
        "\n",
        "    def boost_epsilon(self, amount: float):\n",
        "        self.epsilon = min(0.9, self.epsilon + amount)\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# Meta loop utilities (abstraction/theory/simulator/validator)\n",
        "# ============================================================\n",
        "\n",
        "def abstract_state(obs: torch.Tensor) -> torch.Tensor:\n",
        "    return obs\n",
        "\n",
        "def identify_generalizations(episode: Dict[str, Any]) -> Dict[str, Any]:\n",
        "    dists = episode.get(\"dists\", [])\n",
        "    trend = float(dists[-1] - dists[0]) if len(dists) >= 2 else 0.0\n",
        "    return {\"distance_trend\": trend, \"steps\": len(dists)}\n",
        "\n",
        "def construct_theory(abstractions: Dict[str, Any]) -> Dict[str, Any]:\n",
        "    trend = abstractions.get(\"distance_trend\", 0.0)\n",
        "    return {\"should_explore_more\": trend >= 0.0}\n",
        "\n",
        "def generate_simulators(theory: Dict[str, Any]) -> Dict[str, Any]:\n",
        "    if theory.get(\"should_explore_more\", False):\n",
        "        return {\"noise_candidates\": [0.05, 0.1, 0.2]}\n",
        "    return {\"noise_candidates\": [0.05, 0.1]}\n",
        "\n",
        "def meta_validate(results: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
        "    if not results:\n",
        "        return {\"usefulness\": 0.0, \"parsimony\": 1.0, \"novelty\": 0.0, \"best\": None}\n",
        "    best = max(results, key=lambda r: (r.get(\"success_rate\", 0.0), -r.get(\"avg_steps\", 1e9)))\n",
        "    return {\"usefulness\": 1.0, \"parsimony\": 0.8, \"novelty\": 0.2, \"best\": best}\n",
        "\n",
        "def planning_cycle(mem: EntangledEpisodicMemory, episode_log: Dict[str, Any]) -> Dict[str, Any]:\n",
        "    abstractions = identify_generalizations(episode_log)\n",
        "    theory = construct_theory(abstractions)\n",
        "    sims = generate_simulators(theory)\n",
        "    results = [{\"noise\": n, \"success_rate\": 1.0 - n, \"avg_steps\": 10 + int(10 * n)} for n in sims[\"noise_candidates\"]]\n",
        "    meta = meta_validate(results)\n",
        "    return {\"abstractions\": abstractions, \"theory\": theory, \"meta\": meta}\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# Reflection\n",
        "# ============================================================\n",
        "\n",
        "REFLECT_PROMPT = \"\"\"You are a meta-cognitive agent.\n",
        "\n",
        "Below is a transcript of your actions, states, and outcomes during the task.\n",
        "\n",
        "Please:\n",
        "1. Identify at least 2 failure points or suboptimal decisions.\n",
        "2. Hypothesize why these failures occurred.\n",
        "3. Propose improvements to policy, memory use, or planning strategy.\n",
        "\n",
        "Transcript:\n",
        "{log}\n",
        "\"\"\"\n",
        "\n",
        "def reflect_on_episode(agent_log: str, prompt_template: str) -> Dict[str, Any]:\n",
        "    if \"{log}\" not in prompt_template:\n",
        "        raise ValueError(\"prompt_template must contain '{log}'\")\n",
        "    prompt = prompt_template.format(log=agent_log)\n",
        "\n",
        "    def _default_llm(p: str) -> str:\n",
        "        lines = [ln.strip() for ln in p.splitlines() if ln.strip()]\n",
        "        failures = [ln for ln in lines if any(w in ln.lower() for w in [\"fail\", \"error\", \"dead-end\", \"stale\"])]\n",
        "        if not failures:\n",
        "            failures = [\"No explicit failure lines detected; reward curve suggests suboptimal exploration.\"]\n",
        "        hyp = [\n",
        "            \"Insufficient situational memory caused plan drift.\",\n",
        "            \"Under-exploration due to premature exploitation.\"\n",
        "        ]\n",
        "        imps = [\n",
        "            \"Increase retrieval top-k and apply recency weighting in writes.\",\n",
        "            \"Boost epsilon temporarily after failures; decay when success stabilizes.\"\n",
        "        ]\n",
        "        return (\n",
        "            \"Failures:\\n- \" + \"\\n- \".join(failures[:3]) + \"\\n\\n\"\n",
        "            \"Hypotheses:\\n- \" + \"\\n- \".join(hyp) + \"\\n\\n\"\n",
        "            \"Improvements:\\n- \" + \"\\n- \".join(imps)\n",
        "        )\n",
        "\n",
        "    def _default_parse(s: str) -> Dict[str, Any]:\n",
        "        out = {\"failures\": [], \"hypotheses\": [], \"improvements\": [], \"raw\": s}\n",
        "        sec, buf = None, []\n",
        "        def _flush():\n",
        "            nonlocal sec, buf\n",
        "            if sec and buf:\n",
        "                out[sec] = [b.lstrip(\"- \").strip() for b in buf if b.strip()]\n",
        "                buf = []\n",
        "        for ln in s.splitlines():\n",
        "            l = ln.strip()\n",
        "            if not l: continue\n",
        "            low = l.lower()\n",
        "            if low.startswith(\"failures:\"):\n",
        "                _flush(); sec = \"failures\"\n",
        "            elif low.startswith(\"hypotheses:\"):\n",
        "                _flush(); sec = \"hypotheses\"\n",
        "            elif low.startswith(\"improvements:\"):\n",
        "                _flush(); sec = \"improvements\"\n",
        "            else:\n",
        "                buf.append(l)\n",
        "        _flush()\n",
        "        return out\n",
        "\n",
        "    raw = _default_llm(prompt)\n",
        "    return _default_parse(raw)\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# Encoder helpers\n",
        "# ============================================================\n",
        "\n",
        "def embed_obs(obs: torch.Tensor, key_dim: int) -> torch.Tensor:\n",
        "    x = obs\n",
        "    feats = [x]\n",
        "    for k in range(1, 1 + max(1, key_dim // 4)):\n",
        "        feats.append(torch.sin(k * x))\n",
        "        feats.append(torch.cos(k * x))\n",
        "    z = torch.cat(feats, dim=-1)\n",
        "    if z.numel() < key_dim:\n",
        "        z = torch.cat([z, torch.zeros(key_dim - z.numel(), device=obs.device)], dim=-1)\n",
        "    return z[:key_dim]\n",
        "\n",
        "def value_from_step(reward: float, action: int, dist: float, value_dim: int, device: torch.device) -> torch.Tensor:\n",
        "    v = torch.zeros(value_dim, device=device)\n",
        "    v[0] = float(reward); v[1] = float(action); v[2] = 1.0 if dist >= 0 else -1.0\n",
        "    if value_dim > 3: v[3] = 1.0\n",
        "    return v\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# Main loop\n",
        "# ============================================================\n",
        "\n",
        "def run(cfg: Config):\n",
        "    device = device_or_cpu(cfg.device)\n",
        "    set_seed(cfg.seed)\n",
        "    print(f\"[INFO] Device={device}, Episodes={cfg.episodes}, Steps/Ep={cfg.steps_per_episode}\")\n",
        "\n",
        "    # Env\n",
        "    env_cfg = LineSeekConfig(\n",
        "        line_length=cfg.line_length, obs_noise_std=cfg.obs_noise_std,\n",
        "        success_reward=cfg.success_reward, step_penalty=cfg.step_penalty,\n",
        "        steps_per_episode=cfg.steps_per_episode,\n",
        "    )\n",
        "    env = LineSeekEnv(env_cfg, device=device)\n",
        "\n",
        "    # Policy\n",
        "    pol_cfg = PolicyConfig(\n",
        "        obs_dim=2, hidden_dim=cfg.hidden_dim, actions=3, lr=cfg.lr,\n",
        "        gamma=cfg.gamma, epsilon=cfg.epsilon_start,\n",
        "        epsilon_min=cfg.epsilon_min, epsilon_decay=cfg.epsilon_decay,\n",
        "    )\n",
        "    policy = AgentPolicy(pol_cfg, device=device)\n",
        "    print(f\"[INFO] Epsilon start={policy.epsilon:.3f}\")\n",
        "\n",
        "    # Memory\n",
        "    mem = EntangledEpisodicMemory(\n",
        "        slots=cfg.mem_slots, key_dim=cfg.key_dim, value_dim=cfg.value_dim,\n",
        "        temperature=cfg.mem_temperature, ema=cfg.mem_ema,\n",
        "        householder_layers=cfg.mem_householder_layers, device=device\n",
        "    )\n",
        "\n",
        "    for ep in range(1, cfg.episodes + 1):\n",
        "        obs = env.reset(seed=cfg.seed + ep)\n",
        "        total_r = 0.0\n",
        "        agent_log_lines: List[str] = []\n",
        "        dists: List[int] = []\n",
        "        success = False\n",
        "\n",
        "        for t in range(cfg.steps_per_episode):\n",
        "            key = embed_obs(obs, cfg.key_dim).to(device)\n",
        "            _, weights = mem.read(key, topk=8, return_weights=True)\n",
        "\n",
        "            suggested = int(torch.sign(obs[1]).item())\n",
        "            suggested = max(-1, min(1, suggested))\n",
        "\n",
        "            act_from_policy = policy.select_action(obs)\n",
        "            action = act_from_policy if torch.rand(()) < 0.7 else suggested\n",
        "\n",
        "            next_obs, reward, done, info = env.step(action)\n",
        "            total_r += reward; dists.append(int(info[\"dist\"]))\n",
        "\n",
        "            loss = policy.update(obs, action, reward, next_obs, done)\n",
        "\n",
        "            v = value_from_step(reward, action, float(info[\"dist\"]), cfg.value_dim, device=device)\n",
        "            mem.write(key, v, strategy=\"nearest\")\n",
        "\n",
        "            agent_log_lines.append(\n",
        "                f\"t={t:02d} pos={float(obs[0]):+.2f} dist={int(info['dist']):+d} act={action:+d} r={reward:+.2f} loss={loss:.3f}\"\n",
        "            )\n",
        "\n",
        "            obs = next_obs\n",
        "            if done: success = info.get(\"success\", False); break\n",
        "\n",
        "        status = \"SUCCESS\" if success else \"FAIL\"\n",
        "        print(f\"[EP {ep:02d}] {status} total_reward={total_r:+.2f} steps={len(agent_log_lines):d} epsilon={policy.epsilon:.3f}\")\n",
        "\n",
        "        # Reflection & curriculum\n",
        "        if not success:\n",
        "            agent_log = \"\\n\".join(agent_log_lines) + \"\\nOutcome: Goal not achieved; time limit exceeded.\"\n",
        "            reflection = reflect_on_episode(agent_log, REFLECT_PROMPT)\n",
        "            print(\"[REFLECTION] Failures:\", reflection.get(\"failures\", [])[:2])\n",
        "            print(\"[REFLECTION] Improvements:\", reflection.get(\"improvements\", [])[:2])\n",
        "            policy.boost_epsilon(cfg.fail_recovery_eps_boost)\n",
        "        else:\n",
        "            policy.decay_epsilon()\n",
        "\n",
        "        # Planning cycle (meta)\n",
        "        episode_log = {\"dists\": dists, \"success\": success, \"total_reward\": total_r}\n",
        "        meta_out = planning_cycle(mem, episode_log)\n",
        "        best_sim = meta_out[\"meta\"].get(\"best\")\n",
        "        if best_sim:\n",
        "            mem.temperature = max(0.05, min(0.5, 0.1 + 0.5 * (best_sim[\"noise\"])))\n",
        "        if cfg.verbose:\n",
        "            print(f\"[PLAN] abstractions={meta_out['abstractions']} | theory={meta_out['theory']} | mem.tau={mem.temperature:.3f}\")\n",
        "\n",
        "    print(\"\\n=== SUMMARY ===\")\n",
        "    print(f\"Config: {asdict(cfg)}\")\n",
        "    print(f\"Final epsilon: {policy.epsilon:.3f}\")\n",
        "    print(\"[DONE]\")\n",
        "\n",
        "def build_parser():\n",
        "    import argparse\n",
        "    p = argparse.ArgumentParser(\n",
        "        add_help=True,\n",
        "        description=\"UnifiedAI single-file demo (Notebook/CLI friendly)\"\n",
        "    )\n",
        "    p.add_argument(\"--episodes\", type=int, default=10)\n",
        "    p.add_argument(\"--steps_per_episode\", type=int, default=50)\n",
        "    p.add_argument(\"--device\", choices={\"cpu\", \"cuda\"}, default=\"cpu\")\n",
        "    p.add_argument(\"--epsilon_start\", type=float, default=0.25)\n",
        "    p.add_argument(\"--epsilon_min\", type=float, default=0.02)\n",
        "    p.add_argument(\"--epsilon_decay\", type=float, default=0.98)\n",
        "    p.add_argument(\"--lr\", type=float, default=1e-3)\n",
        "    p.add_argument(\"--hidden_dim\", type=int, default=64)\n",
        "    p.add_argument(\"--mem_slots\", type=int, default=256)\n",
        "    p.add_argument(\"--key_dim\", type=int, default=32)\n",
        "    p.add_argument(\"--value_dim\", type=int, default=16)\n",
        "    p.add_argument(\"--mem_temperature\", type=float, default=0.2)\n",
        "    p.add_argument(\"--mem_ema\", type=float, default=0.2)\n",
        "    p.add_argument(\"--mem_householder_layers\", type=int, default=1)\n",
        "    p.add_argument(\"--line_length\", type=int, default=31)\n",
        "    p.add_argument(\"--obs_noise_std\", type=float, default=0.1)\n",
        "    p.add_argument(\"--success_reward\", type=float, default=10.0)\n",
        "    p.add_argument(\"--step_penalty\", type=float, default=-0.05)\n",
        "    p.add_argument(\"--fail_recovery_eps_boost\", type=float, default=0.15)\n",
        "    p.add_argument(\"--seed\", type=int, default=7)\n",
        "    p.add_argument(\n",
        "        \"--verbose\",\n",
        "        type=lambda x: str(x).lower() in {\"1\", \"true\", \"yes\", \"y\"},\n",
        "        default=True\n",
        "    )\n",
        "    return p\n",
        "\n",
        "\n",
        "def main():\n",
        "    import os\n",
        "    parser = build_parser()\n",
        "    args, _ = parser.parse_known_args()\n",
        "    cfg = Config(\n",
        "        seed=args.seed,\n",
        "        device=args.device,\n",
        "        episodes=args.episodes,\n",
        "        steps_per_episode=args.steps_per_episode,\n",
        "        epsilon_start=args.epsilon_start,\n",
        "        epsilon_min=args.epsilon_min,\n",
        "        epsilon_decay=args.epsilon_decay,\n",
        "        lr=args.lr,\n",
        "        hidden_dim=args.hidden_dim,\n",
        "        mem_slots=args.mem_slots,\n",
        "        key_dim=args.key_dim,\n",
        "        value_dim=args.value_dim,\n",
        "        mem_temperature=args.mem_temperature,\n",
        "        mem_ema=args.mem_ema,\n",
        "        mem_householder_layers=args.mem_householder_layers,\n",
        "        line_length=args.line_length,\n",
        "        obs_noise_std=args.obs_noise_std,\n",
        "        success_reward=args.success_reward,\n",
        "        step_penalty=args.step_penalty,\n",
        "        fail_recovery_eps_boost=args.fail_recovery_eps_boost,\n",
        "        verbose=args.verbose,\n",
        "    )\n",
        "    os.environ.setdefault(\"PYTHONHASHSEED\", \"0\")\n",
        "    run(cfg)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ]
}