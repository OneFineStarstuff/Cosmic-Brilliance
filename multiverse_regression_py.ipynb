{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyPOK348ChBn+3t0bz1cV40A",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OneFineStarstuff/Cosmic-Brilliance/blob/main/multiverse_regression_py.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9hH2XAFX_xtB"
      },
      "outputs": [],
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "multiverse_regression.py\n",
        "\n",
        "Predict continuous topology scores from string-theory compactification\n",
        "parameters using a regression neural network. Then convert continuous\n",
        "predictions into discrete bins for classification metrics.\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import (\n",
        "    mean_squared_error,\n",
        "    accuracy_score,\n",
        "    confusion_matrix,\n",
        "    classification_report\n",
        ")\n",
        "\n",
        "# 1. Synthetic Dataset Generation\n",
        "def generate_data(n_samples=15000, input_dim=5, seed=42):\n",
        "    np.random.seed(seed)\n",
        "    params = np.random.uniform(0, 2 * np.pi, size=(n_samples, input_dim)).astype(np.float32)\n",
        "    scores = np.sum(np.sin(params), axis=1).astype(np.float32)\n",
        "    return params, scores\n",
        "\n",
        "# 2. Regression Model Definition\n",
        "class MultiverseRegressor(nn.Module):\n",
        "    def __init__(self, input_dim=5, hidden_dims=[64, 32], drop_p=0.3):\n",
        "        super().__init__()\n",
        "        layers = []\n",
        "        dims = [input_dim] + hidden_dims + [1]\n",
        "        for i in range(len(dims) - 1):\n",
        "            layers.append(nn.Linear(dims[i], dims[i+1]))\n",
        "            if i < len(dims) - 2:\n",
        "                layers.append(nn.ReLU())\n",
        "                layers.append(nn.Dropout(drop_p))\n",
        "        self.net = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x).squeeze(-1)\n",
        "\n",
        "# 3. Training Loop with Early Stopping\n",
        "def train_regressor(\n",
        "    model, train_loader, val_loader,\n",
        "    epochs=100, lr=1e-3, patience=10, device=None\n",
        "):\n",
        "    device = device or torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=1e-5)\n",
        "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
        "        optimizer, mode=\"min\", factor=0.5, patience=5\n",
        "    )\n",
        "    criterion = nn.MSELoss()\n",
        "\n",
        "    best_val_loss = float(\"inf\")\n",
        "    epochs_no_improve = 0\n",
        "\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        # Training\n",
        "        model.train()\n",
        "        train_losses = []\n",
        "        for xb, yb in train_loader:\n",
        "            xb, yb = xb.to(device), yb.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            preds = model(xb)\n",
        "            loss = criterion(preds, yb)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            train_losses.append(loss.item())\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        val_losses = []\n",
        "        with torch.no_grad():\n",
        "            for xb, yb in val_loader:\n",
        "                xb, yb = xb.to(device), yb.to(device)\n",
        "                preds = model(xb)\n",
        "                val_losses.append(criterion(preds, yb).item())\n",
        "\n",
        "        avg_train = np.mean(train_losses)\n",
        "        avg_val   = np.mean(val_losses)\n",
        "        scheduler.step(avg_val)\n",
        "\n",
        "        if avg_val < best_val_loss:\n",
        "            best_val_loss = avg_val\n",
        "            epochs_no_improve = 0\n",
        "            torch.save(model.state_dict(), \"best_regressor.pth\")\n",
        "        else:\n",
        "            epochs_no_improve += 1\n",
        "\n",
        "        if epoch == 1 or epoch % 10 == 0:\n",
        "            print(\n",
        "                f\"Epoch {epoch:03d} ─ \"\n",
        "                f\"Train MSE: {avg_train:.6f} ─ \"\n",
        "                f\"Val MSE: {avg_val:.6f}\"\n",
        "            )\n",
        "\n",
        "        if epochs_no_improve >= patience:\n",
        "            print(f\"Early stopping at epoch {epoch}. \"\n",
        "                  f\"Best Val MSE: {best_val_loss:.6f}\")\n",
        "            break\n",
        "\n",
        "# 4. Main Execution\n",
        "if __name__ == \"__main__\":\n",
        "    # Hyperparameters\n",
        "    INPUT_DIM   = 5\n",
        "    N_SAMPLES   = 15000\n",
        "    TEST_SIZE   = 0.2\n",
        "    BATCH_SIZE  = 256\n",
        "    EPOCHS      = 100\n",
        "    LR          = 1e-3\n",
        "    PATIENCE    = 10\n",
        "    HIDDEN_DIMS = [64, 32]\n",
        "    DROP_P      = 0.3\n",
        "    N_BINS      = 10  # for discrete evaluation\n",
        "\n",
        "    # Generate continuous data\n",
        "    X, y = generate_data(N_SAMPLES, INPUT_DIM)\n",
        "\n",
        "    # Train/validation split\n",
        "    X_train, X_val, y_train, y_val = train_test_split(\n",
        "        X, y, test_size=TEST_SIZE, random_state=42\n",
        "    )\n",
        "\n",
        "    # DataLoaders\n",
        "    train_ds = TensorDataset(torch.from_numpy(X_train), torch.from_numpy(y_train))\n",
        "    val_ds   = TensorDataset(torch.from_numpy(X_val),   torch.from_numpy(y_val))\n",
        "    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
        "    val_loader   = DataLoader(val_ds,   batch_size=BATCH_SIZE)\n",
        "\n",
        "    # Initialize and train model\n",
        "    model = MultiverseRegressor(\n",
        "        input_dim=INPUT_DIM,\n",
        "        hidden_dims=HIDDEN_DIMS,\n",
        "        drop_p=DROP_P\n",
        "    )\n",
        "    train_regressor(\n",
        "        model, train_loader, val_loader,\n",
        "        epochs=EPOCHS, lr=LR, patience=PATIENCE\n",
        "    )\n",
        "\n",
        "    # Load best model\n",
        "    model.load_state_dict(torch.load(\"best_regressor.pth\"))\n",
        "    model.eval()\n",
        "\n",
        "    # Final continuous evaluation\n",
        "    with torch.no_grad():\n",
        "        y_pred_train = model(torch.from_numpy(X_train)).numpy()\n",
        "        y_pred_val   = model(torch.from_numpy(X_val)).numpy()\n",
        "\n",
        "    mse_train = mean_squared_error(y_train, y_pred_train)\n",
        "    mse_val   = mean_squared_error(y_val,   y_pred_val)\n",
        "    print(f\"\\nFinal MSE ─ Train: {mse_train:.6f} ─ Val: {mse_val:.6f}\")\n",
        "\n",
        "    # Map to discrete bins for classification metrics\n",
        "    edges       = np.linspace(y.min(), y.max(), N_BINS + 1)\n",
        "    true_bins   = np.digitize(y_val, edges) - 1\n",
        "    pred_bins   = np.digitize(y_pred_val, edges) - 1\n",
        "\n",
        "    acc = accuracy_score(true_bins, pred_bins)\n",
        "    print(f\"\\nBinned Classification Accuracy: {acc:.3f}\\n\")\n",
        "    print(\"Classification Report:\\n\", classification_report(true_bins, pred_bins, digits=4))\n",
        "    print(\"Confusion Matrix:\\n\", confusion_matrix(true_bins, pred_bins))"
      ]
    }
  ]
}