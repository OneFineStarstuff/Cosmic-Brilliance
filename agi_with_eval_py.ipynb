{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyPbPtqlxHOIalP6EbUPLU8g",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OneFineStarstuff/Cosmic-Brilliance/blob/main/agi_with_eval_py.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install torch torchvision tqdm matplotlib tensorboard"
      ],
      "metadata": {
        "id": "cfz_ssxj_0wj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "agi_with_eval.py\n",
        "\n",
        "– Synthetic train/val split\n",
        "– SelfRecursiveAGI with inner/outer loops + Dropout\n",
        "– Training loop with checkpoint & eval every `eval_interval` epochs\n",
        "– TensorBoard logging (scalars & figures)\n",
        "– Matplotlib plots saved under `out/`\n",
        "– Uses parse_known_args() to ignore Jupyter’s extra CLI flags\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import argparse\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# 1. Data\n",
        "# -----------------------------------------------------------------------------\n",
        "class SyntheticRegressionDataset(Dataset):\n",
        "    def __init__(self, num_samples=2500, input_size=10, val_split=500):\n",
        "        X = torch.randn(num_samples, input_size)\n",
        "        true_w = torch.randn(input_size, 1)\n",
        "        y = X @ true_w + 0.1 * torch.randn(num_samples, 1)\n",
        "        data = list(zip(X, y))\n",
        "        train_len = num_samples - val_split\n",
        "        self.train_set, self.val_set = random_split(data, [train_len, val_split])\n",
        "\n",
        "    def get_loaders(self, batch_size=32):\n",
        "        train_loader = DataLoader(self.train_set, batch_size=batch_size, shuffle=True)\n",
        "        val_loader   = DataLoader(self.val_set,   batch_size=batch_size, shuffle=False)\n",
        "        return train_loader, val_loader\n",
        "\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# 2. Model\n",
        "# -----------------------------------------------------------------------------\n",
        "class SelfRecursiveAGI(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size,\n",
        "                 lr_main=1e-3, lr_self=1e-3, dropout_p=0.1):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(input_size, hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout_p),\n",
        "            nn.Linear(hidden_size, output_size)\n",
        "        )\n",
        "        self.main_opt = optim.Adam(self.parameters(), lr=lr_main)\n",
        "        self.self_opt = optim.Adam(self.parameters(), lr=lr_self)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "    def self_improve(self, loss_fn, x, y, steps=5, clip=1.0):\n",
        "        last = 0.0\n",
        "        for _ in range(steps):\n",
        "            self.self_opt.zero_grad()\n",
        "            loss = loss_fn(self(x), y)\n",
        "            loss.backward()\n",
        "            nn.utils.clip_grad_norm_(self.parameters(), clip)\n",
        "            self.self_opt.step()\n",
        "            last = loss.item()\n",
        "        return last\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def mc_predict(self, x, mc_samples=50):\n",
        "        self.train()  # keep dropout active\n",
        "        preds = []\n",
        "        for _ in range(mc_samples):\n",
        "            preds.append(self(x).cpu().numpy())\n",
        "        arr = np.stack(preds, 0)  # [T, batch, 1]\n",
        "        return arr.mean(0).squeeze(), arr.std(0).squeeze()\n",
        "\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# 3. Training + Evaluation\n",
        "# -----------------------------------------------------------------------------\n",
        "def train_and_evaluate(args):\n",
        "    os.makedirs(args.checkpoint_dir, exist_ok=True)\n",
        "    os.makedirs(\"out\", exist_ok=True)\n",
        "    writer = SummaryWriter(args.logdir)\n",
        "\n",
        "    # Data loaders\n",
        "    ds = SyntheticRegressionDataset(\n",
        "        num_samples=args.num_samples,\n",
        "        input_size=args.input_size,\n",
        "        val_split=args.val_split\n",
        "    )\n",
        "    train_loader, val_loader = ds.get_loaders(args.batch_size)\n",
        "\n",
        "    # Model, scheduler, loss\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = SelfRecursiveAGI(\n",
        "        args.input_size, args.hidden_size, args.output_size,\n",
        "        lr_main=args.lr_main, lr_self=args.lr_self,\n",
        "        dropout_p=args.dropout_p\n",
        "    ).to(device)\n",
        "    scheduler = StepLR(model.main_opt, step_size=args.lr_step, gamma=args.lr_gamma)\n",
        "    loss_fn = nn.MSELoss()\n",
        "\n",
        "    global_step = 0\n",
        "    for epoch in range(1, args.epochs + 1):\n",
        "        model.train()\n",
        "        epoch_loss = 0.0\n",
        "        pbar = tqdm(train_loader, desc=f\"Epoch {epoch}/{args.epochs}\")\n",
        "        for Xb, yb in pbar:\n",
        "            Xb, yb = Xb.to(device), yb.to(device)\n",
        "\n",
        "            # Outer-loop update\n",
        "            model.main_opt.zero_grad()\n",
        "            out    = model(Xb)\n",
        "            l_main = loss_fn(out, yb)\n",
        "            l_main.backward()\n",
        "            nn.utils.clip_grad_norm_(model.parameters(), args.clip)\n",
        "            model.main_opt.step()\n",
        "\n",
        "            # Inner-loop self-improvement\n",
        "            l_self = model.self_improve(loss_fn, Xb, yb,\n",
        "                                        steps=args.self_steps,\n",
        "                                        clip=args.clip)\n",
        "\n",
        "            epoch_loss += l_main.item()\n",
        "            global_step += 1\n",
        "            writer.add_scalar(\"train/loss_main\", l_main.item(), global_step)\n",
        "            writer.add_scalar(\"train/loss_self\", l_self, global_step)\n",
        "\n",
        "            # display on progress bar\n",
        "            pbar.set_postfix(main=f\"{l_main:.4f}\", self_imp=f\"{l_self:.4f}\")\n",
        "\n",
        "        scheduler.step()\n",
        "        avg = epoch_loss / len(train_loader)\n",
        "        print(f\"→ Epoch {epoch} avg loss {avg:.6f}\")\n",
        "\n",
        "        if epoch % args.eval_interval == 0:\n",
        "            # Save checkpoint\n",
        "            ckpt = dict(\n",
        "                epoch=epoch,\n",
        "                model=model.state_dict(),\n",
        "                optim=model.main_opt.state_dict(),\n",
        "                sched=scheduler.state_dict()\n",
        "            )\n",
        "            ckpt_path = os.path.join(args.checkpoint_dir, f\"agi_ep{epoch}.pt\")\n",
        "            torch.save(ckpt, ckpt_path)\n",
        "            print(\" checkpoint saved:\", ckpt_path)\n",
        "\n",
        "            # Evaluate & log\n",
        "            evaluate_and_log(model, val_loader, device, writer,\n",
        "                             epoch, args.mc_samples)\n",
        "\n",
        "    writer.close()\n",
        "\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# 4. Eval + Logging\n",
        "# -----------------------------------------------------------------------------\n",
        "def evaluate_and_log(model, val_loader, device, writer, epoch, mc_samples):\n",
        "    model.eval()\n",
        "    Y, P, S = [], [], []\n",
        "    for Xb, yb in val_loader:\n",
        "        xb   = Xb.to(device)\n",
        "        mean, std = model.mc_predict(xb, mc_samples)\n",
        "        Y.append(yb.numpy().squeeze())\n",
        "        P.append(mean)\n",
        "        S.append(std)\n",
        "\n",
        "    y_true = np.concatenate(Y)\n",
        "    y_pred = np.concatenate(P)\n",
        "    uncert = np.concatenate(S)\n",
        "    err    = np.abs(y_pred - y_true)\n",
        "\n",
        "    # Scalars\n",
        "    writer.add_scalar(\"val/mse\", np.mean((y_pred - y_true)**2), epoch)\n",
        "    writer.add_scalar(\"val/mae\", np.mean(err), epoch)\n",
        "\n",
        "    # Scatter plot\n",
        "    fig, ax = plt.subplots(figsize=(4,4))\n",
        "    ax.scatter(y_true, y_pred, s=5, alpha=0.5)\n",
        "    mn, mx = y_true.min(), y_true.max()\n",
        "    ax.plot([mn,mx],[mn,mx],'--', color='gray')\n",
        "    ax.set_xlabel(\"True\"); ax.set_ylabel(\"Pred\")\n",
        "    ax.set_title(f\"Epoch {epoch} Pred vs True\")\n",
        "    writer.add_figure(\"val/scatter\", fig, epoch)\n",
        "    fig.savefig(f\"out/epoch{epoch}_scatter.png\")\n",
        "    plt.close(fig)\n",
        "\n",
        "    # Residual histogram\n",
        "    fig, ax = plt.subplots()\n",
        "    ax.hist(err, bins=30, alpha=0.7)\n",
        "    ax.set_title(f\"Epoch {epoch} Residuals\")\n",
        "    writer.add_figure(\"val/residuals\", fig, epoch)\n",
        "    fig.savefig(f\"out/epoch{epoch}_resids.png\")\n",
        "    plt.close(fig)\n",
        "\n",
        "    # Reliability diagram\n",
        "    bins = np.linspace(uncert.min(), uncert.max(), 10)\n",
        "    idx  = np.digitize(uncert, bins) - 1\n",
        "    emp  = [err[idx==i].mean() if np.any(idx==i) else np.nan\n",
        "            for i in range(len(bins)-1)]\n",
        "    centers = 0.5*(bins[:-1] + bins[1:])\n",
        "    fig, ax = plt.subplots()\n",
        "    ax.plot(centers, emp, '-o', label=\"emp err\")\n",
        "    ax.plot([uncert.min(),uncert.max()],\n",
        "            [uncert.min(),uncert.max()],\n",
        "            '--', color='gray', label=\"ideal\")\n",
        "    ax.set_xlabel(\"σ_pred\"); ax.set_ylabel(\"emp |err|\")\n",
        "    ax.set_title(f\"Epoch {epoch} Reliability\")\n",
        "    ax.legend()\n",
        "    writer.add_figure(\"val/reliability\", fig, epoch)\n",
        "    fig.savefig(f\"out/epoch{epoch}_reliability.png\")\n",
        "    plt.close(fig)\n",
        "\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# 5. Argument Parsing & Entry Point\n",
        "# -----------------------------------------------------------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument(\"--epochs\",         type=int,   default=20)\n",
        "    parser.add_argument(\"--batch_size\",     type=int,   default=32)\n",
        "    parser.add_argument(\"--input_size\",     type=int,   default=10)\n",
        "    parser.add_argument(\"--hidden_size\",    type=int,   default=64)\n",
        "    parser.add_argument(\"--output_size\",    type=int,   default=1)\n",
        "    parser.add_argument(\"--lr_main\",        type=float, default=1e-3)\n",
        "    parser.add_argument(\"--lr_self\",        type=float, default=1e-3)\n",
        "    parser.add_argument(\"--dropout_p\",      type=float, default=0.1)\n",
        "    parser.add_argument(\"--self_steps\",     type=int,   default=3)\n",
        "    parser.add_argument(\"--clip\",           type=float, default=1.0)\n",
        "    parser.add_argument(\"--lr_step\",        type=int,   default=5)\n",
        "    parser.add_argument(\"--lr_gamma\",       type=float, default=0.5)\n",
        "    parser.add_argument(\"--eval_interval\",  type=int,   default=5)\n",
        "    parser.add_argument(\"--mc_samples\",     type=int,   default=50)\n",
        "    parser.add_argument(\"--num_samples\",    type=int,   default=2500)\n",
        "    parser.add_argument(\"--val_split\",      type=int,   default=500)\n",
        "    parser.add_argument(\"--checkpoint_dir\", type=str,   default=\"checkpoints\")\n",
        "    parser.add_argument(\"--logdir\",         type=str,   default=\"runs/exp1\")\n",
        "\n",
        "    # ignore unknown args (e.g. Jupyter’s -f flag)\n",
        "    args, _ = parser.parse_known_args()\n",
        "    train_and_evaluate(args)"
      ],
      "metadata": {
        "id": "Rj50IzwLDARf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}