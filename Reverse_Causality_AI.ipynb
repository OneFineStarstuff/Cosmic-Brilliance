{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyPhWd8PQLvF7Mou0qHiABb9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OneFineStarstuff/Cosmic-Brilliance/blob/main/Reverse_Causality_AI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "le1AjjTN5Erg"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "# Reproducibility\n",
        "SEED = 42\n",
        "np.random.seed(SEED)\n",
        "tf.random.set_seed(SEED)\n",
        "\n",
        "def make_reverse_causal_data(n_samples=12000, length=50, n_classes=10,\n",
        "                             noise_std=0.12, burst_prob=0.10, seed=SEED):\n",
        "    \"\"\"\n",
        "    Build labelable 1D time series:\n",
        "    - Class c has its own frequency and phase.\n",
        "    - A class-specific burst is more likely near the end (future), so backward context helps.\n",
        "    Output: X (n, length, 1), y (n,)\n",
        "    \"\"\"\n",
        "    rng = np.random.default_rng(seed)\n",
        "    per_class = n_samples // n_classes\n",
        "    X, y = [], []\n",
        "\n",
        "    freqs  = np.linspace(0.6, 2.1, n_classes)\n",
        "    phases = np.linspace(0.0, np.pi, n_classes)\n",
        "    t = np.linspace(0, 1, length)\n",
        "    last_win = slice(length - 10, length - 4)  # \"future\" window\n",
        "\n",
        "    for c in range(n_classes):\n",
        "        f, ph = freqs[c], phases[c]\n",
        "        for _ in range(per_class):\n",
        "            base = np.sin(2*np.pi*f*t + ph)\n",
        "            trend = 0.3 * (t - 0.5) * ((c % 2) * 2 - 1)  # alternate up/down trend\n",
        "            signal = 0.85*base + 0.15*trend\n",
        "\n",
        "            # AR(1) noise for temporal correlation\n",
        "            eps = rng.normal(0, noise_std, size=length)\n",
        "            ar = np.zeros_like(eps)\n",
        "            phi = 0.6\n",
        "            for i in range(1, length):\n",
        "                ar[i] = phi * ar[i-1] + eps[i]\n",
        "\n",
        "            x = signal + ar\n",
        "\n",
        "            # Future-coded burst (class-dependent amplitude)\n",
        "            if rng.random() < burst_prob:\n",
        "                amp = 0.8 + 0.15 * c  # class-specific intensity\n",
        "                k = rng.integers(last_win.start, last_win.stop - 2)\n",
        "                x[k:k+3] += rng.normal(amp, 0.05, size=3)\n",
        "\n",
        "            X.append(x[:, None].astype(\"float32\"))\n",
        "            y.append(c)\n",
        "\n",
        "    X = np.stack(X, axis=0)\n",
        "    y = np.array(y, dtype=np.int32)\n",
        "\n",
        "    # Shuffle\n",
        "    idx = rng.permutation(len(X))\n",
        "    return X[idx], y[idx]\n",
        "\n",
        "# Data\n",
        "X, y = make_reverse_causal_data(n_samples=12000, length=50, n_classes=10, noise_std=0.12, burst_prob=0.10)\n",
        "\n",
        "# Split 70/15/15\n",
        "n = len(X)\n",
        "n_train = int(0.70 * n); n_val = int(0.15 * n)\n",
        "X_train, y_train = X[:n_train], y[:n_train]\n",
        "X_val,   y_val   = X[n_train:n_train+n_val], y[n_train:n_train+n_val]\n",
        "X_test,  y_test  = X[n_train+n_val:],       y[n_train+n_val:]\n",
        "\n",
        "# Standardize by train stats\n",
        "mean = X_train.mean(axis=(0,1), keepdims=True)\n",
        "std  = X_train.std(axis=(0,1), keepdims=True) + 1e-7\n",
        "X_train = (X_train - mean) / std\n",
        "X_val   = (X_val   - mean) / std\n",
        "X_test  = (X_test  - mean) / std\n",
        "\n",
        "# tf.data pipelines\n",
        "BATCH = 128\n",
        "train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train)).shuffle(8192, seed=SEED).batch(BATCH).prefetch(tf.data.AUTOTUNE)\n",
        "val_ds   = tf.data.Dataset.from_tensor_slices((X_val, y_val)).batch(BATCH).prefetch(tf.data.AUTOTUNE)\n",
        "test_ds  = tf.data.Dataset.from_tensor_slices((X_test, y_test)).batch(BATCH).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "# Reverse Causal AI: forward (past) + backward (future) encoders with learned gate\n",
        "class ReverseCausalAI(tf.keras.Model):\n",
        "    def __init__(self, n_classes=10, width=64, l2=1e-5, drop=0.2):\n",
        "        super().__init__()\n",
        "        self.pre = tf.keras.layers.Conv1D(width, 5, padding='same', activation='relu',\n",
        "                                          kernel_regularizer=tf.keras.regularizers.l2(l2))\n",
        "        self.gru_f = tf.keras.layers.GRU(width, go_backwards=False,\n",
        "                                         kernel_regularizer=tf.keras.regularizers.l2(l2))\n",
        "        self.gru_b = tf.keras.layers.GRU(width, go_backwards=True,\n",
        "                                         kernel_regularizer=tf.keras.regularizers.l2(l2))\n",
        "        self.proj_f = tf.keras.layers.Dense(width, activation=None)\n",
        "        self.proj_b = tf.keras.layers.Dense(width, activation=None)\n",
        "        self.gate = tf.keras.layers.Dense(width, activation='sigmoid')\n",
        "        self.dropout = tf.keras.layers.Dropout(drop)\n",
        "        self.out = tf.keras.layers.Dense(n_classes, activation='softmax')\n",
        "\n",
        "    def call(self, inputs, training=False):\n",
        "        z = self.pre(inputs)                 # local temporal features\n",
        "        h_f = self.gru_f(z)                  # past→present\n",
        "        h_b = self.gru_b(z)                  # future→present (reverse-time summary)\n",
        "        f = self.proj_f(h_f)\n",
        "        b = self.proj_b(h_b)\n",
        "        g = self.gate(tf.concat([f, b], axis=-1))  # learned reverse-causal gate\n",
        "        fused = (1.0 - g) * f + g * b\n",
        "        fused = self.dropout(fused, training=training)\n",
        "        return self.out(fused)\n",
        "\n",
        "model = ReverseCausalAI(n_classes=10, width=64, l2=1e-5, drop=0.2)\n",
        "\n",
        "# Optimizer and compile\n",
        "try:\n",
        "    optimizer = tf.keras.optimizers.AdamW(learning_rate=1e-3, weight_decay=1e-4)\n",
        "except AttributeError:\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
        "\n",
        "model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Callbacks & logging\n",
        "os.makedirs(\"results\", exist_ok=True)\n",
        "callbacks = [\n",
        "    tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=8, restore_best_weights=True),\n",
        "    tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=3, min_lr=1e-5, verbose=1),\n",
        "    tf.keras.callbacks.CSVLogger(\"results/training_history.csv\"),\n",
        "    tf.keras.callbacks.ModelCheckpoint(\"results/reverse_causal_best.keras\",\n",
        "                                       monitor=\"val_accuracy\", save_best_only=True)\n",
        "]\n",
        "\n",
        "history = model.fit(train_ds, validation_data=val_ds, epochs=60, callbacks=callbacks, verbose=1)\n",
        "\n",
        "# Evaluate\n",
        "test_loss, test_acc = model.evaluate(test_ds, verbose=0)\n",
        "print(f\"✅ Test accuracy: {test_acc:.4f} | Test loss: {test_loss:.4f}\")\n",
        "print(\"✅ Training history saved to results/training_history.csv\")\n",
        "print(\"✅ Best model saved to results/reverse_causal_best.keras\")\n",
        "\n",
        "# Confusion matrix\n",
        "y_pred = model.predict(test_ds, verbose=0).argmax(axis=1)\n",
        "cm = tf.math.confusion_matrix(y_test, y_pred, num_classes=10).numpy()\n",
        "print(\"Confusion matrix:\\n\", cm)"
      ]
    }
  ]
}